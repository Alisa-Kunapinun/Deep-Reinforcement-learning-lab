{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Markov Decision Process\n",
    "\n",
    "Reference:\n",
    "\n",
    "- Pytorch 1.x Reinforcement Learning Cookbook (Packtpub)\n",
    "- Hands-On Reinforcement Learning for Games (Packtpub)\n",
    "- https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html\n",
    "- Reinforcement Learning: An Introduction (Sutton et al.)\n",
    "- https://github.com/werner-duvaud/muzero-general (simulator code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Markov decision process?\n",
    "Markov decision process (MDP) is a discrete-time stochastic control process.\n",
    "It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
    "MDPs are useful for studying optimization problems solved via dynamic programming.\n",
    "They are used in many disciplines, including robotics, automatic control, economics and manufacturing.\n",
    "\n",
    "The MDP model is based on the idea of an environment that evolves as a Markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chain\n",
    "\n",
    "A Markov chain is a model of the dynamics of a discrete time system that obeys the (first order) **Markov property**, meaning that *the state $s^{t+1}$ at time $t+1$ is conditionally given the state at time $t$, but it is independent of the state at times $0, \\ldots, t-1$*, i.e.,\n",
    "\n",
    "$$ p(s^{t+1} \\mid s^t, s^{t-1}, \\ldots, s^0) = p(s^{t+1} \\mid s^t). $$\n",
    "\n",
    "We might say that **the current state is all you need to know to predict the next state**.\n",
    "\n",
    "A Markov chain is defined by a set of possible states $S={s_0, s_1, \\ldots, s_n}$ and a transition matrix $T(s,s')$ containing the propbabilities of state $s$ (current state) transitioning to state $s'$ (next state).\n",
    "\n",
    "Here is a visualization of a simple Markov chain:\n",
    "\n",
    "<img src=\"img/RL_markov.png\" title=\"Markov chain\" style=\"width: 600px;\" />\n",
    "\n",
    "You might be interested in [this Markov chain simulator](https://setosa.io/ev/markov-chains/) and the [full screen diagram](https://setosa.io/markov/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Markov chain\n",
    "\n",
    "Assume we have **Three states** (**A**, **B**, **C**), the Markov chain is:\n",
    "\n",
    "<img src=\"img/MarkovChain.PNG\" title=\"Markov chain example\" style=\"width: 400px;\" />\n",
    "\n",
    "The transition matrix is:\n",
    "\n",
    "$$\n",
    "T =\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.5 & 0.2 \\\\ \n",
    "0.8 & 0.1 & 0.1 \\\\\n",
    "0.1 & 0.3 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each row means current state, and each column means the next state to go. If you consider $0.5$ in the matrix (row 0, and column 1), you can say that \"now you are in state **A** and the possible to go to next state of **B** is 0.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 0.5000, 0.2000],\n",
      "        [0.8000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.3000, 0.6000]])\n"
     ]
    }
   ],
   "source": [
    "# import pytorch\n",
    "import torch\n",
    "\n",
    "T = torch.tensor([[0.3, 0.5, 0.2],\n",
    "                  [0.8, 0.1, 0.1],\n",
    "                  [0.1, 0.3, 0.6]])\n",
    "\n",
    "# show the matrix\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability after k steps\n",
    "\n",
    "The probability after $k$ steps equation is:\n",
    "$$ T_n = T^n $$\n",
    "\n",
    "Thus, if we want to find the T state at step 2, 5, 10, 20, we can do as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state transition at k=2: tensor([[0.5100, 0.2600, 0.2300],\n",
      "        [0.3300, 0.4400, 0.2300],\n",
      "        [0.3300, 0.2600, 0.4100]])\n",
      "state transition at k=5: tensor([[0.3991, 0.3230, 0.2779],\n",
      "        [0.4153, 0.3100, 0.2746],\n",
      "        [0.3926, 0.3165, 0.2908]])\n",
      "state transition at k=10: tensor([[0.4026, 0.3170, 0.2804],\n",
      "        [0.4024, 0.3172, 0.2804],\n",
      "        [0.4024, 0.3170, 0.2806]])\n",
      "state transition at k=20: tensor([[0.4024, 0.3171, 0.2805],\n",
      "        [0.4024, 0.3171, 0.2805],\n",
      "        [0.4024, 0.3171, 0.2805]])\n"
     ]
    }
   ],
   "source": [
    "T_2 = torch.matrix_power(T,2)\n",
    "print('state transition at k=2:', T_2)\n",
    "T_5 = torch.matrix_power(T,5)\n",
    "print('state transition at k=5:', T_5)\n",
    "T_10 = torch.matrix_power(T,10)\n",
    "print('state transition at k=10:', T_10)\n",
    "T_20 = torch.matrix_power(T,20)\n",
    "print('state transition at k=20:', T_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state transition after step 10 to 20 steps are going to converges. This means that, no matter what state the process is in, it has the same probability of transitioning to A (40.24%), B (31.71%), and C (28.05%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have the initial distribution of three state is:\n",
    "\n",
    "$$\n",
    "V=\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The initial distribution means you can go to any states at the first time, but it depends on the probability. In this matrix, we can say that there are possible to go to state A more than other states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.5000, 0.3000]])\n"
     ]
    }
   ],
   "source": [
    "V_0 = torch.tensor([[0.2, 0.5, 0.3]])\n",
    "\n",
    "print(V_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to calculate the state distribution after state 1, 2, 5, 10, and 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of states after 1 step:\n",
      "tensor([[0.4900, 0.2400, 0.2700]])\n",
      "\n",
      "Distribution of states after 2 step:\n",
      "tensor([[0.3660, 0.3500, 0.2840]])\n",
      "\n",
      "Distribution of states after 5 step:\n",
      "tensor([[0.4053, 0.3146, 0.2801]])\n",
      "\n",
      "Distribution of states after 10 step:\n",
      "tensor([[0.4024, 0.3171, 0.2805]])\n",
      "\n",
      "Distribution of states after 20 step:\n",
      "tensor([[0.4024, 0.3171, 0.2805]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V_1 = torch.mm(V_0, T)\n",
    "print(\"Distribution of states after 1 step:\\n{}\\n\".format(V_1))\n",
    "V_2 = torch.mm(V_0, T_2)\n",
    "print(\"Distribution of states after 2 step:\\n{}\\n\".format(V_2))\n",
    "V_5 = torch.mm(V_0, T_5)\n",
    "print(\"Distribution of states after 5 step:\\n{}\\n\".format(V_5))\n",
    "V_10 = torch.mm(V_0, T_10)\n",
    "print(\"Distribution of states after 10 step:\\n{}\\n\".format(V_10))\n",
    "V_20 = torch.mm(V_0, T_20)\n",
    "print(\"Distribution of states after 20 step:\\n{}\\n\".format(V_20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, after 10 steps, the state distribution converges. The probability of being in A (40.24%), B (31.71%) and the probability of being in s1 (28.05%) remain unchanged in the long run.\n",
    "\n",
    "Starting with $[0.2,0.5,0.3]$, the state distribution after one iteration becomes $[0.4024, 0.3171, 0.2805]$. Details of its calculation are illustrated in the following diagram:\n",
    "\n",
    "<img src=\"img/MarkovDistributionChart.png\" title=\"Markov chain example\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Come back to MDP\n",
    "\n",
    "MDP is slightly different from a simple Markov chain because MDP has to consider agent's actions which affect to the system's dynamics.\n",
    "Thus, not only probability of transition of nextstate, but also agent's action need to be calculate. We assign $A$ as all possible actions and $a$ as the action which agent selects (at this situation, $a$ must be in $A$ or $a \\in A$.\n",
    "\n",
    "The transition probabilities become a **3D tensor** of size $|S|\\times |A|\\times |S|$\n",
    "mapping each state/action pair to a probability distribution over the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple MDP\n",
    "\n",
    "Suppose we have **three states (s0, s1, s2) and two actions (a0, a1)** and that the state/action transition tensor is as follows:\n",
    "\n",
    "$$T=\n",
    "\\begin{cases}\n",
    " &\n",
    "\\begin{bmatrix}\n",
    "0.8 & 0.1 & 0.1 \\\\ \n",
    "0.1 & 0.6 & 0.3\n",
    "\\end{bmatrix} \\\\ \n",
    " & \n",
    "\\begin{bmatrix}\n",
    "0.7 & 0.2 & 0.1 \\\\ \n",
    "0.1 & 0.8 & 0.1\n",
    "\\end{bmatrix} \\\\  \n",
    " & \n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.2 & 0.2 \\\\ \n",
    "0.1 & 0.4 & 0.5\n",
    "\\end{bmatrix}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The first matrix block is mean current state $s0$, second block is $s1$, and the third block is $s2$.\n",
    "\n",
    "Consider into the matrix block, the rows of matrix block mean actions (a0 is 1st row, and a1 is 2nd row). And the columns of matix block is the same as Markov Chain: next state $s'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8000, 0.1000, 0.1000],\n",
      "         [0.1000, 0.6000, 0.3000]],\n",
      "\n",
      "        [[0.7000, 0.2000, 0.1000],\n",
      "         [0.1000, 0.8000, 0.1000]],\n",
      "\n",
      "        [[0.6000, 0.2000, 0.2000],\n",
      "         [0.1000, 0.4000, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "# State transition function\n",
    "T = torch.tensor([[[0.8, 0.1, 0.1],\n",
    "                   [0.1, 0.6, 0.3]],\n",
    "                  [[0.7, 0.2, 0.1],\n",
    "                   [0.1, 0.8, 0.1]],\n",
    "                  [[0.6, 0.2, 0.2],\n",
    "                   [0.1, 0.4, 0.5]]])\n",
    "\n",
    "# show the matrix\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we want to observe at state 2, and see the probability to change to state 1 with playing action 0, we can write in code with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[2,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete our simple MDP, we need a *reward function* $R$ and a *discount factor* $\\gamma$.\n",
    "\n",
    "**reward function $R$** is a set of rewards that depend on the state and the action taken.\n",
    "\n",
    "**discount factor $\\gamma$** is how important future rewards are to the current state. Discount factor is a value between 0 and 1. A reward $R$ that occurs $n$ steps in the future from the current state, is multiplied by $\\gamma^n$ to describe its importance to the current state (Thus, current reward is $\\gamma^tR$, where $t$ is a number step.\n",
    "\n",
    "Suppose $R = [ -1, 0.1, 0.9 ]$ and $\\gamma = 0.5$. Let's define our MDP in Python with PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "R = torch.tensor([-1.,0.1,0.9])\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent's goal\n",
    "\n",
    "Once the MDP is defined, the agent's goal is to **maximize its expected reward**.\n",
    "\n",
    "If we start in state $s^0$ and perform a series of actions $a^0, a^1, \\ldots a^{T-1}$ placing us in state $s^1, s^2, \\ldots s^T$, we obtain the total reward $R_F$\n",
    "\n",
    "$$R_F=\\sum_{t=0}^T \\gamma^{t} R(s^t)$$\n",
    "\n",
    "The agent's goal is to behave so as to maximize the expected total reward. To do so,\n",
    "it should come up with a policy $\\pi : S \\times A \\rightarrow \\mathbb{R}$ giving a probability distribution\n",
    "over actions that can be executed in each state, then when in state $s$, sample action $a$ according to that\n",
    "distribution $\\pi(s,\\cdot)$, and repeat.\n",
    "\n",
    "Now the agent's goal can be clearly specified as finding an optimal policy\n",
    "\n",
    "$$ \\pi^* = \\textrm{argmax}_\\pi \\mathbb{E}_{a^t \\sim \\pi(s^t), s^{t} \\sim T(s^{t-1},a^{t-1})}\\left[ \\sum_{t=0}^T \\gamma^{t} R(s^t) \\right]$$\n",
    "\n",
    "Under a particular policy $\\pi$, then, the *value* of state $s$ is the expected reward we obtain by following $\\pi$ from state $s$:\n",
    "\n",
    "$$ V^\\pi(s) = \\mathbb{E}_{a^t \\sim \\pi(s^t), s^{t} \\sim T(s^{t-1},a^{t-1}) \\mid s^0=s}\\left[ R(s) + \\sum_{t=1}^T \\gamma^{t} R(s^t) \\right]$$\n",
    "\n",
    "The value function clearly obeys the *Bellman equations*\n",
    "\n",
    "$$ V^\\pi(s) = R(s) + \\gamma \\sum_{s',a'} \\pi(s,a') T(s,a',s') V^\\pi(s'). $$ \n",
    "\n",
    "#### Too difficult?\n",
    "\n",
    "OK, lets see the easier version of equation\n",
    "\n",
    "The equation of V relate at time t is:\n",
    "\n",
    "$$V_{t+1}=R+\\gamma * T * V_t$$\n",
    "\n",
    "When the value converges, which mean $V_{t+1}=V_t$, so we can derive the value $V$ as:\n",
    "\n",
    "$$\n",
    "V=R+\\gamma * T * V \\\\\n",
    "V = (I-\\gamma * T)^{-1} * R.\n",
    "$$\n",
    "\n",
    "The function can be writed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_value_matrix_inversion(gamma, trans_matrix, rewards):\n",
    "    inv = torch.inverse(torch.eye(rewards.shape[0]) - gamma * trans_matrix)\n",
    "    V = torch.mm(inv, rewards.reshape(-1, 1))\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asume that we select action $a_0$ in all curcumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find transition matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8000, 0.1000, 0.1000],\n",
      "        [0.7000, 0.2000, 0.1000],\n",
      "        [0.6000, 0.2000, 0.2000]])\n"
     ]
    }
   ],
   "source": [
    "trans_matrix = T[:, action]\n",
    "\n",
    "print(trans_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate value from the bellman equation above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value function under the optimal policy is:\n",
      "tensor([[-1.6781],\n",
      "        [-0.5202],\n",
      "        [ 0.3828]])\n"
     ]
    }
   ],
   "source": [
    "V = cal_value_matrix_inversion(gamma, trans_matrix, R)\n",
    "\n",
    "print(\"The value function under the optimal policy is:\\n{}\".format(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab exercise 1\n",
    "\n",
    "1. Try to use different $\\gamma$ values from 0 to 1 for 5 values. 0 means we only care about the immediate reward, and 1 means we do not care the reward\n",
    "2. Try to find the value $V$ in step $k=1,2,3,..,10$ with the action $a_0$ and random action. Show the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy evaluation\n",
    "\n",
    "To determine how good a particular policy is, we use policy evaluation.\n",
    "Policy evaluation is an iterative algorithm. It starts with arbitrary values for each state\n",
    "and then iteratively updates the values based on the Bellman equations until the values converge. Assume $\\pi$ is the value of a policy, the update equation is:\n",
    "\n",
    "$$ V(s) = \\sum_a \\pi(s,a) \\left[ R(s,a) + \\gamma \\sum_{s'} T(s,a',s') V(s') \\right] $$ \n",
    "\n",
    "$\\pi(s,a)$: the probability of taking action $a$ in stat $s$ under policy $\\pi$\n",
    "$R(s,a)$: the reward received in state $s$ by taking action $a$\n",
    "\n",
    "You can see this algorithm's pseudocode in Sutton's book on page 75.\n",
    "\n",
    "Because Policy evaluation is a loop processing, you need to stop by conditions. There are 2 ways to terminate the iterative updating process.\n",
    "- Fix number of iterations\n",
    "- Set some specific threshold and terminating the process when the values of all states change to lower than the threshold.\n",
    "\n",
    "Here we compute the value of the three states in our MDP assuming the agent always peforms the first action. The function process are:\n",
    "\n",
    "1. Initializes the policy values as all zeros.\n",
    "2. Updates the values based on the Bellman expectation equation.\n",
    "3. Computes the maximal change of the values across all states.\n",
    "4. If the maximal change is greater than the threshold, it keeps updating the values. Otherwise, it terminates the evaluation process and returns the latest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, trans_matrix, rewards, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Perform policy evaluation\n",
    "    @param policy: policy matrix containing actions and their \n",
    "                            probability in each state\n",
    "    @param trans_matrix: transformation matrix\n",
    "    @param rewards: rewards for each state\n",
    "    @param gamma: discount factor\n",
    "    @param threshold: the evaluation will stop once values \n",
    "                           for all states are less than the threshold\n",
    "    @return: values of the given policy for all possible states\n",
    "    \"\"\"\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    V_his = [V]\n",
    "    i = 0\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        i += 1\n",
    "        for state, actions in enumerate(policy):\n",
    "            for action, action_prob in enumerate(actions):\n",
    "                V_temp[state] += action_prob * (R[state] + gamma * torch.dot(trans_matrix[state, action], V))\n",
    "        max_delta = torch.max(torch.abs(V - V_temp))\n",
    "        V = V_temp.clone()\n",
    "        V_his.append(V)\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V, V_his"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the threshold used to determine when to stop the evaluation process and the optimal policy where action a0 is chosen under all circumstances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0001\n",
    "\n",
    "policy_optimal = torch.tensor([[1.0, 0.0],\n",
    "                               [1.0, 0.0],\n",
    "                               [1.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the policy_evaluation function to find the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value function under the optimal policy is:\n",
      "tensor([-1.6780, -0.5201,  0.3829])\n"
     ]
    }
   ],
   "source": [
    "V, V_his = policy_evaluation(policy_optimal, T, R, gamma, threshold)\n",
    "\n",
    "print(\"The value function under the optimal policy is:\\n{}\".format(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting history of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(V_history, gamma):\n",
    "    s = []\n",
    "    text = []\n",
    "    for i in range(len(V_history[0])):\n",
    "        s_x, = plt.plot([v[i] for v in V_history])\n",
    "        s.append(s_x)\n",
    "        text.append('State s' + str(i))\n",
    "\n",
    "    plt.title('Optimal policy with gamma = {}'.format(str(gamma)))\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Policy values')\n",
    "    plt.legend(s, text, loc=\"upper left\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(V_his, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab exercise 2\n",
    "\n",
    "1. Change discount factor to 0.1, 0.2, and 0.99 and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration algorithm\n",
    "\n",
    "The idea behind value iteration is quite similar to that of policy evaluation. It is also an iterative algorithm. It starts with arbitrary policy values and then iteratively updates the values based on the Bellman optimality equation until they converge. So in each iteration, instead of taking the expectation (average) of values across all actions, it picks the action that achieves the maximal policy values:\n",
    "\n",
    "$$ V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} T(s,a',s') V^*(s') \\right] $$ \n",
    "\n",
    "$V^*(s)$ denotes the optimal value, which is the value of the optimal policy\n",
    "\n",
    "Once the optimal values are computed, we obtain the optimal policy:\n",
    "\n",
    "$$ \\pi^*(s) = \\text{argmax}_a \\sum_{s'}T(s,a,s') \\left[ R(s,a) + \\gamma V^*(s') \\right] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply it to gym environment\n",
    "\n",
    "In this step, we try to use *FrozenLake* environment to see how to implement the value iteration.\n",
    "\n",
    "FrozenLake is a typical Gym environment with a discrete state space. It is about moving an agent from the starting location to the goal location in a grid world, and at the same time avoiding traps. Currently, We use four by four grid (https://gym.openai.com/envs/FrozenLake-v0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import gym library and pytorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Frozen environment and take a look states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states: 16\n",
      "number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "n_state = env.observation_space.n\n",
    "print(\"number of states:\", n_state)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "print(\"number of actions:\", n_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset and render the environment. Because the graphical of the environment is **text** style and no image pop-up. We can use render() directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "state:  0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"state: \", state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to put an down action to make the agent work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "left = 0\n",
    "down = 1\n",
    "right = 2\n",
    "up = 3\n",
    "new_state, reward, is_done, info = env.step(down)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output of environment class after take action. In the <code>info</code>, and <code>new_state</code>, it says that the agent lands in <code>new_state</code>, with probability 0.3333333333333333. The reward is 0 because the agent has not reached the goal and the episode is not done. You might see the agent landing in different state, or staying in state 0 because of the slippery surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  1\n",
      "reward:  0.0\n",
      "Finish?:  False\n",
      "information:  {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(\"state: \", new_state)\n",
    "\n",
    "print(\"reward: \", reward)\n",
    "\n",
    "print(\"Finish?: \", is_done)\n",
    "\n",
    "print(\"information: \", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that simulates a FrozenLake episode given a policy and returns the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=False):\n",
    "    state = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    step = 1\n",
    "    while not is_done:\n",
    "        action = policy[state].item()\n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        if render:\n",
    "            print('step: ', step)\n",
    "            env.render()\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if is_done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run 500 episodes with random policy, and see the results\n",
    "\n",
    "We randomly generated a policy that was composed of 16 actions for the 16 states. Keep in mind that in FrozenLake, the movement direction is only partially dependent on the chosen action. This increases the uncertainty of control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under random policy: 0.002\n",
      "tensor([1, 1, 0, 3, 1, 0, 1, 0, 3, 0, 2, 2, 1, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "n_episode = 500\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episode):\n",
    "    # Create random policy with integer value deal to number of actions with n_state size.\n",
    "    random_policy = torch.randint(high=n_action, size=(n_state,))\n",
    "    # run an episode\n",
    "    total_reward = run_episode(env, random_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "    if total_reward == 1:\n",
    "        best_policy = random_policy\n",
    "        break\n",
    "    \n",
    "print('Average total reward under random policy: {}'.format(sum(total_rewards) / n_episode))\n",
    "# see the best policy\n",
    "print(best_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run 500 episodes again using the <code>best_policy</code>, to check that the winning rate is good or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under best policy: 0.046\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episode):# run an episode\n",
    "    total_reward = run_episode(env, best_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "print('Average total reward under best policy: {}'.format(sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  1\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  2\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  3\n",
      "  (Down)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  4\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  5\n",
      "  (Down)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  6\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  7\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  8\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  9\n",
      "  (Down)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  10\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  11\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  12\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "step:  13\n",
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Total reward under best policy: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_reward = run_episode(env, best_policy, True)\n",
    "\n",
    "print('Total reward under best policy: {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving FrozenLake with value iteration\n",
    "\n",
    "At above, we found that the best policy from random policy is not good enough, so let's implement value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set discount factor $\\gamma$ as 0.9 and convergence threshold 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "threshold = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create value iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Solve a given environment with value iteration algorithm\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param threshold: the evaluation will stop once values for \n",
    "                       all states are less than the threshold\n",
    "    @return: values of the optimal policy for the given \n",
    "              environment\n",
    "    \"\"\"\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.empty(n_state)\n",
    "        for state in range(n_state):\n",
    "            v_actions = torch.zeros(n_action)\n",
    "            for action in range(n_action):\n",
    "                # calculate the optimal policy using probability in environment\n",
    "                for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
    "                    v_actions[action] += trans_prob * (reward + gamma * V[new_state])\n",
    "            V_temp[state] = torch.max(v_actions)\n",
    "        max_delta = torch.max(torch.abs(V - V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal value using value iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values:\n",
      "tensor([0.0689, 0.0614, 0.0744, 0.0558, 0.0918, 0.0000, 0.1122, 0.0000, 0.1454,\n",
      "        0.2475, 0.2996, 0.0000, 0.0000, 0.3799, 0.6390, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "V_optimal = value_iteration(env, gamma, threshold)\n",
    "\n",
    "print('Optimal values:\\n{}'.format(V_optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to extract from optimal value to be optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_optimal_policy(env, V_optimal, gamma):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy based on the optimal values\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param V_optimal: optimal values\n",
    "    @param gamma: discount factor\n",
    "    @return: optimal policy\n",
    "    \"\"\"\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    optimal_policy = torch.zeros(n_state)\n",
    "    for state in range(n_state):\n",
    "        v_actions = torch.zeros(n_action)\n",
    "        for action in range(n_action):\n",
    "            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
    "                v_actions[action] += trans_prob * (reward \n",
    "                           + gamma * V_optimal[new_state])\n",
    "        optimal_policy[state] = torch.argmax(v_actions)\n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy:\n",
      "tensor([0., 3., 0., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = extract_optimal_policy(env, V_optimal, gamma)\n",
    "\n",
    "print('Optimal policy:\\n{}'.format(optimal_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run 500 episodes again using the <code>optimal_policy</code>, to check that the winning rate is good or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under optimal policy: 0.728\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episode):# run an episode\n",
    "    total_reward = run_episode(env, optimal_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "print('Average total reward under optimal policy: {}'.format(sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the optimal_policy accuracy is better than the random_policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab exercise 3\n",
    "\n",
    "1. Change discount factor to at least 20 different data and plot the average reward (y-axis) via the discount factor (x-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement with policy iteration\n",
    "\n",
    "A policy iteration algorithm can be subdivided into two components: policy evaluation and policy improvement. It starts with an arbitrary policy. And in each iteration, it first computes the policy values given the latest policy, based on the Bellman expectation equation; it then extracts an improved policy out of the resulting policy values, based on the Bellman optimality equation. It iteratively evaluates the policy and generates an improved version until the policy doesn't change any more.\n",
    "\n",
    "$$ V(s) = \\sum_{s'} T(s,a',s') \\left[ R(s,a,s') + \\gamma V(s') \\right] $$ \n",
    "\n",
    "Once the optimal values are computed, we obtain the optimal policy:\n",
    "\n",
    "$$ \\pi(s) = \\text{argmax}_a \\sum_{s'}T(s,a,s') \\left[ R(s,a,s') + \\gamma V(s') \\right] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify a policy_evaluation function to support in gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Perform policy evaluation\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param policy: policy matrix containing actions and \n",
    "                            their probability in each state\n",
    "    @param gamma: discount factor\n",
    "    @param threshold: the evaluation will stop once values for all states are less than the threshold\n",
    "    @return: values of the given policy\n",
    "    \"\"\"\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state in range(n_state):\n",
    "            action = policy[state].item()\n",
    "            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
    "                V_temp[state] += trans_prob * (reward + gamma * V[new_state])\n",
    "        max_delta = torch.max(torch.abs(V - V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "           break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a policy_improvement function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma):\n",
    "    \"\"\"\n",
    "    Obtain an improved policy based on the values\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param V: policy values\n",
    "    @param gamma: discount factor\n",
    "    @return: the policy\n",
    "    \"\"\"\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    policy = torch.zeros(n_state)\n",
    "    for state in range(n_state):\n",
    "        v_actions = torch.zeros(n_action)\n",
    "        for action in range(n_action):\n",
    "            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
    "                v_actions[action] += trans_prob * (reward + gamma * V[new_state])\n",
    "        policy[state] = torch.argmax(v_actions)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create policy_iteration function. The function combines policy_evaluation and policy_improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma, threshold):\n",
    "    \"\"\"\n",
    "    Solve a given environment with policy iteration algorithm\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param threshold: the evaluation will stop once values \n",
    "                    for all states are less than the threshold\n",
    "    @return: optimal values and the optimal policy for the given \n",
    "              environment\n",
    "    \"\"\"\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    policy = torch.randint(high=n_action, size=(n_state,)).float()\n",
    "    while True:\n",
    "        V = policy_evaluation(env, policy, gamma, threshold)\n",
    "        policy_improved = policy_improvement(env, V, gamma)\n",
    "        if torch.equal(policy_improved, policy):\n",
    "            return V, policy_improved\n",
    "        policy = policy_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values:\n",
      "tensor([0.0689, 0.0614, 0.0744, 0.0558, 0.0918, 0.0000, 0.1122, 0.0000, 0.1454,\n",
      "        0.2475, 0.2996, 0.0000, 0.0000, 0.3799, 0.6390, 0.0000])\n",
      "Optimal policy:\n",
      "tensor([0., 3., 0., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "V_optimal, optimal_policy = policy_iteration(env, gamma, threshold)\n",
    "\n",
    "print('Optimal values:\\n{}'.format(V_optimal))\n",
    "print('Optimal policy:\\n{}'.format(optimal_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run 500 episodes again using the <code>optimal_policy</code>, to check that the winning rate is good or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under optimal policy: 0.746\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episode):# run an episode\n",
    "    total_reward = run_episode(env, optimal_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "print('Average total reward under optimal policy: {}'.format(sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab exercise 4\n",
    "\n",
    "1. Change discount factor to at least 20 different data and plot the average reward (y-axis) via the discount factor (x-axis).\n",
    "2. See the policy evaluation, value iteration, and policy iteration, which one is better in\n",
    " - Fast convergence\n",
    " - Best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
