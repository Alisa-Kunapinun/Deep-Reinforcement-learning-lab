{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 Setup and installation environment for reinforcement learning\n",
    "\n",
    "## Reinforcement learning\n",
    "\n",
    "Reinforcement Learning (RL) is a machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback on its actions and experiences. RL uses rewards and punishment as signals for \"good\" and \"bad\" behavior.\n",
    "\n",
    "Reinforcement learning compound with:\n",
    "1. Environment\n",
    "2. Agent\n",
    "\n",
    "Generally, at each step, \n",
    "1. The **agent** outputs an ***action***, which is input to the **environment**.\n",
    "2. The **environment** evolves according to its dynamics and change to be ***new state***.\n",
    "3. The **agent** observes the ***new state*** of the **environment** and (optionally) a ***reward***\n",
    "\n",
    "The process continues until hopefully the agent learns what behavior maximizes its reward.\n",
    "\n",
    "<img src=\"img/RL.jpg\" title=\"Introduction\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, our responsibility is to create and design **agent** as smart as possible. However, we cannot let our agent learns without environment. In this lab, we must install python, pytorch, and openAI gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "One of the popular simulation environment for RL is OpenAI Gym.\n",
    "\n",
    "[OpenAI](https://openai.com) is a research company trying to develop systems exhibiting *artificial general intelligence* (AGI).\n",
    "They developed Gym to support the development of RL algorithms. Gym\n",
    "provides many reinforcement learning simulations and tasks. Visit [the Gym website](https://gym.openai.com) for a full list of environments.\n",
    "\n",
    "<img src=\"img/RL_gym.PNG\" title=\"Gym example\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Environment in your PC\n",
    "\n",
    "***Note***: you can use *google colab*, if your computer does not support.\n",
    "\n",
    "***Note2***: No need to follow this instruction, if you can do it, show your result to me is fine.\n",
    "\n",
    "**System requirement**: Ubuntu (Linux), Nvidia GPU (optional, really, but you may cry when try to run more complex agent)\n",
    "    \n",
    "Things to install for desktop version:\n",
    "1. VSCode, or PyCharm, or jupyter, or conda, or Visual Studio (For windows user only) --> Select one as you like\n",
    "2. Python 3.8.xx or upper\n",
    "3. important Libraries: numpy, matplotlib, etc.\n",
    "4. PyTorch library (for NVidia GPU need cuda version)\n",
    "5. mujoco_py library\n",
    "6. OpenAI library: gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VSCode\n",
    "Visual Studio Code is a lightweight yet full featured cross platform IDE for software development that has recently caught up in terms of capabilities and popularity with other popular IDEs for Python such as PyCharm. It is reputed to be easier to configure and use, also. We'll give it a try this semester. Download and install VSCode from the [Visual Studio downloads page](https://code.visualstudio.com/download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Windows User\n",
    "You can use Windows but there is not manual in here. Please follow from the links:\n",
    "1. Install python from the [link](https://www.python.org/downloads/windows/), select version 3.8.xx (You can use more latest version, but I cannot confirm it works properly. At least I know 3.9.xx is working properly.)\n",
    "2. Install [visual studio code](https://code.visualstudio.com/docs/python/python-tutorial) and setup python interpreter.\n",
    "3. Install pytorch from [link](https://pytorch.org/)\n",
    "4. Install [OpenAI Gym](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30)\n",
    "5. Install [OpenAI Gym with Box2D and Mujoco](https://medium.com/@sayanmndl21/install-openai-gym-with-box2d-and-mujoco-in-windows-10-e25ee9b5c1d5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Linux User\n",
    "\n",
    "#### Step 1: Install VS code\n",
    "\n",
    "Download VS code from [here](https://code.visualstudio.com/download) and install it.\n",
    "\n",
    "#### Step 2: Install Python\n",
    "\n",
    "For the person who already has python program can skip this.\n",
    "\n",
    "1. Open *terminal*\n",
    "2. Do Step\n",
    "    1. Update and Refresh Repository lists:\n",
    "        - `$ sudo apt-get update`\n",
    "        - `$ sudo apt-get upgrade`\n",
    "    2. Install Python:\n",
    "        - `$ sudo apt-get install build-essential cmake python3-numpy python3-dev python3-tk libavcodec-dev libavformat-dev libavutil-dev libswscale-dev libavresample-dev libdc1394-dev libeigen3-dev libgtk-3-dev libvtk7-qt-dev`\n",
    "        - check python version: `$ python --version` or `$ python3 --version`\n",
    "    3. Upgrade pip3: `sudo -H pip3 install --upgrade pip`\n",
    "    4. Install Jupyter: `$ pip install jupyter` (optional)\n",
    "        - Test Run Jupyter Notebook: `$ jupyter notebook`\n",
    "        \n",
    "3. Open VS code:\n",
    "    1. Before do anything, create a folder which you want to put your code into\n",
    "    2. At VS code, go to *File* --> *Open Folder...* --> Select the folder path.\n",
    "    \n",
    "    <img src=\"img/VScod01.PNG\" style=\"width: 600px;\">\n",
    "    \n",
    "    3. You can save **Workspace** to link the folder to workspace file. Go to *File* --> *Save Workspace* --> set the workspace name. After that you can open your code via workspace file.\n",
    "    4. Create a new file by click at icon new file. Type file name and type of python file\n",
    "    \n",
    "        <img src=\"img/VScod02.PNG\" style=\"width: 300px;\">\n",
    "        \n",
    "        - .py : Python script file. It runs fast but difficult to show simulator.\n",
    "        - .ipynb : Jupyter notebook file. It can make note, like lecture note. Coding can stop by cell. Easy to see your simulation, but it is slower and possible to collapse easily.\n",
    "    5. Select interpreter as below:\n",
    "    \n",
    "    <img src=\"img/VScod03.PNG\" style=\"width: 600px;\">\n",
    "    \n",
    "    6. If using Jupyter notebook file, you can select interpreter at here:\n",
    "    \n",
    "    <img src=\"img/VScod04.PNG\" style=\"width: 600px;\">\n",
    "    \n",
    "#### Step 3: Install PyTorch\n",
    "\n",
    "Go to this [link](https://pytorch.org/) scroll down until found the page as below. Select one option (Usually select Linux, Pip, Python, CUDA10.2 or CUDA11.3 or CPU, but nevermind this website has automatically selected your suitable installation mode). Copy the text below into your terminal.\n",
    "\n",
    "<img src=\"img/pytorch.PNG\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Install Mujoco and openAI-gym\n",
    "\n",
    "You must install mujoco first, you do in 2 ways\n",
    "\n",
    "1. **Install mujoco_py (full version)** Follow from the [link](https://github.com/openai/mujoco-py)\n",
    "\n",
    "***Note***: If you use jupyter notebook and want to use command line in terminal, add '!' in front of command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install important library\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "# Get Mujoco\n",
    "!mkdir ~/.mujoco\n",
    "!wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "!tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "!rm mujoco.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "!echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
    "!echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
    "# THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "!echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "!ldconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Mujoco-py\n",
    "!pip3 install -U 'mujoco-py<2.2,>=2.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "try:\n",
    "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
    "except KeyError:\n",
    "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
    "try:\n",
    "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "except KeyError:\n",
    "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "    \n",
    "# presetup so we don't see output on first env initialization\n",
    "import mujoco_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it does not work (GL error or something), uninstall it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall mujoco-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install **free-mujoco-py**\n",
    "\n",
    "this version is easy to setup, but it is a little bit slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install free-mujoco-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!pip install -U gym>=0.21.0\n",
    "!pip install -U gym[atari,accept-rom-license]\n",
    "!pip install -U gym[Robotics,classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: After install gym, try this code. If there are some error occur, you must solve the problem, unless **uninstall mujoco-py and reinstall free-mujoco-py** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"FetchPickAndPlace-v1\")\n",
    "env.reset()\n",
    "env.render()\n",
    "time.sleep(5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Google Colab\n",
    "\n",
    "For the person who cannot setup your computer, you can use Google Colab for run and train in this class. However, you **Must** do these installation step every time when the system shutdown. For free version, system will shut down every 12 hours, and pro version (3xx baht per month) will shut down every 24 hours.\n",
    "\n",
    "You can access Google colab from this [colab](https://colab.research.google.com/). Sign in as your e-mail and it will ready to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a Google colab file and copy the code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Install xvfb & other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install x11-utils > /dev/null 2>&1 \n",
    "!pip install pyglet > /dev/null 2>&1 \n",
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Install mujoco\n",
    "This step used for 3D simulators. Ex. Robots, and some control plants\n",
    "\n",
    "In some simulators, it does not require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include this at the top of your colab code\n",
    "import os\n",
    "if not os.path.exists('.mujoco_setup_complete'):\n",
    "  # Get the prereqs\n",
    "  !apt-get -qq update\n",
    "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "  # Get Mujoco\n",
    "  !mkdir ~/.mujoco\n",
    "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "  !rm mujoco.tar.gz\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
    "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
    "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "  !ldconfig\n",
    "  # Install Mujoco-py\n",
    "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
    "  # run once\n",
    "  !touch .mujoco_setup_complete\n",
    "\n",
    "try:\n",
    "  if _mujoco_run_once:\n",
    "    pass\n",
    "except NameError:\n",
    "  _mujoco_run_once = False\n",
    "if not _mujoco_run_once:\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  try:\n",
    "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
    "  except KeyError:\n",
    "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
    "  try:\n",
    "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  except KeyError:\n",
    "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  # presetup so we don't see output on first env initialization\n",
    "  import mujoco_py\n",
    "  _mujoco_run_once = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Install pyvirtual display\n",
    "\n",
    "This is for show simulator in the google colab (and jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!pip install -U gym>=0.21.0\n",
    "!pip install -U gym[atari,accept-rom-license]\n",
    "!pip install -U gym[Robotics,classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the example and test in colab from the [link](https://colab.research.google.com/drive/1WonMpHUG_0MO8jedG7ePmoGOo-JVPS7r?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your environment\n",
    "\n",
    "After installation finish, use the code below to check that your installation is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import all your libraries, including matplotlib & ipythondisplay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then you want to import Display from pyvirtual display & initialise your screen size, in this example 400x300... :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using gym's \"rgb_array\" render functionally, render to a \"Screen\" variable, then plot the screen variable using Matplotlib!\n",
    "\n",
    "(rendered indirectly using Ipython display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.make(\"DoubleDunk-v0\")\n",
    "# env = gym.make(\"SpaceInvaders-v0\")\n",
    "# env = gym.make(\"Acrobot-v1\") # double invert pendulum\n",
    "# env = gym.make(\"Ant-v2\")\n",
    "env = gym.make(\"FetchPickAndPlace-v1\")\n",
    "env.reset()\n",
    "prev_screen = env.render(mode='rgb_array')\n",
    "plt.imshow(prev_screen)\n",
    "\n",
    "for i in range(50):\n",
    "  action = env.action_space.sample()\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  screen = env.render(mode='rgb_array')\n",
    "\n",
    "  plt.imshow(screen)\n",
    "  ipythondisplay.clear_output(wait=True)\n",
    "  ipythondisplay.display(plt.gcf())\n",
    "\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save simulator video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create video folder name \"video_rl\"\n",
    "\n",
    "You can create yourself, or use python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "vdo_path = 'video_rl/'\n",
    "if not os.path.exists(vdo_path):\n",
    "  print(\"No folder \", vdo_path, 'exist. Create the folder')\n",
    "  os.mkdir(vdo_path)\n",
    "  print(\"Create directory finished\")\n",
    "else:\n",
    "  print(vdo_path, 'existed, do nothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: modify openAI gym code at last section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add wrappers monitor library\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change environment as you want\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.make(\"DoubleDunk-v0\")\n",
    "# env = gym.make(\"SpaceInvaders-v0\")\n",
    "# env = gym.make(\"Acrobot-v1\") # double invert pendulum\n",
    "# env = gym.make(\"Ant-v2\")\n",
    "# env = gym.make(\"InvertedDoublePendulum-v2\")\n",
    "\n",
    "# Set environment and monitor it in video\n",
    "env = Monitor(gym.make('Ant-v2'), vdo_path, force=True)\n",
    "env.reset()\n",
    "prev_screen = env.render(mode='rgb_array')\n",
    "plt.imshow(prev_screen)\n",
    "\n",
    "for i in range(500):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    # Show screen in jupyter or colab, very slow\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show your vdo\n",
    "\n",
    "<video controls src=\"img/openai_test2.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check PyTorch available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.has_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you found <code>torch.has_cuda</code> is True, you can use GPU for run. If you don't have it, the code still can run but it will be slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a random search policy\n",
    "\n",
    "Now, let's implement in CartPole environment.\n",
    "\n",
    "You can use .py or .ipynb file.\n",
    "\n",
    "1. First of all, import Gym and PyTorch packages and CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "\n",
    "# for real-time show\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "# for save video\n",
    "from gym.wrappers import Monitor\n",
    "save_vdo = False\n",
    "\n",
    "import os\n",
    "vdo_path = 'video_rl/'\n",
    "if not os.path.exists(vdo_path):\n",
    "  print(\"No folder \", vdo_path, 'exist. Create the folder')\n",
    "  os.mkdir(vdo_path)\n",
    "  print(\"Create directory finished\")\n",
    "else:\n",
    "  print(vdo_path, 'existed, do nothing')\n",
    "\n",
    "if save_vdo:\n",
    "    env = Monitor(gym.make('CartPole-v0'), vdo_path, force=True)\n",
    "else:\n",
    "    env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check number of states, and number of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state = env.observation_space.shape\n",
    "print('State matrix:', n_state, 'number of state', n_state[0])\n",
    "\n",
    "n_action = env.action_space.n\n",
    "print('number of action:', n_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create <code>run_episode</code> function for run and simulate when give input weight and return all reward in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, weight, show=False):\n",
    "    # reset to default state\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        # Get state situation from environment\n",
    "        state = torch.from_numpy(state).float()\n",
    "        # Calculate action from maximum possible\n",
    "        action = torch.argmax(torch.matmul(state, weight))\n",
    "        # Send action to environment to get next state\n",
    "        state, reward, is_done, _ = env.step(action.item())\n",
    "        \n",
    "        if show:\n",
    "            # render screen to show\n",
    "            screen = env.render(mode='rgb_array')\n",
    "            plt.imshow(screen)\n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())\n",
    "        \n",
    "        # sum all rewards\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>weight</code> $W$ is used to get possibility actions $pA$ from current state $S$ happened. To calulate probability actions, you can multiply matrix:\n",
    "\n",
    "$$pA=SW$$\n",
    "\n",
    "To get the actions, in reinforcement learning, you can do as random actions (from probability) or maximum probability. In this implementation, we select action $a$ from maximum probability. To get index of maximum value, use <code>torch.argmax()</code> function. This function return an array tensor, to address this, use <code>.item()</code> to get one-element tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Try to run 1 episode from random weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random weight\n",
    "weight = torch.rand(n_state[0], n_action)\n",
    "# Run 1 episode to get total_reward (Show simulator)\n",
    "total_reward = run_episode(env, weight, True)\n",
    "print('Episode {}: {}'.format(0, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. OK! Let's find the best weight from searching using the maximum reward in 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "best_total_reward = 0\n",
    "best_weight = None\n",
    "total_rewards = []\n",
    "# Set number of episode\n",
    "n_episode = 1000\n",
    "for episode in range(n_episode):\n",
    "    weight = torch.rand(n_state[0], n_action)\n",
    "    # Run 1 episode to get total_reward (not show simulator)\n",
    "    total_reward = run_episode(env, weight, False)\n",
    "    print('Episode {}: {}'.format(episode+1, total_reward))\n",
    "    # find the best weight from best reward\n",
    "    if total_reward > best_total_reward:\n",
    "        best_weight = weight\n",
    "        best_total_reward =  total_reward\n",
    "    # keep all total_rewards\n",
    "    total_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average total reward over {} episode: {}'.format(\n",
    "           n_episode, sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the rewards are not improved by episode step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Simulate the result from the best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1 episode to get total_reward (Show simulator)\n",
    "total_reward = run_episode(env, best_weight, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Plot the total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This library is used for plot\n",
    "# import matplotlib.pyplot as plt\n",
    "plt.plot(total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. See the average reward from new 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "best_total_reward = 0\n",
    "total_rewards_eval = []\n",
    "# Set number of episode\n",
    "n_episode = 1000\n",
    "for episode in range(n_episode):\n",
    "    # Run 1 episode to get total_reward (not show simulator)\n",
    "    total_reward_eval = run_episode(env, best_weight, False)\n",
    "    print('Episode {}: {}'.format(episode+1, total_reward_eval))\n",
    "    # keep all total_rewards\n",
    "    total_rewards_eval.append(total_reward_eval)\n",
    "    \n",
    "print('Average total reward over {} episode: {}'.format(\n",
    "           n_episode, sum(total_rewards_eval) / n_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab work\n",
    "\n",
    "1. Setup and install Python, PyTorch and OpenAI environment (with mujoco_py) in **any environment**. (Windows, Linux, MacOS or Colab, in PyCharm, VS code, Jupyter, or other)\n",
    "    - Show your result that you can use OpenAI and PyTorch.\n",
    "    - Save an 3D environment into vdo at least 5 second.\n",
    "2. (Optional) For the person who have lag of python and pytorch, please study it.\n",
    "    - [Python tutorial](https://www.w3schools.com/python/)\n",
    "    - [Numpy tutorial](https://www.w3schools.com/python/numpy/default.asp)\n",
    "    - [MatPlotLib](https://matplotlib.org/stable/tutorials/index)\n",
    "    - [PyTorch tutorial](https://pytorch.org/tutorials/)\n",
    "3. Try to implement [**Hill-climbing**](https://en.wikipedia.org/wiki/Hill_climbing) algorithm in *CartPole*. The weight for each episode can be calculated by:\n",
    "    $$W_n=W_b+\\alpha W_r$$\n",
    "    \n",
    "    when $W_n$ is the new weight which input into each episode, $W_b$ is the best weight, $\\alpha$ is learning rate scale, and $W_r$ is the new random weight. At default, letting $\\alpha=0.01$\n",
    "\n",
    "    - Plot the graph while training and see the different between random search and hill-climbing\n",
    "    - Change $\\alpha$ to be 0.5, 0.1, and 0.001. See the different.\n",
    "    - Do a short report (1-2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
