{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: PPO and DDPG\n",
    "\n",
    "In this lab, we continue to Policy gradient method. There are many algorithms which improve from ordinary Actor critic. We sample 2 kinds of method: PPO and DDPG.\n",
    "\n",
    "Reference:\n",
    "- https://medium.com/deepgamingai/proximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-22337981f815\n",
    "- https://github.com/alirezakazemipour/Continuous-PPO\n",
    "- https://github.com/MWeltevrede/PPO\n",
    "- https://github.com/alirezakazemipour/DDPG-HER\n",
    "- https://sites.ualberta.ca/~pilarski/docs/papers/Pilarski_2013_ICORR_Postprint.pdf\n",
    "- https://github.com/TianhongDai/hindsight-experience-replay\n",
    "- https://github.com/alirezakazemipour/DDPG-HER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO)\n",
    "\n",
    "The PPO algorithm was introduced by the OpenAI team in 2017 and quickly became one of the most popular RL methods usurping the Deep-Q learning method. It involves collecting a small batch of experiences interacting with the environment and using that batch to update its decision-making policy. Once the policy is updated with this batch, the experiences are thrown away and a newer batch is collected with the newly updated policy. This is the reason why it is an “on-policy learning” approach where the experience samples collected are only useful for updating the current policy once.\n",
    "\n",
    "The key contribution of PPO is ensuring that a new update of the policy does not change it too much from the previous policy. This leads to less variance in training at the cost of some bias, but ensures smoother training and also makes sure the agent does not go down an unrecoverable path of taking senseless actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actor model\n",
    "\n",
    "Same as normal actor model, the Actor model performs the task of learning what action to take under a particular **observed state** of the environment.\n",
    "\n",
    "### The critic model\n",
    "\n",
    "Also the same, we send the action predicted by the Actor to the environment and observe what happens in the game. If something positive happens as a result of our action, like agent can increase some plus point, then the environment sends back a positive response in the form of a reward. If an own goal occurs due to our action, then we get a negative reward. This reward is taken in by the **Critic model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "from torch import nn\n",
    "from torch.distributions import normal\n",
    "import torch\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.mu = nn.Linear(in_features=64, out_features=self.n_actions)\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, self.n_actions))\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        std = self.log_std.exp()\n",
    "        dist = normal.Normal(mu, std)\n",
    "\n",
    "        return dist\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_states):\n",
    "        super(Critic, self).__init__()\n",
    "        self.n_states = n_states\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.value = nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        value = self.value(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "Agent is the AI agent core which contain the network model to use and to load, save and update any weights of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "\n",
    "# from model import Actor, Critic\n",
    "from torch.optim import Adam\n",
    "from torch import from_numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env_name, n_iter, n_states, action_bounds, n_actions, lr):\n",
    "        self.env_name = env_name\n",
    "        self.n_iter = n_iter\n",
    "        self.action_bounds = action_bounds\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.device = torch.device(process_device)\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.current_policy = Actor(n_states=self.n_states,\n",
    "                                    n_actions=self.n_actions).to(self.device)\n",
    "        self.critic = Critic(n_states=self.n_states).to(self.device)\n",
    "\n",
    "        self.actor_optimizer = Adam(self.current_policy.parameters(), lr=self.lr, eps=1e-5)\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=self.lr, eps=1e-5)\n",
    "\n",
    "        self.critic_loss = torch.nn.MSELoss()\n",
    "\n",
    "        self.scheduler = lambda step: max(1.0 - float(step / self.n_iter), 0)\n",
    "\n",
    "        self.actor_scheduler = LambdaLR(self.actor_optimizer, lr_lambda=self.scheduler)\n",
    "        self.critic_scheduler = LambdaLR(self.actor_optimizer, lr_lambda=self.scheduler)\n",
    "\n",
    "    def choose_dist(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        state = from_numpy(state).float().to(self.device)\n",
    "        with torch.no_grad():\n",
    "            dist = self.current_policy(state)\n",
    "\n",
    "        # action *= self.action_bounds[1]\n",
    "        # action = np.clip(action, self.action_bounds[0], self.action_bounds[1])\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def get_value(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        state = from_numpy(state).float().to(self.device)\n",
    "        with torch.no_grad():\n",
    "            value = self.critic(state)\n",
    "\n",
    "        return value.detach().cpu().numpy()\n",
    "\n",
    "    def optimize(self, actor_loss, critic_loss):\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.current_policy.parameters(), 0.5)\n",
    "        # torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.current_policy.parameters(), 0.5)\n",
    "        # torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    def schedule_lr(self):\n",
    "        # self.total_scheduler.step()\n",
    "        self.actor_scheduler.step()\n",
    "        self.critic_scheduler.step()\n",
    "\n",
    "    def save_weights(self, iteration, state_rms):\n",
    "        torch.save({\"current_policy_state_dict\": self.current_policy.state_dict(),\n",
    "                    \"critic_state_dict\": self.critic.state_dict(),\n",
    "                    \"actor_optimizer_state_dict\": self.actor_optimizer.state_dict(),\n",
    "                    \"critic_optimizer_state_dict\": self.critic_optimizer.state_dict(),\n",
    "                    \"actor_scheduler_state_dict\": self.actor_scheduler.state_dict(),\n",
    "                    \"critic_scheduler_state_dict\": self.critic_scheduler.state_dict(),\n",
    "                    \"iteration\": iteration,\n",
    "                    \"state_rms_mean\": state_rms.mean,\n",
    "                    \"state_rms_var\": state_rms.var,\n",
    "                    \"state_rms_count\": state_rms.count}, self.env_name + \"_weights.pth\")\n",
    "\n",
    "    def load_weights(self):\n",
    "        checkpoint = torch.load(self.env_name + \"_weights.pth\")\n",
    "        self.current_policy.load_state_dict(checkpoint[\"current_policy_state_dict\"])\n",
    "        self.critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer_state_dict\"])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer_state_dict\"])\n",
    "        self.actor_scheduler.load_state_dict(checkpoint[\"actor_scheduler_state_dict\"])\n",
    "        self.critic_scheduler.load_state_dict(checkpoint[\"critic_scheduler_state_dict\"])\n",
    "        iteration = checkpoint[\"iteration\"]\n",
    "        state_rms_mean = checkpoint[\"state_rms_mean\"]\n",
    "        state_rms_var = checkpoint[\"state_rms_var\"]\n",
    "\n",
    "        return iteration, state_rms_mean, state_rms_var\n",
    "\n",
    "    def set_to_eval_mode(self):\n",
    "        self.current_policy.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "    def set_to_train_mode(self):\n",
    "        self.current_policy.train()\n",
    "        self.critic.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean, Std\n",
    "\n",
    "In actor critic model, to perform the stat before go to the model, it is necessary to do something like a batch normalization to the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running_mean_std.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    # -> It's indeed batch normalization :D\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
    "\n",
    "\n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model function\n",
    "\n",
    "The <code>evaluate_model</code> is the function which uses while training. The actor critic model needs to finish each episode before training the output of accumulate rewards is the result to say how clever of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(agent, env, state_rms, action_bounds):\n",
    "    total_rewards = 0\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        s = np.clip((s - state_rms.mean) / (state_rms.var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "        dist = agent.choose_dist(s)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        # action = np.clip(action, action_bounds[0], action_bounds[1])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # env.render()\n",
    "        s = next_state\n",
    "        total_rewards += reward\n",
    "    # env.close()\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO\n",
    "\n",
    "An important step in the PPO algorithm is to run through this entire loop with the two models for a fixed number of steps known as PPO steps. So essentially, we are interacting with our environemt for certain number of steps and collecting the states, actions, rewards, etc. which we will use for training.\n",
    "\n",
    "### Generalized Advantage Estimation (GAE)\n",
    "\n",
    "Advantage can be defined as a way to measure how much better off we can be by taking a particular action when we are in a particular state. We want to use the rewards that we collected at each time step and calculate how much of an advantage we were able to obtain by taking the action that we took. So if we took a good action, we want to calculate how much better off we were by taking that action, not only in the short run but also over a longer period of time. This way, even if we do not immediately score a goal in the next time step after shooting, we still look at few time steps after that action into the longer future to see if we scored a goal.\n",
    "\n",
    "In order to calculate this, we’ll use an algorithm known as Generalized Advantage Estimation or GAE. So let’s take a look at how this algorithm works.\n",
    "\n",
    "1. Initialize advantage: $gae=0$\n",
    "2. Loop backwards: $t=stop$ to $t=0$\n",
    "3. Define delta: $\\delta=r_t+\\gamma \\cdot V(s_{t+1}) \\cdot m_t - V(s_t)$\n",
    "4. Update value of gae: $gae_t=\\delta + \\gamma \\cdot \\lambda \\cdot m_t \\cdot gae_{t+1}$\n",
    "5. Calculate returns R: $R_t(s_t,a_t)=gae_t+V(s_t)$\n",
    "6. Loop from number 2\n",
    "\n",
    "Given:\n",
    "- $m$: a mask value is used because if the game is over then the next state in our batch will be from a newly restarted game so we do not want to consider that and therefore mask value is taken as 0.\n",
    "- $\\gamma$: discount factor in order to reduce the value of the future state (default 0.99)\n",
    "- $\\lambda$: is a smoothing parameter used for reducing the variance in training which makes it more stable. (default 0.95)\n",
    "\n",
    "### Custom PPO loss\n",
    "\n",
    "This is the most important part of the Proximal Policy Optimization algorithm. So let’s first understand this loss function.\n",
    "\n",
    "Recall that $\\pi$ indicates the policy that is defined by our Actor neural network model. By training this model, we want to improve this policy so that it gives us better and better actions over time. Now a major problem in some Reinforcement Learning approaches is that once our model adopts a bad policy, it only takes bad actions in the game, so we are unable to generate any good actions from there on leading us down an unrecoverable path in training. PPO tries to address this by only making small updates to the model in an update step, thereby stabilizing the training process. The PPO loss can be calculated as follows.\n",
    "\n",
    "1. Calulate how much the policy has changed: $ratio = \\pi_{new}/ \\pi_{old}$\n",
    "2. Express in log form: $ratio = [\\log (\\pi_{new}) - \\log(\\pi_{old})].exp()$\n",
    "3. Calculate Actor loss as minimum of two functions:\n",
    "$$p_1 = ratio \\cdot advantage$$\n",
    "$$p_2 = clip(ratio, 1-\\epsilon, 1+\\epsilon) \\cdot advantage$$ where $\\epsilon=0.2$ (for example)\n",
    "$$actor_{loss}=min(p_1,p_2)$$\n",
    "4. Calculate Critic loss as MSE between returns and critic value: $critic_{loss}=(R-V(s))^2$\n",
    "5. Calculate Total loss: $total_{loss}=critic_{loss} \\cdot critic_{discount} + actor_loss \\cdot entropy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "# from running_mean_std import RunningMeanStd\n",
    "# from test import evaluate_model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, env, test_env, env_name, n_iterations, agent, epochs, mini_batch_size, epsilon, horizon):\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.test_env = test_env\n",
    "        self.agent = agent\n",
    "        self.epsilon = epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.state_rms = RunningMeanStd(shape=(self.agent.n_states,))\n",
    "\n",
    "        self.running_reward = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_mini_batch(mini_batch_size, states, actions, returns, advs, values, log_probs):\n",
    "        full_batch_size = len(states)\n",
    "        for _ in range(full_batch_size // mini_batch_size):\n",
    "            indices = np.random.randint(0, full_batch_size, mini_batch_size)\n",
    "            yield states[indices], actions[indices], returns[indices], advs[indices], values[indices],\\\n",
    "                  log_probs[indices]\n",
    "\n",
    "    def train(self, states, actions, advs, values, log_probs):\n",
    "\n",
    "        values = np.vstack(values[:-1])\n",
    "        log_probs = np.vstack(log_probs)\n",
    "        returns = advs + values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        actions = np.vstack(actions)\n",
    "        for epoch in range(self.epochs):\n",
    "            for state, action, return_, adv, old_value, old_log_prob in self.choose_mini_batch(self.mini_batch_size,\n",
    "                                                                                               states, actions, returns,\n",
    "                                                                                               advs, values, log_probs):\n",
    "                state = torch.Tensor(state).to(self.agent.device)\n",
    "                action = torch.Tensor(action).to(self.agent.device)\n",
    "                return_ = torch.Tensor(return_).to(self.agent.device)\n",
    "                adv = torch.Tensor(adv).to(self.agent.device)\n",
    "                old_value = torch.Tensor(old_value).to(self.agent.device)\n",
    "                old_log_prob = torch.Tensor(old_log_prob).to(self.agent.device)\n",
    "\n",
    "                value = self.agent.critic(state)\n",
    "                # clipped_value = old_value + torch.clamp(value - old_value, -self.epsilon, self.epsilon)\n",
    "                # clipped_v_loss = (clipped_value - return_).pow(2)\n",
    "                # unclipped_v_loss = (value - return_).pow(2)\n",
    "                # critic_loss = 0.5 * torch.max(clipped_v_loss, unclipped_v_loss).mean()\n",
    "                critic_loss = self.agent.critic_loss(value, return_)\n",
    "\n",
    "                new_log_prob = self.calculate_log_probs(self.agent.current_policy, state, action)\n",
    "\n",
    "                ratio = (new_log_prob - old_log_prob).exp()\n",
    "                actor_loss = self.compute_actor_loss(ratio, adv)\n",
    "\n",
    "                self.agent.optimize(actor_loss, critic_loss)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def step(self):\n",
    "        state = self.env.reset()\n",
    "        for iteration in range(1, 1 + self.n_iterations):\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            dones = []\n",
    "\n",
    "            self.start_time = time.time()\n",
    "            for t in range(self.horizon):\n",
    "                # self.state_rms.update(state)\n",
    "                state = np.clip((state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "                dist = self.agent.choose_dist(state)\n",
    "                action = dist.sample()\n",
    "                # action = np.clip(action, self.agent.action_bounds[0], self.agent.action_bounds[1])\n",
    "                log_prob = dist.log_prob(action).cpu()\n",
    "                action = action.cpu().numpy()[0]\n",
    "                value = self.agent.get_value(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                dones.append(done)\n",
    "\n",
    "                if done:\n",
    "                    state = self.env.reset()\n",
    "                else:\n",
    "                    state = next_state\n",
    "            # self.state_rms.update(next_state)\n",
    "            next_state = np.clip((next_state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "            next_value = self.agent.get_value(next_state) * (1 - done)\n",
    "            values.append(next_value)\n",
    "\n",
    "            advs = self.get_gae(rewards, values, dones)\n",
    "            states = np.vstack(states)\n",
    "            actor_loss, critic_loss = self.train(states, actions, advs, values, log_probs)\n",
    "            # self.agent.set_weights()\n",
    "            self.agent.schedule_lr()\n",
    "            eval_rewards = evaluate_model(self.agent, self.test_env, self.state_rms, self.agent.action_bounds)\n",
    "            self.state_rms.update(states)\n",
    "            self.print_logs(iteration, actor_loss, critic_loss, eval_rewards)\n",
    "            print(\"iteration: \", iteration, \"\\teval_rewards: \", eval_rewards)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "\n",
    "        advs = []\n",
    "        gae = 0\n",
    "\n",
    "        dones.append(0)\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * (values[step + 1]) * (1 - dones[step]) - values[step]\n",
    "            gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "            advs.append(gae)\n",
    "\n",
    "        advs.reverse()\n",
    "        return np.vstack(advs)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_log_probs(model, states, actions):\n",
    "        policy_distribution = model(states)\n",
    "        return policy_distribution.log_prob(actions)\n",
    "\n",
    "    def compute_actor_loss(self, ratio, adv):\n",
    "        pg_loss1 = adv * ratio\n",
    "        pg_loss2 = adv * torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "        loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "        return loss\n",
    "\n",
    "    def print_logs(self, iteration, actor_loss, critic_loss, eval_rewards):\n",
    "        if iteration == 1:\n",
    "            self.running_reward = eval_rewards\n",
    "        else:\n",
    "            self.running_reward = self.running_reward * 0.99 + eval_rewards * 0.01\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iter:{iteration}| \"\n",
    "                  f\"Ep_Reward:{eval_rewards:.3f}| \"\n",
    "                  f\"Running_reward:{self.running_reward:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Iter_duration:{time.time() - self.start_time:.3f}| \"\n",
    "                  f\"lr:{self.agent.actor_scheduler.get_last_lr()}\")\n",
    "            self.agent.save_weights(iteration, self.state_rms)\n",
    "\n",
    "        with SummaryWriter(self.env_name + \"/logs\") as writer:\n",
    "            writer.add_scalar(\"Episode running reward\", self.running_reward, iteration)\n",
    "            writer.add_scalar(\"Episode reward\", eval_rewards, iteration)\n",
    "            writer.add_scalar(\"Actor loss\", actor_loss, iteration)\n",
    "            writer.add_scalar(\"Critic loss\", critic_loss, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n"
     ]
    }
   ],
   "source": [
    "# play.py\n",
    "\n",
    "from mujoco_py.generated import const\n",
    "from mujoco_py import GlfwContext\n",
    "import numpy as np\n",
    "import cv2\n",
    "GlfwContext(offscreen=True)\n",
    "\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, env_name, max_episode=1):\n",
    "        self.env = env\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        _, self.state_rms_mean, self.state_rms_var = self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = torch.device(process_device)\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.VideoWriter = cv2.VideoWriter(env_name + \".avi\", self.fourcc, 50.0, (250, 250))\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        for _ in range(self.max_episode):\n",
    "            s = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            for _ in range(self.env._max_episode_steps):\n",
    "                s = np.clip((s - self.state_rms_mean) / (self.state_rms_var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "                dist = self.agent.choose_dist(s)\n",
    "                action = dist.sample().cpu().numpy()[0]\n",
    "                s_, r, done, _ = self.env.step(action)\n",
    "                episode_reward += r\n",
    "                if done:\n",
    "                    break\n",
    "                s = s_\n",
    "                # self.env.render(mode=\"human\")\n",
    "                # self.env.viewer.cam.type = const.CAMERA_FIXED\n",
    "                # self.env.viewer.cam.fixedcamid = 0\n",
    "                # time.sleep(0.03)\n",
    "                I = self.env.render(mode='rgb_array')\n",
    "                I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "                I = cv2.resize(I, (250, 250))\n",
    "                self.VideoWriter.write(I)\n",
    "                # cv2.imshow(\"env\", I)\n",
    "                # cv2.waitKey(10)\n",
    "            print(f\"episode reward:{episode_reward:3.3f}\")\n",
    "        self.env.close()\n",
    "        self.VideoWriter.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try some environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import mujoco_py\n",
    "# from agent import Agent\n",
    "# from train import Train\n",
    "# from play import Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double inverted pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07536218 -0.01266603  0.0984087   0.99991978  0.99514608 -0.06849933\n",
      " -0.11647428  0.04592492  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"InvertedDoublePendulum\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "print(test_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 500\n",
    "lr = 3e-4\n",
    "epochs = 10\n",
    "clip_range = 0.2\n",
    "mini_batch_size = 64\n",
    "T = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states:11\n",
      "action bounds:[-1.0, 1.0]\n",
      "number of actions:1\n",
      "iteration:  1 \teval_rewards:  64.03977869424199\n",
      "iteration:  2 \teval_rewards:  63.74281212489949\n",
      "iteration:  3 \teval_rewards:  82.0762700197893\n",
      "iteration:  4 \teval_rewards:  101.29183773500634\n",
      "iteration:  5 \teval_rewards:  35.62218981504196\n",
      "iteration:  6 \teval_rewards:  91.74456902843956\n",
      "iteration:  7 \teval_rewards:  137.40959328327475\n",
      "iteration:  8 \teval_rewards:  202.67234090600732\n",
      "iteration:  9 \teval_rewards:  165.36867069691579\n",
      "iteration:  10 \teval_rewards:  91.92686569958862\n",
      "iteration:  11 \teval_rewards:  231.3105264898099\n",
      "iteration:  12 \teval_rewards:  193.60033788769636\n",
      "iteration:  13 \teval_rewards:  137.124092421563\n",
      "iteration:  14 \teval_rewards:  192.64245312619192\n",
      "iteration:  15 \teval_rewards:  82.50928230821982\n",
      "iteration:  16 \teval_rewards:  211.33300091274936\n",
      "iteration:  17 \teval_rewards:  72.3585688225138\n",
      "iteration:  18 \teval_rewards:  146.55086380189505\n",
      "iteration:  19 \teval_rewards:  202.1422349203148\n",
      "iteration:  20 \teval_rewards:  295.373224813731\n",
      "iteration:  21 \teval_rewards:  137.41367218756983\n",
      "iteration:  22 \teval_rewards:  527.7851409988699\n",
      "iteration:  23 \teval_rewards:  89.90677036092592\n",
      "iteration:  24 \teval_rewards:  202.66383820651998\n",
      "iteration:  25 \teval_rewards:  386.6255649915953\n",
      "iteration:  26 \teval_rewards:  424.66369850587154\n",
      "iteration:  27 \teval_rewards:  407.77047825268045\n",
      "iteration:  28 \teval_rewards:  220.7566524471067\n",
      "iteration:  29 \teval_rewards:  407.75903176372026\n",
      "iteration:  30 \teval_rewards:  247.88608408656341\n",
      "iteration:  31 \teval_rewards:  220.99456268570106\n",
      "iteration:  32 \teval_rewards:  230.99542148159688\n",
      "iteration:  33 \teval_rewards:  425.97458833630736\n",
      "iteration:  34 \teval_rewards:  378.974434216894\n",
      "iteration:  35 \teval_rewards:  398.30649401501574\n",
      "iteration:  36 \teval_rewards:  509.7306240325926\n",
      "iteration:  37 \teval_rewards:  388.53097389759677\n",
      "iteration:  38 \teval_rewards:  389.3567975474812\n",
      "iteration:  39 \teval_rewards:  379.26039273865797\n",
      "iteration:  40 \teval_rewards:  351.2705065276329\n",
      "iteration:  41 \teval_rewards:  315.0485809916664\n",
      "iteration:  42 \teval_rewards:  472.0495458298537\n",
      "iteration:  43 \teval_rewards:  584.6338161785137\n",
      "iteration:  44 \teval_rewards:  445.8307187849004\n",
      "iteration:  45 \teval_rewards:  138.0945925918771\n",
      "iteration:  46 \teval_rewards:  250.1142392821674\n",
      "iteration:  47 \teval_rewards:  687.9644538885132\n",
      "iteration:  48 \teval_rewards:  1171.9643796818996\n",
      "iteration:  49 \teval_rewards:  398.9891331757577\n",
      "iteration:  50 \teval_rewards:  574.9936715054341\n",
      "iteration:  51 \teval_rewards:  724.6927385142887\n",
      "iteration:  52 \teval_rewards:  1162.075537887594\n",
      "iteration:  53 \teval_rewards:  100.2104819855751\n",
      "iteration:  54 \teval_rewards:  957.5656788466353\n",
      "iteration:  55 \teval_rewards:  1553.2281617459755\n",
      "iteration:  56 \teval_rewards:  4718.247987690691\n",
      "iteration:  57 \teval_rewards:  900.4261023238449\n",
      "iteration:  58 \teval_rewards:  1636.644543158387\n",
      "iteration:  59 \teval_rewards:  4632.574950437273\n",
      "iteration:  60 \teval_rewards:  9308.971196231147\n",
      "iteration:  61 \teval_rewards:  3347.909466292154\n",
      "iteration:  62 \teval_rewards:  434.1465329405079\n",
      "iteration:  63 \teval_rewards:  761.1581197252093\n",
      "iteration:  64 \teval_rewards:  4809.087704861437\n",
      "iteration:  65 \teval_rewards:  1162.5897224115351\n",
      "iteration:  66 \teval_rewards:  108.83164901573545\n",
      "iteration:  67 \teval_rewards:  491.8231286795352\n",
      "iteration:  68 \teval_rewards:  5982.480315898074\n",
      "iteration:  69 \teval_rewards:  612.6107062209616\n",
      "iteration:  70 \teval_rewards:  463.82678644006353\n",
      "iteration:  71 \teval_rewards:  2277.9917166196537\n",
      "iteration:  72 \teval_rewards:  1916.7955710971048\n",
      "iteration:  73 \teval_rewards:  4168.609423076819\n",
      "iteration:  74 \teval_rewards:  2119.969066895403\n",
      "iteration:  75 \teval_rewards:  9308.607343708582\n",
      "iteration:  76 \teval_rewards:  9307.750453941342\n",
      "iteration:  77 \teval_rewards:  8773.17632017228\n",
      "iteration:  78 \teval_rewards:  9312.293704714848\n",
      "iteration:  79 \teval_rewards:  9309.053649460348\n",
      "iteration:  80 \teval_rewards:  4029.4605547194587\n",
      "iteration:  81 \teval_rewards:  9308.46176005845\n",
      "iteration:  82 \teval_rewards:  9307.13418698227\n",
      "iteration:  83 \teval_rewards:  9306.98071105775\n",
      "iteration:  84 \teval_rewards:  9308.519858287234\n",
      "iteration:  85 \teval_rewards:  2937.6535522999443\n",
      "iteration:  86 \teval_rewards:  2258.624689838504\n",
      "iteration:  87 \teval_rewards:  2577.9058761986294\n",
      "iteration:  88 \teval_rewards:  1709.9166019398758\n",
      "iteration:  89 \teval_rewards:  2398.7217794507847\n",
      "iteration:  90 \teval_rewards:  5544.141007386912\n",
      "iteration:  91 \teval_rewards:  1059.438828908736\n",
      "iteration:  92 \teval_rewards:  9306.768338274036\n",
      "iteration:  93 \teval_rewards:  658.4100209652491\n",
      "iteration:  94 \teval_rewards:  9306.535475452674\n",
      "iteration:  95 \teval_rewards:  9311.60192453162\n",
      "iteration:  96 \teval_rewards:  9309.696908118005\n",
      "iteration:  97 \teval_rewards:  1505.8336474806636\n",
      "iteration:  98 \teval_rewards:  752.4778286121926\n",
      "iteration:  99 \teval_rewards:  9310.415575529929\n",
      "Iter:100| Ep_Reward:9309.259| Running_reward:1913.049| Actor_Loss:-0.143| Critic_Loss:3464.631| Iter_duration:2.768| lr:[0.00023999999999999998]\n",
      "iteration:  100 \teval_rewards:  9309.258711429911\n",
      "iteration:  101 \teval_rewards:  3469.3689695427734\n",
      "iteration:  102 \teval_rewards:  7662.536163586702\n",
      "iteration:  103 \teval_rewards:  4709.757385940454\n",
      "iteration:  104 \teval_rewards:  3314.347352149863\n",
      "iteration:  105 \teval_rewards:  6823.770311790906\n",
      "iteration:  106 \teval_rewards:  5940.111303281576\n",
      "iteration:  107 \teval_rewards:  4289.926627611635\n",
      "iteration:  108 \teval_rewards:  2428.5468591948306\n",
      "iteration:  109 \teval_rewards:  9307.992118495378\n",
      "iteration:  110 \teval_rewards:  9307.792669375138\n",
      "iteration:  111 \teval_rewards:  518.1120220196468\n",
      "iteration:  112 \teval_rewards:  2446.3804677844582\n",
      "iteration:  113 \teval_rewards:  6261.857085980902\n",
      "iteration:  114 \teval_rewards:  82.5695269242894\n",
      "iteration:  115 \teval_rewards:  3106.5684507560354\n",
      "iteration:  116 \teval_rewards:  6456.994973451733\n",
      "iteration:  117 \teval_rewards:  901.0929974824819\n",
      "iteration:  118 \teval_rewards:  9307.576284886078\n",
      "iteration:  119 \teval_rewards:  9306.18865067922\n",
      "iteration:  120 \teval_rewards:  8380.510929578984\n",
      "iteration:  121 \teval_rewards:  2043.071666074979\n",
      "iteration:  122 \teval_rewards:  9306.317284131763\n",
      "iteration:  123 \teval_rewards:  9308.068731012696\n",
      "iteration:  124 \teval_rewards:  9303.60824826117\n",
      "iteration:  125 \teval_rewards:  4931.630941634446\n",
      "iteration:  126 \teval_rewards:  9303.928106312336\n",
      "iteration:  127 \teval_rewards:  2797.875179068983\n",
      "iteration:  128 \teval_rewards:  2211.298910453756\n",
      "iteration:  129 \teval_rewards:  5395.594663470863\n",
      "iteration:  130 \teval_rewards:  9308.845570360549\n",
      "iteration:  131 \teval_rewards:  7825.920519616446\n",
      "iteration:  132 \teval_rewards:  3301.8938136364563\n",
      "iteration:  133 \teval_rewards:  9310.84888466019\n",
      "iteration:  134 \teval_rewards:  239.21357396506568\n",
      "iteration:  135 \teval_rewards:  9308.597498199246\n",
      "iteration:  136 \teval_rewards:  1957.3160335450737\n",
      "iteration:  137 \teval_rewards:  9305.23249363668\n",
      "iteration:  138 \teval_rewards:  9304.83308657807\n",
      "iteration:  139 \teval_rewards:  255.72277236428076\n",
      "iteration:  140 \teval_rewards:  332.9735438918358\n",
      "iteration:  141 \teval_rewards:  108.41965860128768\n",
      "iteration:  142 \teval_rewards:  9307.894100195857\n",
      "iteration:  143 \teval_rewards:  9305.480231819636\n",
      "iteration:  144 \teval_rewards:  9307.630500314704\n",
      "iteration:  145 \teval_rewards:  9310.355124720429\n",
      "iteration:  146 \teval_rewards:  9311.282166976995\n",
      "iteration:  147 \teval_rewards:  537.0615056076449\n",
      "iteration:  148 \teval_rewards:  9311.488415222273\n",
      "iteration:  149 \teval_rewards:  9309.721093528335\n",
      "iteration:  150 \teval_rewards:  9308.862188133779\n",
      "iteration:  151 \teval_rewards:  9307.387937966641\n",
      "iteration:  152 \teval_rewards:  9313.931559096189\n",
      "iteration:  153 \teval_rewards:  9309.420078243706\n",
      "iteration:  154 \teval_rewards:  9308.715890977071\n",
      "iteration:  155 \teval_rewards:  9312.686495238935\n",
      "iteration:  156 \teval_rewards:  9312.076590276156\n",
      "iteration:  157 \teval_rewards:  192.6892449157875\n",
      "iteration:  158 \teval_rewards:  9314.69341966017\n",
      "iteration:  159 \teval_rewards:  9309.622606897676\n",
      "iteration:  160 \teval_rewards:  9200.321520230234\n",
      "iteration:  161 \teval_rewards:  324.0278626798566\n",
      "iteration:  162 \teval_rewards:  9315.850693596143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  163 \teval_rewards:  9315.962147434388\n",
      "iteration:  164 \teval_rewards:  6874.685507227264\n",
      "iteration:  165 \teval_rewards:  9317.336091838215\n",
      "iteration:  166 \teval_rewards:  9316.639212664384\n",
      "iteration:  167 \teval_rewards:  9315.863040890195\n",
      "iteration:  168 \teval_rewards:  9318.584590704444\n",
      "iteration:  169 \teval_rewards:  8205.337259213149\n",
      "iteration:  170 \teval_rewards:  920.8386430463262\n",
      "iteration:  171 \teval_rewards:  4358.810264543855\n",
      "iteration:  172 \teval_rewards:  4610.554912077459\n",
      "iteration:  173 \teval_rewards:  9315.685483586303\n",
      "iteration:  174 \teval_rewards:  117.5451375973566\n",
      "iteration:  175 \teval_rewards:  9316.780506935445\n",
      "iteration:  176 \teval_rewards:  9317.461674761767\n",
      "iteration:  177 \teval_rewards:  9318.73281559386\n",
      "iteration:  178 \teval_rewards:  9314.937245331988\n",
      "iteration:  179 \teval_rewards:  9317.366959815405\n",
      "iteration:  180 \teval_rewards:  9317.98321525861\n",
      "iteration:  181 \teval_rewards:  9318.02587188344\n",
      "iteration:  182 \teval_rewards:  9318.750120346207\n",
      "iteration:  183 \teval_rewards:  9311.008897606747\n",
      "iteration:  184 \teval_rewards:  9314.91959555743\n",
      "iteration:  185 \teval_rewards:  9317.593288898097\n",
      "iteration:  186 \teval_rewards:  9311.213058340036\n",
      "iteration:  187 \teval_rewards:  6357.36304289034\n",
      "iteration:  188 \teval_rewards:  9310.510307141201\n",
      "iteration:  189 \teval_rewards:  9317.038729476519\n",
      "iteration:  190 \teval_rewards:  72.72528792736783\n",
      "iteration:  191 \teval_rewards:  9313.59301854561\n",
      "iteration:  192 \teval_rewards:  7774.684407137138\n",
      "iteration:  193 \teval_rewards:  3137.3517781162004\n",
      "iteration:  194 \teval_rewards:  9312.793719128085\n",
      "iteration:  195 \teval_rewards:  9316.621947901875\n",
      "iteration:  196 \teval_rewards:  9312.51806449609\n",
      "iteration:  197 \teval_rewards:  9312.395002159192\n",
      "iteration:  198 \teval_rewards:  9318.528566891064\n",
      "iteration:  199 \teval_rewards:  9316.924038616538\n",
      "Iter:200| Ep_Reward:2026.987| Running_reward:5203.842| Actor_Loss:-0.139| Critic_Loss:605.278| Iter_duration:2.436| lr:[0.00017999999999999998]\n",
      "iteration:  200 \teval_rewards:  2026.9869362355496\n",
      "iteration:  201 \teval_rewards:  9316.80619368187\n",
      "iteration:  202 \teval_rewards:  9315.29382612086\n",
      "iteration:  203 \teval_rewards:  9316.226446108803\n",
      "iteration:  204 \teval_rewards:  323.7651022846237\n",
      "iteration:  205 \teval_rewards:  9320.730616442108\n",
      "iteration:  206 \teval_rewards:  9320.516525524958\n",
      "iteration:  207 \teval_rewards:  9321.487934213186\n",
      "iteration:  208 \teval_rewards:  9323.526093331717\n",
      "iteration:  209 \teval_rewards:  9324.086059848265\n",
      "iteration:  210 \teval_rewards:  9320.294928769368\n",
      "iteration:  211 \teval_rewards:  2774.2708797953183\n",
      "iteration:  212 \teval_rewards:  9317.559415290261\n",
      "iteration:  213 \teval_rewards:  9319.678200532177\n",
      "iteration:  214 \teval_rewards:  9222.465011962424\n",
      "iteration:  215 \teval_rewards:  9320.069746552736\n",
      "iteration:  216 \teval_rewards:  9318.362048257628\n",
      "iteration:  217 \teval_rewards:  9318.709700034218\n",
      "iteration:  218 \teval_rewards:  9317.161156083825\n",
      "iteration:  219 \teval_rewards:  1217.641876429582\n",
      "iteration:  220 \teval_rewards:  417.25508986858375\n",
      "iteration:  221 \teval_rewards:  230.14574627769008\n",
      "iteration:  222 \teval_rewards:  9318.751358881917\n",
      "iteration:  223 \teval_rewards:  9316.380008594211\n",
      "iteration:  224 \teval_rewards:  9311.823377677409\n",
      "iteration:  225 \teval_rewards:  938.360916650538\n",
      "iteration:  226 \teval_rewards:  9312.705169939456\n",
      "iteration:  227 \teval_rewards:  2681.1868059576977\n",
      "iteration:  228 \teval_rewards:  9314.384389383114\n",
      "iteration:  229 \teval_rewards:  9314.776196779017\n",
      "iteration:  230 \teval_rewards:  1318.0347892140264\n",
      "iteration:  231 \teval_rewards:  9315.407886093628\n",
      "iteration:  232 \teval_rewards:  9318.252755344023\n",
      "iteration:  233 \teval_rewards:  2955.359457574693\n",
      "iteration:  234 \teval_rewards:  9312.121601031797\n",
      "iteration:  235 \teval_rewards:  154.4635812941111\n",
      "iteration:  236 \teval_rewards:  3100.2218203316406\n",
      "iteration:  237 \teval_rewards:  9317.810927344655\n",
      "iteration:  238 \teval_rewards:  6475.424894050274\n",
      "iteration:  239 \teval_rewards:  9317.473960735471\n",
      "iteration:  240 \teval_rewards:  295.4928310759108\n",
      "iteration:  241 \teval_rewards:  9320.522955355831\n",
      "iteration:  242 \teval_rewards:  9318.955135078479\n",
      "iteration:  243 \teval_rewards:  9317.296711738225\n",
      "iteration:  244 \teval_rewards:  9315.806368724325\n",
      "iteration:  245 \teval_rewards:  9314.850826676262\n",
      "iteration:  246 \teval_rewards:  9316.14675908563\n",
      "iteration:  247 \teval_rewards:  1673.669538745106\n",
      "iteration:  248 \teval_rewards:  9315.22927902574\n",
      "iteration:  249 \teval_rewards:  9314.90817658069\n",
      "iteration:  250 \teval_rewards:  5124.984316766416\n",
      "iteration:  251 \teval_rewards:  9319.917628710617\n",
      "iteration:  252 \teval_rewards:  9318.43641024994\n",
      "iteration:  253 \teval_rewards:  9317.115988198722\n",
      "iteration:  254 \teval_rewards:  9319.64010074614\n",
      "iteration:  255 \teval_rewards:  9319.273821369594\n",
      "iteration:  256 \teval_rewards:  3462.6464615844834\n",
      "iteration:  257 \teval_rewards:  3830.1867581900956\n",
      "iteration:  258 \teval_rewards:  9318.542529269283\n",
      "iteration:  259 \teval_rewards:  9320.537986286641\n",
      "iteration:  260 \teval_rewards:  9316.687248290198\n",
      "iteration:  261 \teval_rewards:  9318.141755622963\n",
      "iteration:  262 \teval_rewards:  9314.53818122625\n",
      "iteration:  263 \teval_rewards:  9315.633744523275\n",
      "iteration:  264 \teval_rewards:  1712.6560617286198\n",
      "iteration:  265 \teval_rewards:  920.6626409818441\n",
      "iteration:  266 \teval_rewards:  9317.412579191485\n",
      "iteration:  267 \teval_rewards:  9311.888854524816\n",
      "iteration:  268 \teval_rewards:  9312.91662411654\n",
      "iteration:  269 \teval_rewards:  9313.82688682275\n",
      "iteration:  270 \teval_rewards:  9316.160390635487\n",
      "iteration:  271 \teval_rewards:  146.0986286940603\n",
      "iteration:  272 \teval_rewards:  5904.623013378016\n",
      "iteration:  273 \teval_rewards:  9318.07677617919\n",
      "iteration:  274 \teval_rewards:  9318.604544715161\n",
      "iteration:  275 \teval_rewards:  9314.984247598402\n",
      "iteration:  276 \teval_rewards:  9315.386003072726\n",
      "iteration:  277 \teval_rewards:  9314.529776915631\n",
      "iteration:  278 \teval_rewards:  9317.943860133237\n",
      "iteration:  279 \teval_rewards:  6323.912058858586\n",
      "iteration:  280 \teval_rewards:  9321.034932375605\n",
      "iteration:  281 \teval_rewards:  9320.116483316608\n",
      "iteration:  282 \teval_rewards:  9315.659884423096\n",
      "iteration:  283 \teval_rewards:  9320.135864959233\n",
      "iteration:  284 \teval_rewards:  9318.377867043044\n",
      "iteration:  285 \teval_rewards:  9318.218936908244\n",
      "iteration:  286 \teval_rewards:  9315.36208236492\n",
      "iteration:  287 \teval_rewards:  9317.40215386836\n",
      "iteration:  288 \teval_rewards:  2204.167965916211\n",
      "iteration:  289 \teval_rewards:  9317.640314745613\n",
      "iteration:  290 \teval_rewards:  9314.871828608639\n",
      "iteration:  291 \teval_rewards:  9316.01382199957\n",
      "iteration:  292 \teval_rewards:  8970.441837764192\n",
      "iteration:  293 \teval_rewards:  9316.728902794683\n",
      "iteration:  294 \teval_rewards:  9316.428083975023\n",
      "iteration:  295 \teval_rewards:  9315.445081452905\n",
      "iteration:  296 \teval_rewards:  9315.457338304406\n",
      "iteration:  297 \teval_rewards:  9317.567590345345\n",
      "iteration:  298 \teval_rewards:  9314.66119412726\n",
      "iteration:  299 \teval_rewards:  9316.457572542968\n",
      "Iter:300| Ep_Reward:9315.340| Running_reward:6904.725| Actor_Loss:0.017| Critic_Loss:8911.417| Iter_duration:3.671| lr:[0.00011999999999999999]\n",
      "iteration:  300 \teval_rewards:  9315.33972467356\n",
      "iteration:  301 \teval_rewards:  9317.327207331777\n",
      "iteration:  302 \teval_rewards:  2651.6779755856664\n",
      "iteration:  303 \teval_rewards:  9315.790390746826\n",
      "iteration:  304 \teval_rewards:  9316.646788177455\n",
      "iteration:  305 \teval_rewards:  9315.950348474082\n",
      "iteration:  306 \teval_rewards:  9315.95630776271\n",
      "iteration:  307 \teval_rewards:  9317.142479248576\n",
      "iteration:  308 \teval_rewards:  9316.561531818825\n",
      "iteration:  309 \teval_rewards:  9313.630935925072\n",
      "iteration:  310 \teval_rewards:  9311.148361240774\n",
      "iteration:  311 \teval_rewards:  9314.33942056908\n",
      "iteration:  312 \teval_rewards:  9314.8169084379\n",
      "iteration:  313 \teval_rewards:  9315.992524516238\n",
      "iteration:  314 \teval_rewards:  9313.564524393521\n",
      "iteration:  315 \teval_rewards:  9317.847759020644\n",
      "iteration:  316 \teval_rewards:  9315.015090849756\n",
      "iteration:  317 \teval_rewards:  9316.058080322488\n",
      "iteration:  318 \teval_rewards:  9314.328063011259\n",
      "iteration:  319 \teval_rewards:  9315.54707009927\n",
      "iteration:  320 \teval_rewards:  9313.730672698224\n",
      "iteration:  321 \teval_rewards:  9314.529159208241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  322 \teval_rewards:  9315.888537277713\n",
      "iteration:  323 \teval_rewards:  9312.987045595499\n",
      "iteration:  324 \teval_rewards:  9310.723622021733\n",
      "iteration:  325 \teval_rewards:  9308.797166572105\n",
      "iteration:  326 \teval_rewards:  9307.798285211747\n",
      "iteration:  327 \teval_rewards:  9307.249092605087\n",
      "iteration:  328 \teval_rewards:  4010.789010446765\n",
      "iteration:  329 \teval_rewards:  99.49830482360085\n",
      "iteration:  330 \teval_rewards:  9304.62829213886\n",
      "iteration:  331 \teval_rewards:  9306.892613995646\n",
      "iteration:  332 \teval_rewards:  9303.773082331749\n",
      "iteration:  333 \teval_rewards:  369.6817429849325\n",
      "iteration:  334 \teval_rewards:  9305.400245533963\n",
      "iteration:  335 \teval_rewards:  9303.285245749512\n",
      "iteration:  336 \teval_rewards:  9301.458025479198\n",
      "iteration:  337 \teval_rewards:  4005.9247013703816\n",
      "iteration:  338 \teval_rewards:  9305.020682446457\n",
      "iteration:  339 \teval_rewards:  9302.287450501648\n",
      "iteration:  340 \teval_rewards:  9306.814041864754\n",
      "iteration:  341 \teval_rewards:  1746.3026553144034\n",
      "iteration:  342 \teval_rewards:  2405.538930263115\n",
      "iteration:  343 \teval_rewards:  9308.60065060168\n",
      "iteration:  344 \teval_rewards:  812.4721568606016\n",
      "iteration:  345 \teval_rewards:  5255.370845085795\n",
      "iteration:  346 \teval_rewards:  1271.991226167084\n",
      "iteration:  347 \teval_rewards:  4525.411216064493\n",
      "iteration:  348 \teval_rewards:  9301.192828371488\n",
      "iteration:  349 \teval_rewards:  9303.651538069405\n",
      "iteration:  350 \teval_rewards:  9305.505202332277\n",
      "iteration:  351 \teval_rewards:  8182.922360286161\n",
      "iteration:  352 \teval_rewards:  9297.932704439641\n",
      "iteration:  353 \teval_rewards:  5021.191086215283\n",
      "iteration:  354 \teval_rewards:  9302.568710449432\n",
      "iteration:  355 \teval_rewards:  8890.234672078703\n",
      "iteration:  356 \teval_rewards:  9304.462673656435\n",
      "iteration:  357 \teval_rewards:  9293.621262463676\n",
      "iteration:  358 \teval_rewards:  537.5862706820217\n",
      "iteration:  359 \teval_rewards:  8903.214662825027\n",
      "iteration:  360 \teval_rewards:  9305.565348442133\n",
      "iteration:  361 \teval_rewards:  9304.956528532046\n",
      "iteration:  362 \teval_rewards:  9305.684473191975\n",
      "iteration:  363 \teval_rewards:  1152.1437887006816\n",
      "iteration:  364 \teval_rewards:  9300.823942362655\n",
      "iteration:  365 \teval_rewards:  9301.849024051238\n",
      "iteration:  366 \teval_rewards:  9058.543951417927\n",
      "iteration:  367 \teval_rewards:  9301.382490732602\n",
      "iteration:  368 \teval_rewards:  6647.444969426083\n",
      "iteration:  369 \teval_rewards:  9303.344428439876\n",
      "iteration:  370 \teval_rewards:  6591.480266206143\n",
      "iteration:  371 \teval_rewards:  9303.583982090444\n",
      "iteration:  372 \teval_rewards:  779.181572077893\n",
      "iteration:  373 \teval_rewards:  9303.541478580657\n",
      "iteration:  374 \teval_rewards:  4919.7150991314165\n",
      "iteration:  375 \teval_rewards:  9307.133485727272\n",
      "iteration:  376 \teval_rewards:  1980.136233996914\n",
      "iteration:  377 \teval_rewards:  9307.046468964554\n",
      "iteration:  378 \teval_rewards:  9301.866459593179\n",
      "iteration:  379 \teval_rewards:  4352.8329073200375\n",
      "iteration:  380 \teval_rewards:  9307.806400357173\n",
      "iteration:  381 \teval_rewards:  9305.167671916406\n",
      "iteration:  382 \teval_rewards:  9308.073568723667\n",
      "iteration:  383 \teval_rewards:  9310.857370683507\n",
      "iteration:  384 \teval_rewards:  9309.552229605275\n",
      "iteration:  385 \teval_rewards:  9310.017825804873\n",
      "iteration:  386 \teval_rewards:  9310.421182991358\n",
      "iteration:  387 \teval_rewards:  9308.008177849482\n",
      "iteration:  388 \teval_rewards:  9308.368153819029\n",
      "iteration:  389 \teval_rewards:  9306.57987469018\n",
      "iteration:  390 \teval_rewards:  9308.616833683018\n",
      "iteration:  391 \teval_rewards:  9308.864221942084\n",
      "iteration:  392 \teval_rewards:  9308.117325349274\n",
      "iteration:  393 \teval_rewards:  9308.392408239308\n",
      "iteration:  394 \teval_rewards:  9310.233329151171\n",
      "iteration:  395 \teval_rewards:  9309.819842759438\n",
      "iteration:  396 \teval_rewards:  9307.905765747617\n",
      "iteration:  397 \teval_rewards:  3078.9069949841373\n",
      "iteration:  398 \teval_rewards:  9312.156371187606\n",
      "iteration:  399 \teval_rewards:  9301.605654825978\n",
      "Iter:400| Ep_Reward:9306.519| Running_reward:7581.897| Actor_Loss:-0.080| Critic_Loss:3979.978| Iter_duration:3.399| lr:[5.999999999999998e-05]\n",
      "iteration:  400 \teval_rewards:  9306.518645409795\n",
      "iteration:  401 \teval_rewards:  9302.656870381436\n",
      "iteration:  402 \teval_rewards:  9310.497211958274\n",
      "iteration:  403 \teval_rewards:  9307.984252546257\n",
      "iteration:  404 \teval_rewards:  9309.432375431072\n",
      "iteration:  405 \teval_rewards:  6169.105959217892\n",
      "iteration:  406 \teval_rewards:  9309.649522924916\n",
      "iteration:  407 \teval_rewards:  9308.238443213671\n",
      "iteration:  408 \teval_rewards:  9309.847380077674\n",
      "iteration:  409 \teval_rewards:  9309.696648811592\n",
      "iteration:  410 \teval_rewards:  9309.172813253366\n",
      "iteration:  411 \teval_rewards:  9309.108164693414\n",
      "iteration:  412 \teval_rewards:  9309.27883166428\n",
      "iteration:  413 \teval_rewards:  9307.682277461863\n",
      "iteration:  414 \teval_rewards:  9305.439370831113\n",
      "iteration:  415 \teval_rewards:  9307.51002263578\n",
      "iteration:  416 \teval_rewards:  9309.886138403366\n",
      "iteration:  417 \teval_rewards:  9310.947784497037\n",
      "iteration:  418 \teval_rewards:  9309.60347256059\n",
      "iteration:  419 \teval_rewards:  9304.916085389263\n",
      "iteration:  420 \teval_rewards:  9305.076952099198\n",
      "iteration:  421 \teval_rewards:  9309.385447691786\n",
      "iteration:  422 \teval_rewards:  9309.831573663134\n",
      "iteration:  423 \teval_rewards:  2342.732184528935\n",
      "iteration:  424 \teval_rewards:  9313.092967356062\n",
      "iteration:  425 \teval_rewards:  9313.247683545947\n",
      "iteration:  426 \teval_rewards:  9309.516981335912\n",
      "iteration:  427 \teval_rewards:  9311.9995136776\n",
      "iteration:  428 \teval_rewards:  6922.8541332181485\n",
      "iteration:  429 \teval_rewards:  9308.70177541179\n",
      "iteration:  430 \teval_rewards:  9305.663212512261\n",
      "iteration:  431 \teval_rewards:  9309.85852778441\n",
      "iteration:  432 \teval_rewards:  9309.795544921022\n",
      "iteration:  433 \teval_rewards:  9306.9990892614\n",
      "iteration:  434 \teval_rewards:  9308.276984371661\n",
      "iteration:  435 \teval_rewards:  9307.984179189943\n",
      "iteration:  436 \teval_rewards:  9307.863941956064\n",
      "iteration:  437 \teval_rewards:  9310.886185769004\n",
      "iteration:  438 \teval_rewards:  9306.527785808526\n",
      "iteration:  439 \teval_rewards:  9309.876055664363\n",
      "iteration:  440 \teval_rewards:  6003.711015661952\n",
      "iteration:  441 \teval_rewards:  9309.892596729876\n",
      "iteration:  442 \teval_rewards:  9310.700762132992\n",
      "iteration:  443 \teval_rewards:  9309.203526571888\n",
      "iteration:  444 \teval_rewards:  9311.56252843394\n",
      "iteration:  445 \teval_rewards:  9308.217867145273\n",
      "iteration:  446 \teval_rewards:  9310.602569004002\n",
      "iteration:  447 \teval_rewards:  9309.64649194101\n",
      "iteration:  448 \teval_rewards:  9308.84426283014\n",
      "iteration:  449 \teval_rewards:  9310.055469307435\n",
      "iteration:  450 \teval_rewards:  9311.249835423465\n",
      "iteration:  451 \teval_rewards:  9309.008278705644\n",
      "iteration:  452 \teval_rewards:  8263.405993270595\n",
      "iteration:  453 \teval_rewards:  7669.418205157269\n",
      "iteration:  454 \teval_rewards:  9308.49730045779\n",
      "iteration:  455 \teval_rewards:  9309.47620679003\n",
      "iteration:  456 \teval_rewards:  9306.811956906306\n",
      "iteration:  457 \teval_rewards:  9308.51173262226\n",
      "iteration:  458 \teval_rewards:  9310.507360880354\n",
      "iteration:  459 \teval_rewards:  8143.9825659029675\n",
      "iteration:  460 \teval_rewards:  9310.915438031305\n",
      "iteration:  461 \teval_rewards:  9312.328927899936\n",
      "iteration:  462 \teval_rewards:  9311.511796120376\n",
      "iteration:  463 \teval_rewards:  9309.669373459083\n",
      "iteration:  464 \teval_rewards:  798.4847623313923\n",
      "iteration:  465 \teval_rewards:  9307.654510446264\n",
      "iteration:  466 \teval_rewards:  9306.885323368406\n",
      "iteration:  467 \teval_rewards:  9312.08958967818\n",
      "iteration:  468 \teval_rewards:  9309.236630643314\n",
      "iteration:  469 \teval_rewards:  9313.64212082395\n",
      "iteration:  470 \teval_rewards:  9311.669390014396\n",
      "iteration:  471 \teval_rewards:  9311.800375729645\n",
      "iteration:  472 \teval_rewards:  9313.237419338882\n",
      "iteration:  473 \teval_rewards:  9313.586309483122\n",
      "iteration:  474 \teval_rewards:  9315.05776470178\n",
      "iteration:  475 \teval_rewards:  9314.201151779682\n",
      "iteration:  476 \teval_rewards:  9312.617198209826\n",
      "iteration:  477 \teval_rewards:  9312.653554309294\n",
      "iteration:  478 \teval_rewards:  3591.4279670128253\n",
      "iteration:  479 \teval_rewards:  9313.960398839898\n",
      "iteration:  480 \teval_rewards:  9316.23209467229\n",
      "iteration:  481 \teval_rewards:  9315.931619220559\n",
      "iteration:  482 \teval_rewards:  9315.687598897472\n",
      "iteration:  483 \teval_rewards:  9315.677634162568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  484 \teval_rewards:  8213.491306392212\n",
      "iteration:  485 \teval_rewards:  9314.770547919577\n",
      "iteration:  486 \teval_rewards:  9316.6979087542\n",
      "iteration:  487 \teval_rewards:  9317.636023529607\n",
      "iteration:  488 \teval_rewards:  9317.520868440082\n",
      "iteration:  489 \teval_rewards:  9319.474215519209\n",
      "iteration:  490 \teval_rewards:  9319.18852103317\n",
      "iteration:  491 \teval_rewards:  9320.319624978725\n",
      "iteration:  492 \teval_rewards:  9319.700286064564\n",
      "iteration:  493 \teval_rewards:  9318.007617019966\n",
      "iteration:  494 \teval_rewards:  9320.611505869576\n",
      "iteration:  495 \teval_rewards:  9321.761009472146\n",
      "iteration:  496 \teval_rewards:  9319.380058963781\n",
      "iteration:  497 \teval_rewards:  9320.648214920024\n",
      "iteration:  498 \teval_rewards:  9322.327200599295\n",
      "iteration:  499 \teval_rewards:  9320.498121677067\n",
      "Iter:500| Ep_Reward:9320.218| Running_reward:8466.298| Actor_Loss:-0.140| Critic_Loss:1691.802| Iter_duration:3.473| lr:[0.0]\n",
      "iteration:  500 \teval_rewards:  9320.218088668296\n",
      "episode reward:9320.583\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walker 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Walker2d\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "n_iterations = 1500\n",
    "\n",
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states:17\n",
      "action bounds:[-1.0, 1.0]\n",
      "number of actions:6\n",
      "iteration:  1 \teval_rewards:  0.5474504746026824\n",
      "iteration:  2 \teval_rewards:  0.6139819397717825\n",
      "iteration:  3 \teval_rewards:  15.773683076676134\n",
      "iteration:  4 \teval_rewards:  19.35315174005951\n",
      "iteration:  5 \teval_rewards:  31.776205368983803\n",
      "iteration:  6 \teval_rewards:  51.92884940344699\n",
      "iteration:  7 \teval_rewards:  124.46137137369442\n",
      "iteration:  8 \teval_rewards:  89.33800299587827\n",
      "iteration:  9 \teval_rewards:  37.98785101513507\n",
      "iteration:  10 \teval_rewards:  57.80770145720188\n",
      "iteration:  11 \teval_rewards:  398.0232305750942\n",
      "iteration:  12 \teval_rewards:  219.2012841307943\n",
      "iteration:  13 \teval_rewards:  316.2882735451255\n",
      "iteration:  14 \teval_rewards:  91.5811223126468\n",
      "iteration:  15 \teval_rewards:  59.94967645616366\n",
      "iteration:  16 \teval_rewards:  316.7300725306001\n",
      "iteration:  17 \teval_rewards:  482.75372123908244\n",
      "iteration:  18 \teval_rewards:  307.34882314051424\n",
      "iteration:  19 \teval_rewards:  331.29939983334435\n",
      "iteration:  20 \teval_rewards:  301.1742615336524\n",
      "iteration:  21 \teval_rewards:  376.6402810871299\n",
      "iteration:  22 \teval_rewards:  54.50847683739232\n",
      "iteration:  23 \teval_rewards:  225.8424724376022\n",
      "iteration:  24 \teval_rewards:  85.11283301095176\n",
      "iteration:  25 \teval_rewards:  264.69305420062295\n",
      "iteration:  26 \teval_rewards:  101.80136629185344\n",
      "iteration:  27 \teval_rewards:  443.8832785417097\n",
      "iteration:  28 \teval_rewards:  335.35966617120425\n",
      "iteration:  29 \teval_rewards:  371.7098934080925\n",
      "iteration:  30 \teval_rewards:  452.2855896744194\n",
      "iteration:  31 \teval_rewards:  346.3394909968723\n",
      "iteration:  32 \teval_rewards:  361.27945818164585\n",
      "iteration:  33 \teval_rewards:  325.3510988520634\n",
      "iteration:  34 \teval_rewards:  542.9441835353157\n",
      "iteration:  35 \teval_rewards:  352.44056232000514\n",
      "iteration:  36 \teval_rewards:  446.8188004273092\n",
      "iteration:  37 \teval_rewards:  301.7306753160963\n",
      "iteration:  38 \teval_rewards:  451.7311844584459\n",
      "iteration:  39 \teval_rewards:  409.5915051993828\n",
      "iteration:  40 \teval_rewards:  316.4847398534381\n",
      "iteration:  41 \teval_rewards:  420.9624814847448\n",
      "iteration:  42 \teval_rewards:  347.930781491284\n",
      "iteration:  43 \teval_rewards:  344.67953169006023\n",
      "iteration:  44 \teval_rewards:  402.3824933616361\n",
      "iteration:  45 \teval_rewards:  369.6998788658723\n",
      "iteration:  46 \teval_rewards:  441.8482856445946\n",
      "iteration:  47 \teval_rewards:  354.5344333094015\n",
      "iteration:  48 \teval_rewards:  352.84760055582126\n",
      "iteration:  49 \teval_rewards:  437.3039756776059\n",
      "iteration:  50 \teval_rewards:  370.21791284869454\n",
      "iteration:  51 \teval_rewards:  358.85820902775095\n",
      "iteration:  52 \teval_rewards:  442.68507748078133\n",
      "iteration:  53 \teval_rewards:  364.8800655727081\n",
      "iteration:  54 \teval_rewards:  452.3467152546603\n",
      "iteration:  55 \teval_rewards:  479.7036796264677\n",
      "iteration:  56 \teval_rewards:  264.48349290203464\n",
      "iteration:  57 \teval_rewards:  435.320797149117\n",
      "iteration:  58 \teval_rewards:  426.8863861724255\n",
      "iteration:  59 \teval_rewards:  446.45544996245985\n",
      "iteration:  60 \teval_rewards:  398.1964663329382\n",
      "iteration:  61 \teval_rewards:  443.74053227516384\n",
      "iteration:  62 \teval_rewards:  385.91935364890804\n",
      "iteration:  63 \teval_rewards:  469.6354877742809\n",
      "iteration:  64 \teval_rewards:  453.82205524211764\n",
      "iteration:  65 \teval_rewards:  358.50739411384865\n",
      "iteration:  66 \teval_rewards:  487.6037282565553\n",
      "iteration:  67 \teval_rewards:  484.18767651427436\n",
      "iteration:  68 \teval_rewards:  420.05898741807306\n",
      "iteration:  69 \teval_rewards:  223.85949281403788\n",
      "iteration:  70 \teval_rewards:  459.8187964413443\n",
      "iteration:  71 \teval_rewards:  324.6348517657128\n",
      "iteration:  72 \teval_rewards:  422.4056979177005\n",
      "iteration:  73 \teval_rewards:  479.44707255209585\n",
      "iteration:  74 \teval_rewards:  282.451484368281\n",
      "iteration:  75 \teval_rewards:  422.18353490144113\n",
      "iteration:  76 \teval_rewards:  482.17788999179\n",
      "iteration:  77 \teval_rewards:  444.9704891534046\n",
      "iteration:  78 \teval_rewards:  443.8499688171544\n",
      "iteration:  79 \teval_rewards:  445.77642238040545\n",
      "iteration:  80 \teval_rewards:  464.52888970301507\n",
      "iteration:  81 \teval_rewards:  484.9415634990634\n",
      "iteration:  82 \teval_rewards:  476.5609131420507\n",
      "iteration:  83 \teval_rewards:  376.24183922594756\n",
      "iteration:  84 \teval_rewards:  452.51840014936806\n",
      "iteration:  85 \teval_rewards:  455.0199362401184\n",
      "iteration:  86 \teval_rewards:  409.90359612114696\n",
      "iteration:  87 \teval_rewards:  412.45865282359483\n",
      "iteration:  88 \teval_rewards:  333.1842048785958\n",
      "iteration:  89 \teval_rewards:  224.24783727986033\n",
      "iteration:  90 \teval_rewards:  522.2913366766113\n",
      "iteration:  91 \teval_rewards:  464.9580982993599\n",
      "iteration:  92 \teval_rewards:  443.9528705978266\n",
      "iteration:  93 \teval_rewards:  458.0554273471745\n",
      "iteration:  94 \teval_rewards:  500.82146425340466\n",
      "iteration:  95 \teval_rewards:  443.85410357214647\n",
      "iteration:  96 \teval_rewards:  403.3360267879232\n",
      "iteration:  97 \teval_rewards:  365.90700766437493\n",
      "iteration:  98 \teval_rewards:  483.2692450243112\n",
      "iteration:  99 \teval_rewards:  433.0498108170547\n",
      "Iter:100| Ep_Reward:396.552| Running_reward:235.732| Actor_Loss:-0.156| Critic_Loss:10.738| Iter_duration:3.169| lr:[0.00028]\n",
      "iteration:  100 \teval_rewards:  396.5521111200651\n",
      "iteration:  101 \teval_rewards:  417.6713996895905\n",
      "iteration:  102 \teval_rewards:  490.9627280276418\n",
      "iteration:  103 \teval_rewards:  341.70896034568244\n",
      "iteration:  104 \teval_rewards:  453.5898046065161\n",
      "iteration:  105 \teval_rewards:  497.64530129710784\n",
      "iteration:  106 \teval_rewards:  554.2823772333034\n",
      "iteration:  107 \teval_rewards:  534.1963435823847\n",
      "iteration:  108 \teval_rewards:  498.495000791943\n",
      "iteration:  109 \teval_rewards:  519.7387215057364\n",
      "iteration:  110 \teval_rewards:  434.097142688096\n",
      "iteration:  111 \teval_rewards:  555.9378364127708\n",
      "iteration:  112 \teval_rewards:  492.00208011164324\n",
      "iteration:  113 \teval_rewards:  429.6317175638894\n",
      "iteration:  114 \teval_rewards:  546.5674302016367\n",
      "iteration:  115 \teval_rewards:  696.2436695554828\n",
      "iteration:  116 \teval_rewards:  561.6923251862275\n",
      "iteration:  117 \teval_rewards:  534.9991540387811\n",
      "iteration:  118 \teval_rewards:  643.9005925711871\n",
      "iteration:  119 \teval_rewards:  453.20816575617454\n",
      "iteration:  120 \teval_rewards:  1347.7433640295067\n",
      "iteration:  121 \teval_rewards:  698.3207692426327\n",
      "iteration:  122 \teval_rewards:  649.9613060866725\n",
      "iteration:  123 \teval_rewards:  652.8115646623563\n",
      "iteration:  124 \teval_rewards:  562.6173586713331\n",
      "iteration:  125 \teval_rewards:  606.701501351786\n",
      "iteration:  126 \teval_rewards:  527.2836441448703\n",
      "iteration:  127 \teval_rewards:  568.6497324257176\n",
      "iteration:  128 \teval_rewards:  542.8459197686998\n",
      "iteration:  129 \teval_rewards:  857.1354435039602\n",
      "iteration:  130 \teval_rewards:  121.79630654377911\n",
      "iteration:  131 \teval_rewards:  1288.0356246108227\n",
      "iteration:  132 \teval_rewards:  511.9075478241832\n",
      "iteration:  133 \teval_rewards:  624.5151792098001\n",
      "iteration:  134 \teval_rewards:  343.69449313087887\n",
      "iteration:  135 \teval_rewards:  574.359194154154\n",
      "iteration:  136 \teval_rewards:  646.6026984728007\n",
      "iteration:  137 \teval_rewards:  294.4978326381444\n",
      "iteration:  138 \teval_rewards:  1467.0568150889474\n",
      "iteration:  139 \teval_rewards:  117.09991925190705\n",
      "iteration:  140 \teval_rewards:  435.51005669789055\n",
      "iteration:  141 \teval_rewards:  605.5076587726687\n",
      "iteration:  142 \teval_rewards:  565.177573696768\n",
      "iteration:  143 \teval_rewards:  631.2395505069435\n",
      "iteration:  144 \teval_rewards:  714.2241284826652\n",
      "iteration:  145 \teval_rewards:  532.463242666546\n",
      "iteration:  146 \teval_rewards:  760.7851556900689\n",
      "iteration:  147 \teval_rewards:  614.3490305296282\n",
      "iteration:  148 \teval_rewards:  716.2750111306282\n",
      "iteration:  149 \teval_rewards:  553.5487836007885\n",
      "iteration:  150 \teval_rewards:  514.6030661966337\n",
      "iteration:  151 \teval_rewards:  644.6538613384864\n",
      "iteration:  152 \teval_rewards:  634.6104564097416\n",
      "iteration:  153 \teval_rewards:  615.5302833760343\n",
      "iteration:  154 \teval_rewards:  482.5230357558021\n",
      "iteration:  155 \teval_rewards:  548.2218643849773\n",
      "iteration:  156 \teval_rewards:  655.1106433851832\n",
      "iteration:  157 \teval_rewards:  700.5153024977085\n",
      "iteration:  158 \teval_rewards:  412.04995358684437\n",
      "iteration:  159 \teval_rewards:  862.1369856585155\n",
      "iteration:  160 \teval_rewards:  669.994692081618\n",
      "iteration:  161 \teval_rewards:  921.2295594226193\n",
      "iteration:  162 \teval_rewards:  653.906974086118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  163 \teval_rewards:  888.9483645152048\n",
      "iteration:  164 \teval_rewards:  486.75770694428275\n",
      "iteration:  165 \teval_rewards:  1015.5358849543604\n",
      "iteration:  166 \teval_rewards:  546.9023407497731\n",
      "iteration:  167 \teval_rewards:  676.1004398603636\n",
      "iteration:  168 \teval_rewards:  643.6662969409498\n",
      "iteration:  169 \teval_rewards:  1182.8665920716717\n",
      "iteration:  170 \teval_rewards:  695.2769996475423\n",
      "iteration:  171 \teval_rewards:  804.0799886059275\n",
      "iteration:  172 \teval_rewards:  1761.5279231806273\n",
      "iteration:  173 \teval_rewards:  995.0857910712595\n",
      "iteration:  174 \teval_rewards:  2142.5040084635475\n",
      "iteration:  175 \teval_rewards:  527.7998313297564\n",
      "iteration:  176 \teval_rewards:  1566.2782646551846\n",
      "iteration:  177 \teval_rewards:  807.0183580925667\n",
      "iteration:  178 \teval_rewards:  544.0792757393914\n",
      "iteration:  179 \teval_rewards:  632.9602496972141\n",
      "iteration:  180 \teval_rewards:  802.0993088228045\n",
      "iteration:  181 \teval_rewards:  497.9137518096788\n",
      "iteration:  182 \teval_rewards:  857.392931675635\n",
      "iteration:  183 \teval_rewards:  531.7404465263121\n",
      "iteration:  184 \teval_rewards:  744.1995553222857\n",
      "iteration:  185 \teval_rewards:  378.2446700245827\n",
      "iteration:  186 \teval_rewards:  884.0758180161774\n",
      "iteration:  187 \teval_rewards:  811.9287591111594\n",
      "iteration:  188 \teval_rewards:  646.6309777422708\n",
      "iteration:  189 \teval_rewards:  489.5928869581648\n",
      "iteration:  190 \teval_rewards:  729.9695889433043\n",
      "iteration:  191 \teval_rewards:  480.99203544375405\n",
      "iteration:  192 \teval_rewards:  533.3402530188833\n",
      "iteration:  193 \teval_rewards:  706.0175895723726\n",
      "iteration:  194 \teval_rewards:  489.20878918487597\n",
      "iteration:  195 \teval_rewards:  627.735137216927\n",
      "iteration:  196 \teval_rewards:  534.6595079688465\n",
      "iteration:  197 \teval_rewards:  933.8070453488897\n",
      "iteration:  198 \teval_rewards:  922.922672974753\n",
      "iteration:  199 \teval_rewards:  741.0464035388676\n",
      "Iter:200| Ep_Reward:742.490| Running_reward:525.643| Actor_Loss:-0.163| Critic_Loss:54.673| Iter_duration:3.220| lr:[0.00026]\n",
      "iteration:  200 \teval_rewards:  742.4896288691257\n",
      "iteration:  201 \teval_rewards:  704.5348717736215\n",
      "iteration:  202 \teval_rewards:  583.9011797555761\n",
      "iteration:  203 \teval_rewards:  429.37107556462365\n",
      "iteration:  204 \teval_rewards:  651.0377815252061\n",
      "iteration:  205 \teval_rewards:  829.020725256228\n",
      "iteration:  206 \teval_rewards:  713.9502600247063\n",
      "iteration:  207 \teval_rewards:  977.4183065559819\n",
      "iteration:  208 \teval_rewards:  739.4041098947205\n",
      "iteration:  209 \teval_rewards:  590.1448116899422\n",
      "iteration:  210 \teval_rewards:  1884.4753333334916\n",
      "iteration:  211 \teval_rewards:  581.8981442056106\n",
      "iteration:  212 \teval_rewards:  1285.9039247751332\n",
      "iteration:  213 \teval_rewards:  319.33790016579985\n",
      "iteration:  214 \teval_rewards:  1587.638450654253\n",
      "iteration:  215 \teval_rewards:  973.8315849377432\n",
      "iteration:  216 \teval_rewards:  695.5679850446606\n",
      "iteration:  217 \teval_rewards:  1058.9336194934835\n",
      "iteration:  218 \teval_rewards:  1300.0958644986988\n",
      "iteration:  219 \teval_rewards:  847.4030255125813\n",
      "iteration:  220 \teval_rewards:  623.8488123160914\n",
      "iteration:  221 \teval_rewards:  633.5386392170792\n",
      "iteration:  222 \teval_rewards:  856.190060039549\n",
      "iteration:  223 \teval_rewards:  961.2209188799535\n",
      "iteration:  224 \teval_rewards:  1074.0171347287533\n",
      "iteration:  225 \teval_rewards:  756.2519365443942\n",
      "iteration:  226 \teval_rewards:  787.2425502017803\n",
      "iteration:  227 \teval_rewards:  534.5056409670007\n",
      "iteration:  228 \teval_rewards:  618.0508472874434\n",
      "iteration:  229 \teval_rewards:  436.64748025848\n",
      "iteration:  230 \teval_rewards:  2398.830102836111\n",
      "iteration:  231 \teval_rewards:  825.5649254861476\n",
      "iteration:  232 \teval_rewards:  1879.248731389248\n",
      "iteration:  233 \teval_rewards:  1025.3507914455258\n",
      "iteration:  234 \teval_rewards:  1145.2600269070347\n",
      "iteration:  235 \teval_rewards:  640.1499016550423\n",
      "iteration:  236 \teval_rewards:  452.7264060204373\n",
      "iteration:  237 \teval_rewards:  1474.9141203554066\n",
      "iteration:  238 \teval_rewards:  589.8137590192491\n",
      "iteration:  239 \teval_rewards:  1191.904069680011\n",
      "iteration:  240 \teval_rewards:  964.3747561005822\n",
      "iteration:  241 \teval_rewards:  460.69808822570513\n",
      "iteration:  242 \teval_rewards:  757.2448575388548\n",
      "iteration:  243 \teval_rewards:  626.3216623050537\n",
      "iteration:  244 \teval_rewards:  605.5284821199465\n",
      "iteration:  245 \teval_rewards:  547.2361977350338\n",
      "iteration:  246 \teval_rewards:  1025.7920687296644\n",
      "iteration:  247 \teval_rewards:  663.4245283399033\n",
      "iteration:  248 \teval_rewards:  471.7039817682362\n",
      "iteration:  249 \teval_rewards:  451.47789632990134\n",
      "iteration:  250 \teval_rewards:  456.68621754865984\n",
      "iteration:  251 \teval_rewards:  632.3339710066049\n",
      "iteration:  252 \teval_rewards:  340.23233842853915\n",
      "iteration:  253 \teval_rewards:  622.7219712194136\n",
      "iteration:  254 \teval_rewards:  611.9398143326507\n",
      "iteration:  255 \teval_rewards:  562.1931898234989\n",
      "iteration:  256 \teval_rewards:  1116.70071356647\n",
      "iteration:  257 \teval_rewards:  462.0069088321889\n",
      "iteration:  258 \teval_rewards:  521.7816638719002\n",
      "iteration:  259 \teval_rewards:  725.5802469216246\n",
      "iteration:  260 \teval_rewards:  583.652065362437\n",
      "iteration:  261 \teval_rewards:  669.8417354940265\n",
      "iteration:  262 \teval_rewards:  1035.162840148211\n",
      "iteration:  263 \teval_rewards:  930.4000482451462\n",
      "iteration:  264 \teval_rewards:  892.2537466275029\n",
      "iteration:  265 \teval_rewards:  1442.3399813034678\n",
      "iteration:  266 \teval_rewards:  725.7030850001476\n",
      "iteration:  267 \teval_rewards:  1058.0563966965635\n",
      "iteration:  268 \teval_rewards:  722.5314086621322\n",
      "iteration:  269 \teval_rewards:  671.393171774189\n",
      "iteration:  270 \teval_rewards:  1285.775538999197\n",
      "iteration:  271 \teval_rewards:  1242.4579424726705\n",
      "iteration:  272 \teval_rewards:  666.9794373696002\n",
      "iteration:  273 \teval_rewards:  1073.151015375226\n",
      "iteration:  274 \teval_rewards:  2454.659771767408\n",
      "iteration:  275 \teval_rewards:  649.9903714875549\n",
      "iteration:  276 \teval_rewards:  1015.5072114822627\n",
      "iteration:  277 \teval_rewards:  1229.5945242035757\n",
      "iteration:  278 \teval_rewards:  2575.336843734838\n",
      "iteration:  279 \teval_rewards:  612.1575297533191\n",
      "iteration:  280 \teval_rewards:  822.8572687733301\n",
      "iteration:  281 \teval_rewards:  2130.1915270641875\n",
      "iteration:  282 \teval_rewards:  421.6969475418557\n",
      "iteration:  283 \teval_rewards:  2560.8595244891794\n",
      "iteration:  284 \teval_rewards:  646.420063637426\n",
      "iteration:  285 \teval_rewards:  675.075446187086\n",
      "iteration:  286 \teval_rewards:  768.982096900053\n",
      "iteration:  287 \teval_rewards:  695.9838364524206\n",
      "iteration:  288 \teval_rewards:  1118.002260611985\n",
      "iteration:  289 \teval_rewards:  1390.8143689791311\n",
      "iteration:  290 \teval_rewards:  888.0632473577779\n",
      "iteration:  291 \teval_rewards:  1027.5776332146543\n",
      "iteration:  292 \teval_rewards:  824.9727478995296\n",
      "iteration:  293 \teval_rewards:  2339.818614041663\n",
      "iteration:  294 \teval_rewards:  945.8787093812982\n",
      "iteration:  295 \teval_rewards:  1436.1034407217703\n",
      "iteration:  296 \teval_rewards:  659.1835723065308\n",
      "iteration:  297 \teval_rewards:  389.8822386140709\n",
      "iteration:  298 \teval_rewards:  2473.6951135178288\n",
      "iteration:  299 \teval_rewards:  671.5058134637059\n",
      "Iter:300| Ep_Reward:1277.397| Running_reward:810.238| Actor_Loss:-0.020| Critic_Loss:75.179| Iter_duration:3.425| lr:[0.00023999999999999998]\n",
      "iteration:  300 \teval_rewards:  1277.3973316320894\n",
      "iteration:  301 \teval_rewards:  1407.8545465240968\n",
      "iteration:  302 \teval_rewards:  778.3513161612782\n",
      "iteration:  303 \teval_rewards:  1210.8556553960725\n",
      "iteration:  304 \teval_rewards:  671.1060820567247\n",
      "iteration:  305 \teval_rewards:  715.9886337354861\n",
      "iteration:  306 \teval_rewards:  959.3504086310505\n",
      "iteration:  307 \teval_rewards:  1443.4526595340471\n",
      "iteration:  308 \teval_rewards:  1159.5316037092593\n",
      "iteration:  309 \teval_rewards:  1411.063794065072\n",
      "iteration:  310 \teval_rewards:  670.6067902817545\n",
      "iteration:  311 \teval_rewards:  489.61890252830074\n",
      "iteration:  312 \teval_rewards:  598.7641008767537\n",
      "iteration:  313 \teval_rewards:  479.2293152372152\n",
      "iteration:  314 \teval_rewards:  673.747304686357\n",
      "iteration:  315 \teval_rewards:  670.8856714100133\n",
      "iteration:  316 \teval_rewards:  450.32807965110504\n",
      "iteration:  317 \teval_rewards:  681.6949885720446\n",
      "iteration:  318 \teval_rewards:  434.2592058656319\n",
      "iteration:  319 \teval_rewards:  1570.8535429173023\n",
      "iteration:  320 \teval_rewards:  1422.9879440295254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  321 \teval_rewards:  1183.2623901836514\n",
      "iteration:  322 \teval_rewards:  633.741767993915\n",
      "iteration:  323 \teval_rewards:  707.1715396556481\n",
      "iteration:  324 \teval_rewards:  1371.4301789346002\n",
      "iteration:  325 \teval_rewards:  1499.7494250711206\n",
      "iteration:  326 \teval_rewards:  979.1115650947456\n",
      "iteration:  327 \teval_rewards:  602.6474870994948\n",
      "iteration:  328 \teval_rewards:  886.0581047335784\n",
      "iteration:  329 \teval_rewards:  532.251519543819\n",
      "iteration:  330 \teval_rewards:  789.2531662643623\n",
      "iteration:  331 \teval_rewards:  790.8145967565578\n",
      "iteration:  332 \teval_rewards:  730.5871386352387\n",
      "iteration:  333 \teval_rewards:  948.4826599557485\n",
      "iteration:  334 \teval_rewards:  709.9121013764268\n",
      "iteration:  335 \teval_rewards:  1686.0983302527643\n",
      "iteration:  336 \teval_rewards:  913.8888129302575\n",
      "iteration:  337 \teval_rewards:  653.7435455705041\n",
      "iteration:  338 \teval_rewards:  724.1062463242588\n",
      "iteration:  339 \teval_rewards:  851.0995302574935\n",
      "iteration:  340 \teval_rewards:  507.28224071824934\n",
      "iteration:  341 \teval_rewards:  554.9228417178203\n",
      "iteration:  342 \teval_rewards:  645.4170044672135\n",
      "iteration:  343 \teval_rewards:  552.4741163862869\n",
      "iteration:  344 \teval_rewards:  1290.2075232919847\n",
      "iteration:  345 \teval_rewards:  715.2732727733832\n",
      "iteration:  346 \teval_rewards:  801.9258447116594\n",
      "iteration:  347 \teval_rewards:  1414.4849400945195\n",
      "iteration:  348 \teval_rewards:  1419.6889045336904\n",
      "iteration:  349 \teval_rewards:  517.0314688212463\n",
      "iteration:  350 \teval_rewards:  439.2547357903258\n",
      "iteration:  351 \teval_rewards:  1186.8127074817767\n",
      "iteration:  352 \teval_rewards:  847.4973245207817\n",
      "iteration:  353 \teval_rewards:  694.0353076959466\n",
      "iteration:  354 \teval_rewards:  1022.9409595072821\n",
      "iteration:  355 \teval_rewards:  590.3712128179948\n",
      "iteration:  356 \teval_rewards:  2773.1478307594425\n",
      "iteration:  357 \teval_rewards:  725.3040164243466\n",
      "iteration:  358 \teval_rewards:  522.1960074454255\n",
      "iteration:  359 \teval_rewards:  931.3607171228308\n",
      "iteration:  360 \teval_rewards:  1948.1171574891093\n",
      "iteration:  361 \teval_rewards:  825.2091975772947\n",
      "iteration:  362 \teval_rewards:  1319.31217427011\n",
      "iteration:  363 \teval_rewards:  607.8258399915321\n",
      "iteration:  364 \teval_rewards:  1148.8822074906404\n",
      "iteration:  365 \teval_rewards:  912.653996518735\n",
      "iteration:  366 \teval_rewards:  717.102408388259\n",
      "iteration:  367 \teval_rewards:  1458.3756725431126\n",
      "iteration:  368 \teval_rewards:  1323.9418906248861\n",
      "iteration:  369 \teval_rewards:  674.5897422848043\n",
      "iteration:  370 \teval_rewards:  1931.4365956111403\n",
      "iteration:  371 \teval_rewards:  675.222929433089\n",
      "iteration:  372 \teval_rewards:  717.5955242761303\n",
      "iteration:  373 \teval_rewards:  840.9550457895772\n",
      "iteration:  374 \teval_rewards:  697.4493755369583\n",
      "iteration:  375 \teval_rewards:  340.87699101504523\n",
      "iteration:  376 \teval_rewards:  805.8949053539404\n",
      "iteration:  377 \teval_rewards:  703.6090864846503\n",
      "iteration:  378 \teval_rewards:  2088.1223554889893\n",
      "iteration:  379 \teval_rewards:  1839.0156354983221\n",
      "iteration:  380 \teval_rewards:  763.4259231126191\n",
      "iteration:  381 \teval_rewards:  885.2108335366452\n",
      "iteration:  382 \teval_rewards:  1258.3436766732473\n",
      "iteration:  383 \teval_rewards:  743.8104995492567\n",
      "iteration:  384 \teval_rewards:  898.5920481675715\n",
      "iteration:  385 \teval_rewards:  524.6413586136067\n",
      "iteration:  386 \teval_rewards:  528.2275758430955\n",
      "iteration:  387 \teval_rewards:  735.0293541978955\n",
      "iteration:  388 \teval_rewards:  1047.1454423790674\n",
      "iteration:  389 \teval_rewards:  782.995923540857\n",
      "iteration:  390 \teval_rewards:  876.0489172107274\n",
      "iteration:  391 \teval_rewards:  684.6199849480715\n",
      "iteration:  392 \teval_rewards:  1427.6227311082937\n",
      "iteration:  393 \teval_rewards:  1137.4005454707446\n",
      "iteration:  394 \teval_rewards:  1271.6473879510252\n",
      "iteration:  395 \teval_rewards:  1746.9023997762054\n",
      "iteration:  396 \teval_rewards:  717.9065484139666\n",
      "iteration:  397 \teval_rewards:  1401.1732009756388\n",
      "iteration:  398 \teval_rewards:  861.6642858876208\n",
      "iteration:  399 \teval_rewards:  1004.6468217306875\n",
      "Iter:400| Ep_Reward:917.915| Running_reward:907.981| Actor_Loss:0.015| Critic_Loss:408.129| Iter_duration:3.337| lr:[0.00022]\n",
      "iteration:  400 \teval_rewards:  917.9145852733943\n",
      "iteration:  401 \teval_rewards:  877.556836065641\n",
      "iteration:  402 \teval_rewards:  692.6590206673385\n",
      "iteration:  403 \teval_rewards:  1245.7681207459682\n",
      "iteration:  404 \teval_rewards:  1079.5627841719077\n",
      "iteration:  405 \teval_rewards:  828.7654097194028\n",
      "iteration:  406 \teval_rewards:  1001.1935117498367\n",
      "iteration:  407 \teval_rewards:  1192.087683934246\n",
      "iteration:  408 \teval_rewards:  919.0390157174288\n",
      "iteration:  409 \teval_rewards:  1536.6181609838393\n",
      "iteration:  410 \teval_rewards:  1051.6691626873867\n",
      "iteration:  411 \teval_rewards:  679.2715779459187\n",
      "iteration:  412 \teval_rewards:  1993.7206574752752\n",
      "iteration:  413 \teval_rewards:  1163.7238798336812\n",
      "iteration:  414 \teval_rewards:  743.1004917267444\n",
      "iteration:  415 \teval_rewards:  742.0513552634353\n",
      "iteration:  416 \teval_rewards:  1018.8710048134443\n",
      "iteration:  417 \teval_rewards:  688.2626705109719\n",
      "iteration:  418 \teval_rewards:  606.2833829664962\n",
      "iteration:  419 \teval_rewards:  711.8974298599985\n",
      "iteration:  420 \teval_rewards:  386.87829445963933\n",
      "iteration:  421 \teval_rewards:  652.3401901388612\n",
      "iteration:  422 \teval_rewards:  732.6055815649408\n",
      "iteration:  423 \teval_rewards:  704.0858992722162\n",
      "iteration:  424 \teval_rewards:  2075.0223009161928\n",
      "iteration:  425 \teval_rewards:  2474.481520587572\n",
      "iteration:  426 \teval_rewards:  2756.711249903717\n",
      "iteration:  427 \teval_rewards:  758.5513869615289\n",
      "iteration:  428 \teval_rewards:  945.8987483858134\n",
      "iteration:  429 \teval_rewards:  1202.2937064678522\n",
      "iteration:  430 \teval_rewards:  1202.3811179540305\n",
      "iteration:  431 \teval_rewards:  706.7327113880682\n",
      "iteration:  432 \teval_rewards:  707.5026291619744\n",
      "iteration:  433 \teval_rewards:  2667.767768225849\n",
      "iteration:  434 \teval_rewards:  447.0381990045066\n",
      "iteration:  435 \teval_rewards:  663.9180950983465\n",
      "iteration:  436 \teval_rewards:  1585.6056428122367\n",
      "iteration:  437 \teval_rewards:  769.134139291551\n",
      "iteration:  438 \teval_rewards:  928.4575627414462\n",
      "iteration:  439 \teval_rewards:  1747.9361014154736\n",
      "iteration:  440 \teval_rewards:  1332.6038526882062\n",
      "iteration:  441 \teval_rewards:  813.2294731132265\n",
      "iteration:  442 \teval_rewards:  1344.456606650433\n",
      "iteration:  443 \teval_rewards:  729.8093905951913\n",
      "iteration:  444 \teval_rewards:  816.5761806441711\n",
      "iteration:  445 \teval_rewards:  783.6375346667107\n",
      "iteration:  446 \teval_rewards:  995.9449079237386\n",
      "iteration:  447 \teval_rewards:  1681.0103900623274\n",
      "iteration:  448 \teval_rewards:  519.3062556732905\n",
      "iteration:  449 \teval_rewards:  1201.2230091713122\n",
      "iteration:  450 \teval_rewards:  578.4083590540021\n",
      "iteration:  451 \teval_rewards:  1905.731750272864\n",
      "iteration:  452 \teval_rewards:  2563.9565614774856\n",
      "iteration:  453 \teval_rewards:  861.9017572858962\n",
      "iteration:  454 \teval_rewards:  661.0921882690745\n",
      "iteration:  455 \teval_rewards:  1616.945910034345\n",
      "iteration:  456 \teval_rewards:  1775.6082923915676\n",
      "iteration:  457 \teval_rewards:  2406.471253254748\n",
      "iteration:  458 \teval_rewards:  931.8056294011433\n",
      "iteration:  459 \teval_rewards:  2351.3583558851005\n",
      "iteration:  460 \teval_rewards:  820.7428945680028\n",
      "iteration:  461 \teval_rewards:  2420.3986175440527\n",
      "iteration:  462 \teval_rewards:  729.2372272533268\n",
      "iteration:  463 \teval_rewards:  687.0465463308716\n",
      "iteration:  464 \teval_rewards:  796.9189359063207\n",
      "iteration:  465 \teval_rewards:  539.9036306901718\n",
      "iteration:  466 \teval_rewards:  1871.359887825797\n",
      "iteration:  467 \teval_rewards:  669.8433130773726\n",
      "iteration:  468 \teval_rewards:  721.1206762548761\n",
      "iteration:  469 \teval_rewards:  707.6412718408914\n",
      "iteration:  470 \teval_rewards:  770.3334764807419\n",
      "iteration:  471 \teval_rewards:  460.84920717013495\n",
      "iteration:  472 \teval_rewards:  993.3776082252672\n",
      "iteration:  473 \teval_rewards:  551.3493315627005\n",
      "iteration:  474 \teval_rewards:  889.2625386354284\n",
      "iteration:  475 \teval_rewards:  2149.967586278895\n",
      "iteration:  476 \teval_rewards:  1338.2087586606733\n",
      "iteration:  477 \teval_rewards:  774.0694751684729\n",
      "iteration:  478 \teval_rewards:  503.3509535223948\n",
      "iteration:  479 \teval_rewards:  700.7700924827484\n",
      "iteration:  480 \teval_rewards:  834.3700993921316\n",
      "iteration:  481 \teval_rewards:  624.2240465079192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  482 \teval_rewards:  647.0222968909992\n",
      "iteration:  483 \teval_rewards:  1114.5742233184785\n",
      "iteration:  484 \teval_rewards:  2772.5330016801204\n",
      "iteration:  485 \teval_rewards:  1366.6079812292865\n",
      "iteration:  486 \teval_rewards:  1373.367843586199\n",
      "iteration:  487 \teval_rewards:  656.9415617521548\n",
      "iteration:  488 \teval_rewards:  959.3114813546408\n",
      "iteration:  489 \teval_rewards:  2073.1990802598734\n",
      "iteration:  490 \teval_rewards:  1337.5094362200318\n",
      "iteration:  491 \teval_rewards:  631.4912604837631\n",
      "iteration:  492 \teval_rewards:  830.0482123801143\n",
      "iteration:  493 \teval_rewards:  707.9863908070498\n",
      "iteration:  494 \teval_rewards:  957.9265810700514\n",
      "iteration:  495 \teval_rewards:  1449.6795479068483\n",
      "iteration:  496 \teval_rewards:  1541.1122517394351\n",
      "iteration:  497 \teval_rewards:  2499.4234322375473\n",
      "iteration:  498 \teval_rewards:  1955.2041051581\n",
      "iteration:  499 \teval_rewards:  2476.883586202698\n",
      "Iter:500| Ep_Reward:1213.307| Running_reward:1082.151| Actor_Loss:0.020| Critic_Loss:109.441| Iter_duration:3.325| lr:[0.0002]\n",
      "iteration:  500 \teval_rewards:  1213.3069339481299\n",
      "iteration:  501 \teval_rewards:  512.9973448311611\n",
      "iteration:  502 \teval_rewards:  1120.7950647755526\n",
      "iteration:  503 \teval_rewards:  2231.911741214734\n",
      "iteration:  504 \teval_rewards:  948.793235237021\n",
      "iteration:  505 \teval_rewards:  745.3497456155488\n",
      "iteration:  506 \teval_rewards:  608.2316181206204\n",
      "iteration:  507 \teval_rewards:  747.4008528389311\n",
      "iteration:  508 \teval_rewards:  1466.8661243253412\n",
      "iteration:  509 \teval_rewards:  1302.6658031724344\n",
      "iteration:  510 \teval_rewards:  705.4787797039694\n",
      "iteration:  511 \teval_rewards:  843.9858731209181\n",
      "iteration:  512 \teval_rewards:  1276.068316539867\n",
      "iteration:  513 \teval_rewards:  1348.2755985790025\n",
      "iteration:  514 \teval_rewards:  532.7002191520971\n",
      "iteration:  515 \teval_rewards:  1041.161594510867\n",
      "iteration:  516 \teval_rewards:  688.053494519733\n",
      "iteration:  517 \teval_rewards:  2510.431967286535\n",
      "iteration:  518 \teval_rewards:  507.41643168047113\n",
      "iteration:  519 \teval_rewards:  615.255305249326\n",
      "iteration:  520 \teval_rewards:  480.8984436250658\n",
      "iteration:  521 \teval_rewards:  579.1567159157795\n",
      "iteration:  522 \teval_rewards:  1966.3524430691086\n",
      "iteration:  523 \teval_rewards:  290.7102615321167\n",
      "iteration:  524 \teval_rewards:  716.0525359794183\n",
      "iteration:  525 \teval_rewards:  491.0281076172615\n",
      "iteration:  526 \teval_rewards:  1769.7589071147577\n",
      "iteration:  527 \teval_rewards:  486.9445666892269\n",
      "iteration:  528 \teval_rewards:  907.0587850274114\n",
      "iteration:  529 \teval_rewards:  1855.4020793155223\n",
      "iteration:  530 \teval_rewards:  2566.825329795175\n",
      "iteration:  531 \teval_rewards:  1127.2261591348274\n",
      "iteration:  532 \teval_rewards:  1746.1682615306536\n",
      "iteration:  533 \teval_rewards:  480.9468155765919\n",
      "iteration:  534 \teval_rewards:  1173.7096461624542\n",
      "iteration:  535 \teval_rewards:  1803.371652193803\n",
      "iteration:  536 \teval_rewards:  1084.2756908992633\n",
      "iteration:  537 \teval_rewards:  191.33844865262904\n",
      "iteration:  538 \teval_rewards:  800.352128366024\n",
      "iteration:  539 \teval_rewards:  2538.2407678802933\n",
      "iteration:  540 \teval_rewards:  670.5532438248975\n",
      "iteration:  541 \teval_rewards:  2469.299712532214\n",
      "iteration:  542 \teval_rewards:  556.0427178023327\n",
      "iteration:  543 \teval_rewards:  1915.4206550527679\n",
      "iteration:  544 \teval_rewards:  608.2301267738242\n",
      "iteration:  545 \teval_rewards:  2664.468052039496\n",
      "iteration:  546 \teval_rewards:  585.7991625055698\n",
      "iteration:  547 \teval_rewards:  494.50650541473817\n",
      "iteration:  548 \teval_rewards:  539.1307796877311\n",
      "iteration:  549 \teval_rewards:  1812.792181358428\n",
      "iteration:  550 \teval_rewards:  2529.39887130253\n",
      "iteration:  551 \teval_rewards:  523.4963482297665\n",
      "iteration:  552 \teval_rewards:  1000.6171287862429\n",
      "iteration:  553 \teval_rewards:  1681.3241234572013\n",
      "iteration:  554 \teval_rewards:  1206.5417252498157\n",
      "iteration:  555 \teval_rewards:  1105.9872357289664\n",
      "iteration:  556 \teval_rewards:  524.7772293187302\n",
      "iteration:  557 \teval_rewards:  1578.9711829097607\n",
      "iteration:  558 \teval_rewards:  1418.0428629158082\n",
      "iteration:  559 \teval_rewards:  1804.209036551697\n",
      "iteration:  560 \teval_rewards:  625.0707024286287\n",
      "iteration:  561 \teval_rewards:  1247.4441868705158\n",
      "iteration:  562 \teval_rewards:  950.2229090934381\n",
      "iteration:  563 \teval_rewards:  1597.505862897453\n",
      "iteration:  564 \teval_rewards:  430.95615215508326\n",
      "iteration:  565 \teval_rewards:  1282.1407353457141\n",
      "iteration:  566 \teval_rewards:  1921.5049450206216\n",
      "iteration:  567 \teval_rewards:  973.5834937145016\n",
      "iteration:  568 \teval_rewards:  1449.081020473523\n",
      "iteration:  569 \teval_rewards:  233.5116709863598\n",
      "iteration:  570 \teval_rewards:  2747.582056636369\n",
      "iteration:  571 \teval_rewards:  1297.6941813098947\n",
      "iteration:  572 \teval_rewards:  547.9713623551876\n",
      "iteration:  573 \teval_rewards:  1460.2397555802872\n",
      "iteration:  574 \teval_rewards:  211.11879819642587\n",
      "iteration:  575 \teval_rewards:  469.6412756250097\n",
      "iteration:  576 \teval_rewards:  2482.690228037653\n",
      "iteration:  577 \teval_rewards:  475.6420059007544\n",
      "iteration:  578 \teval_rewards:  1070.002017027572\n",
      "iteration:  579 \teval_rewards:  2534.0034758792385\n",
      "iteration:  580 \teval_rewards:  801.0589670519445\n",
      "iteration:  581 \teval_rewards:  814.4445821509279\n",
      "iteration:  582 \teval_rewards:  2683.08884004033\n",
      "iteration:  583 \teval_rewards:  1239.3204898803053\n",
      "iteration:  584 \teval_rewards:  2609.4522293346918\n",
      "iteration:  585 \teval_rewards:  515.1404465918167\n",
      "iteration:  586 \teval_rewards:  405.2042392213821\n",
      "iteration:  587 \teval_rewards:  1050.5857887445366\n",
      "iteration:  588 \teval_rewards:  516.2811672199608\n",
      "iteration:  589 \teval_rewards:  472.08254951767907\n",
      "iteration:  590 \teval_rewards:  1879.3909631044614\n",
      "iteration:  591 \teval_rewards:  2623.4996534687484\n",
      "iteration:  592 \teval_rewards:  526.4228043074984\n",
      "iteration:  593 \teval_rewards:  2563.590915014906\n",
      "iteration:  594 \teval_rewards:  1103.8490107498737\n",
      "iteration:  595 \teval_rewards:  728.5253331389486\n",
      "iteration:  596 \teval_rewards:  889.996102211718\n",
      "iteration:  597 \teval_rewards:  1449.8500302448458\n",
      "iteration:  598 \teval_rewards:  633.6815194266012\n",
      "iteration:  599 \teval_rewards:  1626.852608101855\n",
      "Iter:600| Ep_Reward:1324.397| Running_reward:1163.375| Actor_Loss:0.064| Critic_Loss:157.893| Iter_duration:3.437| lr:[0.00017999999999999998]\n",
      "iteration:  600 \teval_rewards:  1324.3974922542209\n",
      "iteration:  601 \teval_rewards:  2607.4452474668683\n",
      "iteration:  602 \teval_rewards:  2506.3529540102168\n",
      "iteration:  603 \teval_rewards:  2718.530847204045\n",
      "iteration:  604 \teval_rewards:  1596.1988234507317\n",
      "iteration:  605 \teval_rewards:  618.2503618537964\n",
      "iteration:  606 \teval_rewards:  334.39108120600935\n",
      "iteration:  607 \teval_rewards:  2293.8265192594035\n",
      "iteration:  608 \teval_rewards:  1785.7458992696777\n",
      "iteration:  609 \teval_rewards:  748.9032762145991\n",
      "iteration:  610 \teval_rewards:  2631.64756254886\n",
      "iteration:  611 \teval_rewards:  2641.3700032387496\n",
      "iteration:  612 \teval_rewards:  1327.5727243250167\n",
      "iteration:  613 \teval_rewards:  1832.9012140152677\n",
      "iteration:  614 \teval_rewards:  497.8183253565109\n",
      "iteration:  615 \teval_rewards:  573.579931579518\n",
      "iteration:  616 \teval_rewards:  2591.6521207260575\n",
      "iteration:  617 \teval_rewards:  2637.540049859224\n",
      "iteration:  618 \teval_rewards:  676.083658260264\n",
      "iteration:  619 \teval_rewards:  2608.2080082436582\n",
      "iteration:  620 \teval_rewards:  795.7887231258749\n",
      "iteration:  621 \teval_rewards:  1187.309367187951\n",
      "iteration:  622 \teval_rewards:  2690.2815370371686\n",
      "iteration:  623 \teval_rewards:  881.7727604879207\n",
      "iteration:  624 \teval_rewards:  2659.7842630388755\n",
      "iteration:  625 \teval_rewards:  663.915987165416\n",
      "iteration:  626 \teval_rewards:  610.4058932155265\n",
      "iteration:  627 \teval_rewards:  682.1977017652099\n",
      "iteration:  628 \teval_rewards:  802.6535876588486\n",
      "iteration:  629 \teval_rewards:  838.2261797253169\n",
      "iteration:  630 \teval_rewards:  823.4472343938322\n",
      "iteration:  631 \teval_rewards:  1567.9389857207739\n",
      "iteration:  632 \teval_rewards:  1815.1409957413614\n",
      "iteration:  633 \teval_rewards:  2726.159924244382\n",
      "iteration:  634 \teval_rewards:  1939.3053880137743\n",
      "iteration:  635 \teval_rewards:  2573.8472099149076\n",
      "iteration:  636 \teval_rewards:  543.950384069557\n",
      "iteration:  637 \teval_rewards:  1164.107815817255\n",
      "iteration:  638 \teval_rewards:  512.5770794447862\n",
      "iteration:  639 \teval_rewards:  1578.87687140584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  640 \teval_rewards:  1519.2794354847126\n",
      "iteration:  641 \teval_rewards:  1598.9398810048788\n",
      "iteration:  642 \teval_rewards:  329.20334954751786\n",
      "iteration:  643 \teval_rewards:  536.3997899946355\n",
      "iteration:  644 \teval_rewards:  2623.497891435181\n",
      "iteration:  645 \teval_rewards:  2770.6628659599087\n",
      "iteration:  646 \teval_rewards:  1684.8987985676747\n",
      "iteration:  647 \teval_rewards:  2125.7445452666193\n",
      "iteration:  648 \teval_rewards:  2112.7270642334765\n",
      "iteration:  649 \teval_rewards:  1138.6353686476748\n",
      "iteration:  650 \teval_rewards:  525.6522553661557\n",
      "iteration:  651 \teval_rewards:  1985.612493395937\n",
      "iteration:  652 \teval_rewards:  2582.2832149304413\n",
      "iteration:  653 \teval_rewards:  794.2654476342407\n",
      "iteration:  654 \teval_rewards:  1931.3518032204463\n",
      "iteration:  655 \teval_rewards:  1039.4047608042763\n",
      "iteration:  656 \teval_rewards:  2850.1505785430036\n",
      "iteration:  657 \teval_rewards:  747.6339768032875\n",
      "iteration:  658 \teval_rewards:  1443.0226984422939\n",
      "iteration:  659 \teval_rewards:  621.9891145764345\n",
      "iteration:  660 \teval_rewards:  1319.0201600178793\n",
      "iteration:  661 \teval_rewards:  1060.2499978008789\n",
      "iteration:  662 \teval_rewards:  1782.951748165876\n",
      "iteration:  663 \teval_rewards:  1401.0537980224715\n",
      "iteration:  664 \teval_rewards:  381.3271840340836\n",
      "iteration:  665 \teval_rewards:  1357.9034140864485\n",
      "iteration:  666 \teval_rewards:  2376.4137350178507\n",
      "iteration:  667 \teval_rewards:  2748.8839864090837\n",
      "iteration:  668 \teval_rewards:  1721.1039832516487\n",
      "iteration:  669 \teval_rewards:  1850.8001713369479\n",
      "iteration:  670 \teval_rewards:  1338.7969646326908\n",
      "iteration:  671 \teval_rewards:  2121.8222106433345\n",
      "iteration:  672 \teval_rewards:  655.9953723594274\n",
      "iteration:  673 \teval_rewards:  1623.2495417199375\n",
      "iteration:  674 \teval_rewards:  2250.2856583619755\n",
      "iteration:  675 \teval_rewards:  490.78416244087464\n",
      "iteration:  676 \teval_rewards:  545.4318136135316\n",
      "iteration:  677 \teval_rewards:  339.3492781636158\n",
      "iteration:  678 \teval_rewards:  431.7235305371264\n",
      "iteration:  679 \teval_rewards:  591.3434666012573\n",
      "iteration:  680 \teval_rewards:  2805.6740266301686\n",
      "iteration:  681 \teval_rewards:  1087.6288207014284\n",
      "iteration:  682 \teval_rewards:  2863.413576731765\n",
      "iteration:  683 \teval_rewards:  1035.0701958822367\n",
      "iteration:  684 \teval_rewards:  2214.3610275777837\n",
      "iteration:  685 \teval_rewards:  1777.648261173909\n",
      "iteration:  686 \teval_rewards:  2011.45900389174\n",
      "iteration:  687 \teval_rewards:  1092.188579693653\n",
      "iteration:  688 \teval_rewards:  1118.9785223762353\n",
      "iteration:  689 \teval_rewards:  2820.334758163283\n",
      "iteration:  690 \teval_rewards:  2498.540188936028\n",
      "iteration:  691 \teval_rewards:  2591.9049307474147\n",
      "iteration:  692 \teval_rewards:  1141.1684288587012\n",
      "iteration:  693 \teval_rewards:  1388.1020425506285\n",
      "iteration:  694 \teval_rewards:  490.86241074354956\n",
      "iteration:  695 \teval_rewards:  2261.372445022054\n",
      "iteration:  696 \teval_rewards:  1485.5846434144007\n",
      "iteration:  697 \teval_rewards:  944.0518216232097\n",
      "iteration:  698 \teval_rewards:  752.1652662247062\n",
      "iteration:  699 \teval_rewards:  552.399409085668\n",
      "Iter:700| Ep_Reward:486.728| Running_reward:1370.158| Actor_Loss:-0.028| Critic_Loss:112.076| Iter_duration:3.132| lr:[0.00015999999999999999]\n",
      "iteration:  700 \teval_rewards:  486.7278802910111\n",
      "iteration:  701 \teval_rewards:  1922.6274327253989\n",
      "iteration:  702 \teval_rewards:  2727.849387357481\n",
      "iteration:  703 \teval_rewards:  982.1215345312424\n",
      "iteration:  704 \teval_rewards:  2463.6824448754996\n",
      "iteration:  705 \teval_rewards:  2045.3671480989474\n",
      "iteration:  706 \teval_rewards:  311.7412591861553\n",
      "iteration:  707 \teval_rewards:  1169.7256409587117\n",
      "iteration:  708 \teval_rewards:  862.292364428167\n",
      "iteration:  709 \teval_rewards:  920.091987608423\n",
      "iteration:  710 \teval_rewards:  2606.152413364783\n",
      "iteration:  711 \teval_rewards:  2291.979710929352\n",
      "iteration:  712 \teval_rewards:  656.8249048240026\n",
      "iteration:  713 \teval_rewards:  2594.7481329947423\n",
      "iteration:  714 \teval_rewards:  645.1550520880088\n",
      "iteration:  715 \teval_rewards:  436.12651538914037\n",
      "iteration:  716 \teval_rewards:  829.0937590240762\n",
      "iteration:  717 \teval_rewards:  632.6031372711087\n",
      "iteration:  718 \teval_rewards:  1092.2608081731726\n",
      "iteration:  719 \teval_rewards:  2637.0140354665714\n",
      "iteration:  720 \teval_rewards:  2431.656248878565\n",
      "iteration:  721 \teval_rewards:  809.6180101288662\n",
      "iteration:  722 \teval_rewards:  1443.489614493938\n",
      "iteration:  723 \teval_rewards:  1714.916763090177\n",
      "iteration:  724 \teval_rewards:  457.2290670494474\n",
      "iteration:  725 \teval_rewards:  1264.9697321559659\n",
      "iteration:  726 \teval_rewards:  623.3470923382481\n",
      "iteration:  727 \teval_rewards:  1265.5266306648116\n",
      "iteration:  728 \teval_rewards:  1477.0164702074308\n",
      "iteration:  729 \teval_rewards:  1928.8875021308886\n",
      "iteration:  730 \teval_rewards:  2659.8284743006584\n",
      "iteration:  731 \teval_rewards:  768.804613365479\n",
      "iteration:  732 \teval_rewards:  2657.5923832279022\n",
      "iteration:  733 \teval_rewards:  1016.4187840747885\n",
      "iteration:  734 \teval_rewards:  1900.9444751834656\n",
      "iteration:  735 \teval_rewards:  2761.7199650979323\n",
      "iteration:  736 \teval_rewards:  1998.4985990431312\n",
      "iteration:  737 \teval_rewards:  2750.8189587569523\n",
      "iteration:  738 \teval_rewards:  569.0022157300918\n",
      "iteration:  739 \teval_rewards:  724.4817223767628\n",
      "iteration:  740 \teval_rewards:  1218.9801980217017\n",
      "iteration:  741 \teval_rewards:  2397.1390261776837\n",
      "iteration:  742 \teval_rewards:  1564.5743966032314\n",
      "iteration:  743 \teval_rewards:  1622.270160282363\n",
      "iteration:  744 \teval_rewards:  745.5980697301543\n",
      "iteration:  745 \teval_rewards:  1636.2812845347287\n",
      "iteration:  746 \teval_rewards:  2824.975857216035\n",
      "iteration:  747 \teval_rewards:  2649.0525295899547\n",
      "iteration:  748 \teval_rewards:  636.0064208243624\n",
      "iteration:  749 \teval_rewards:  2669.879292897324\n",
      "iteration:  750 \teval_rewards:  2735.8572525765208\n",
      "iteration:  751 \teval_rewards:  2377.470395209872\n",
      "iteration:  752 \teval_rewards:  973.9497606849493\n",
      "iteration:  753 \teval_rewards:  2691.623422128797\n",
      "iteration:  754 \teval_rewards:  2740.605081414652\n",
      "iteration:  755 \teval_rewards:  2692.0576954325584\n",
      "iteration:  756 \teval_rewards:  2712.548958330648\n",
      "iteration:  757 \teval_rewards:  766.2766188445839\n",
      "iteration:  758 \teval_rewards:  1005.5464918673166\n",
      "iteration:  759 \teval_rewards:  514.2521989332016\n",
      "iteration:  760 \teval_rewards:  1367.5336455542013\n",
      "iteration:  761 \teval_rewards:  497.57294714360404\n",
      "iteration:  762 \teval_rewards:  605.7105475116157\n",
      "iteration:  763 \teval_rewards:  1303.3395689527508\n",
      "iteration:  764 \teval_rewards:  1268.5588132328078\n",
      "iteration:  765 \teval_rewards:  697.2602339383496\n",
      "iteration:  766 \teval_rewards:  594.5276616943281\n",
      "iteration:  767 \teval_rewards:  2329.9422864763596\n",
      "iteration:  768 \teval_rewards:  632.1779213724287\n",
      "iteration:  769 \teval_rewards:  817.5296642694344\n",
      "iteration:  770 \teval_rewards:  2581.0448406850983\n",
      "iteration:  771 \teval_rewards:  439.6283053027793\n",
      "iteration:  772 \teval_rewards:  461.4955678142418\n",
      "iteration:  773 \teval_rewards:  2016.4915295824892\n",
      "iteration:  774 \teval_rewards:  611.9869907806554\n",
      "iteration:  775 \teval_rewards:  633.6942426812702\n",
      "iteration:  776 \teval_rewards:  2807.662121659918\n",
      "iteration:  777 \teval_rewards:  2355.402376329434\n",
      "iteration:  778 \teval_rewards:  505.2779751331835\n",
      "iteration:  779 \teval_rewards:  977.699914202857\n",
      "iteration:  780 \teval_rewards:  1585.5051473775502\n",
      "iteration:  781 \teval_rewards:  535.586273920057\n",
      "iteration:  782 \teval_rewards:  536.549867965659\n",
      "iteration:  783 \teval_rewards:  1626.8749502326848\n",
      "iteration:  784 \teval_rewards:  914.9270037941394\n",
      "iteration:  785 \teval_rewards:  1049.6244261748209\n",
      "iteration:  786 \teval_rewards:  957.9569075079462\n",
      "iteration:  787 \teval_rewards:  725.2689627418846\n",
      "iteration:  788 \teval_rewards:  2744.036911294263\n",
      "iteration:  789 \teval_rewards:  1599.9313166222028\n",
      "iteration:  790 \teval_rewards:  552.3841584190756\n",
      "iteration:  791 \teval_rewards:  660.7628419397845\n",
      "iteration:  792 \teval_rewards:  2015.1848366401914\n",
      "iteration:  793 \teval_rewards:  648.2406245916451\n",
      "iteration:  794 \teval_rewards:  2265.308869159267\n",
      "iteration:  795 \teval_rewards:  482.00758941219704\n",
      "iteration:  796 \teval_rewards:  519.5441019099326\n",
      "iteration:  797 \teval_rewards:  537.1723821926748\n",
      "iteration:  798 \teval_rewards:  1219.3655206832684\n",
      "iteration:  799 \teval_rewards:  500.7797519546238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:800| Ep_Reward:383.394| Running_reward:1364.743| Actor_Loss:-0.161| Critic_Loss:169.423| Iter_duration:3.459| lr:[0.00014]\n",
      "iteration:  800 \teval_rewards:  383.39358294773416\n",
      "iteration:  801 \teval_rewards:  571.4568996882704\n",
      "iteration:  802 \teval_rewards:  604.6024958141738\n",
      "iteration:  803 \teval_rewards:  1068.9886736586418\n",
      "iteration:  804 \teval_rewards:  589.3925225826565\n",
      "iteration:  805 \teval_rewards:  429.7587073986207\n",
      "iteration:  806 \teval_rewards:  455.5200392808458\n",
      "iteration:  807 \teval_rewards:  424.5053323739997\n",
      "iteration:  808 \teval_rewards:  459.988648066267\n",
      "iteration:  809 \teval_rewards:  817.6047864876816\n",
      "iteration:  810 \teval_rewards:  561.5560505974241\n",
      "iteration:  811 \teval_rewards:  2718.2151765269223\n",
      "iteration:  812 \teval_rewards:  698.7572611482726\n",
      "iteration:  813 \teval_rewards:  437.4765068420921\n",
      "iteration:  814 \teval_rewards:  829.3624326581235\n",
      "iteration:  815 \teval_rewards:  1746.107151997406\n",
      "iteration:  816 \teval_rewards:  745.7621421974696\n",
      "iteration:  817 \teval_rewards:  430.28169861134967\n",
      "iteration:  818 \teval_rewards:  1228.2587948612158\n",
      "iteration:  819 \teval_rewards:  353.1043843759735\n",
      "iteration:  820 \teval_rewards:  2891.5112289272374\n",
      "iteration:  821 \teval_rewards:  531.2980587324136\n",
      "iteration:  822 \teval_rewards:  1140.5911018639772\n",
      "iteration:  823 \teval_rewards:  1572.3382250124855\n",
      "iteration:  824 \teval_rewards:  665.3158571087376\n",
      "iteration:  825 \teval_rewards:  2246.0991015588334\n",
      "iteration:  826 \teval_rewards:  1923.9908272351238\n",
      "iteration:  827 \teval_rewards:  387.7900833808332\n",
      "iteration:  828 \teval_rewards:  1825.9307363747564\n",
      "iteration:  829 \teval_rewards:  466.4407579627138\n",
      "iteration:  830 \teval_rewards:  604.3757655239893\n",
      "iteration:  831 \teval_rewards:  545.4131209059567\n",
      "iteration:  832 \teval_rewards:  514.0135311143257\n",
      "iteration:  833 \teval_rewards:  2332.8211080668184\n",
      "iteration:  834 \teval_rewards:  641.8922059592838\n",
      "iteration:  835 \teval_rewards:  1716.370726395164\n",
      "iteration:  836 \teval_rewards:  187.13784986106234\n",
      "iteration:  837 \teval_rewards:  640.6923135206212\n",
      "iteration:  838 \teval_rewards:  2779.809952703167\n",
      "iteration:  839 \teval_rewards:  429.1587736355895\n",
      "iteration:  840 \teval_rewards:  819.4655948619221\n",
      "iteration:  841 \teval_rewards:  493.9759922792455\n",
      "iteration:  842 \teval_rewards:  532.4410770746063\n",
      "iteration:  843 \teval_rewards:  929.8841616794277\n",
      "iteration:  844 \teval_rewards:  603.3655792486526\n",
      "iteration:  845 \teval_rewards:  428.0507023969172\n",
      "iteration:  846 \teval_rewards:  1023.6322493176194\n",
      "iteration:  847 \teval_rewards:  1109.1518349505802\n",
      "iteration:  848 \teval_rewards:  551.5607193133942\n",
      "iteration:  849 \teval_rewards:  658.2998828279842\n",
      "iteration:  850 \teval_rewards:  816.1070565945774\n",
      "iteration:  851 \teval_rewards:  788.1100251022465\n",
      "iteration:  852 \teval_rewards:  341.46678303830083\n",
      "iteration:  853 \teval_rewards:  1318.9045189213984\n",
      "iteration:  854 \teval_rewards:  970.142728815565\n",
      "iteration:  855 \teval_rewards:  646.5506331484248\n",
      "iteration:  856 \teval_rewards:  649.766972929436\n",
      "iteration:  857 \teval_rewards:  1032.877138089567\n",
      "iteration:  858 \teval_rewards:  667.51382339061\n",
      "iteration:  859 \teval_rewards:  2561.957679853551\n",
      "iteration:  860 \teval_rewards:  708.4808411255061\n",
      "iteration:  861 \teval_rewards:  661.166801831469\n",
      "iteration:  862 \teval_rewards:  554.5611048630494\n",
      "iteration:  863 \teval_rewards:  580.7474427426497\n",
      "iteration:  864 \teval_rewards:  707.2966143428273\n",
      "iteration:  865 \teval_rewards:  690.9711698966163\n",
      "iteration:  866 \teval_rewards:  1222.8419699093504\n",
      "iteration:  867 \teval_rewards:  800.3016411721878\n",
      "iteration:  868 \teval_rewards:  555.1552422946991\n",
      "iteration:  869 \teval_rewards:  513.5893168192378\n",
      "iteration:  870 \teval_rewards:  1559.723071198151\n",
      "iteration:  871 \teval_rewards:  436.2742155322582\n",
      "iteration:  872 \teval_rewards:  631.1428344235417\n",
      "iteration:  873 \teval_rewards:  1119.15192891335\n",
      "iteration:  874 \teval_rewards:  848.6381422807275\n",
      "iteration:  875 \teval_rewards:  640.6289431851009\n",
      "iteration:  876 \teval_rewards:  2357.606585472615\n",
      "iteration:  877 \teval_rewards:  997.1457552668103\n",
      "iteration:  878 \teval_rewards:  284.82582882209476\n",
      "iteration:  879 \teval_rewards:  597.1372941209901\n",
      "iteration:  880 \teval_rewards:  1366.6793148746933\n",
      "iteration:  881 \teval_rewards:  2648.647081151177\n",
      "iteration:  882 \teval_rewards:  1548.7258514982307\n",
      "iteration:  883 \teval_rewards:  1593.4724428842983\n",
      "iteration:  884 \teval_rewards:  2856.3042247467993\n",
      "iteration:  885 \teval_rewards:  639.5605571362731\n",
      "iteration:  886 \teval_rewards:  1199.4953535568695\n",
      "iteration:  887 \teval_rewards:  686.0591620028848\n",
      "iteration:  888 \teval_rewards:  2065.682160741679\n",
      "iteration:  889 \teval_rewards:  1150.7699696211478\n",
      "iteration:  890 \teval_rewards:  799.4801429260337\n",
      "iteration:  891 \teval_rewards:  1507.9761193325298\n",
      "iteration:  892 \teval_rewards:  636.795266560364\n",
      "iteration:  893 \teval_rewards:  989.5035731356752\n",
      "iteration:  894 \teval_rewards:  897.7106652788663\n",
      "iteration:  895 \teval_rewards:  2125.1169048666807\n",
      "iteration:  896 \teval_rewards:  238.54446888841915\n",
      "iteration:  897 \teval_rewards:  587.1490676807251\n",
      "iteration:  898 \teval_rewards:  1452.0404783884828\n",
      "iteration:  899 \teval_rewards:  1076.8015377385752\n",
      "Iter:900| Ep_Reward:1094.511| Running_reward:1147.699| Actor_Loss:-0.156| Critic_Loss:189.057| Iter_duration:3.392| lr:[0.00011999999999999999]\n",
      "iteration:  900 \teval_rewards:  1094.51082928546\n",
      "iteration:  901 \teval_rewards:  204.97690751549425\n",
      "iteration:  902 \teval_rewards:  561.840461115984\n",
      "iteration:  903 \teval_rewards:  729.3225776890129\n",
      "iteration:  904 \teval_rewards:  809.2631155412853\n",
      "iteration:  905 \teval_rewards:  1269.693112514936\n",
      "iteration:  906 \teval_rewards:  522.3053506631185\n",
      "iteration:  907 \teval_rewards:  1006.5602297180313\n",
      "iteration:  908 \teval_rewards:  483.8370495749115\n",
      "iteration:  909 \teval_rewards:  1053.712269363256\n",
      "iteration:  910 \teval_rewards:  547.2475509516802\n",
      "iteration:  911 \teval_rewards:  802.0819488315008\n",
      "iteration:  912 \teval_rewards:  2193.227032537657\n",
      "iteration:  913 \teval_rewards:  2451.21797106162\n",
      "iteration:  914 \teval_rewards:  513.3752111779398\n",
      "iteration:  915 \teval_rewards:  399.0846108939377\n",
      "iteration:  916 \teval_rewards:  514.947291538111\n",
      "iteration:  917 \teval_rewards:  1766.3037534125365\n",
      "iteration:  918 \teval_rewards:  607.6766260384342\n",
      "iteration:  919 \teval_rewards:  1686.2281596961855\n",
      "iteration:  920 \teval_rewards:  661.5189612808913\n",
      "iteration:  921 \teval_rewards:  561.1226355718899\n",
      "iteration:  922 \teval_rewards:  889.1763171046912\n",
      "iteration:  923 \teval_rewards:  571.1248269436464\n",
      "iteration:  924 \teval_rewards:  766.5620596446407\n",
      "iteration:  925 \teval_rewards:  529.3580127543034\n",
      "iteration:  926 \teval_rewards:  1713.268961452273\n",
      "iteration:  927 \teval_rewards:  1459.5154975631003\n",
      "iteration:  928 \teval_rewards:  1585.6377248997183\n",
      "iteration:  929 \teval_rewards:  584.9336471490227\n",
      "iteration:  930 \teval_rewards:  2647.919898408627\n",
      "iteration:  931 \teval_rewards:  2301.2257253307057\n",
      "iteration:  932 \teval_rewards:  2791.401128537485\n",
      "iteration:  933 \teval_rewards:  695.4144451001467\n",
      "iteration:  934 \teval_rewards:  1158.6514055048713\n",
      "iteration:  935 \teval_rewards:  804.341187803572\n",
      "iteration:  936 \teval_rewards:  1659.8075661761222\n",
      "iteration:  937 \teval_rewards:  1233.4611425545947\n",
      "iteration:  938 \teval_rewards:  646.5634737943578\n",
      "iteration:  939 \teval_rewards:  622.3063551991798\n",
      "iteration:  940 \teval_rewards:  545.5090939713314\n",
      "iteration:  941 \teval_rewards:  449.1014106006457\n",
      "iteration:  942 \teval_rewards:  2372.349444529078\n",
      "iteration:  943 \teval_rewards:  1962.6965582339121\n",
      "iteration:  944 \teval_rewards:  541.1164251921803\n",
      "iteration:  945 \teval_rewards:  1895.6802932067428\n",
      "iteration:  946 \teval_rewards:  2039.1328279662591\n",
      "iteration:  947 \teval_rewards:  619.3802621321676\n",
      "iteration:  948 \teval_rewards:  1056.5916194602692\n",
      "iteration:  949 \teval_rewards:  3084.8104855238494\n",
      "iteration:  950 \teval_rewards:  1778.94870340678\n",
      "iteration:  951 \teval_rewards:  520.0482663133381\n",
      "iteration:  952 \teval_rewards:  928.4274289017103\n",
      "iteration:  953 \teval_rewards:  1320.3865130535864\n",
      "iteration:  954 \teval_rewards:  679.795387397044\n",
      "iteration:  955 \teval_rewards:  2937.431633070627\n",
      "iteration:  956 \teval_rewards:  1727.992218185834\n",
      "iteration:  957 \teval_rewards:  756.131075516481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  958 \teval_rewards:  620.7755603177059\n",
      "iteration:  959 \teval_rewards:  538.3061593959194\n",
      "iteration:  960 \teval_rewards:  1790.8649061158553\n",
      "iteration:  961 \teval_rewards:  1760.044991122646\n",
      "iteration:  962 \teval_rewards:  522.3623615102218\n",
      "iteration:  963 \teval_rewards:  1939.1363589199832\n",
      "iteration:  964 \teval_rewards:  1451.1840842303131\n",
      "iteration:  965 \teval_rewards:  534.5302852902357\n",
      "iteration:  966 \teval_rewards:  553.788990643812\n",
      "iteration:  967 \teval_rewards:  590.4616635159093\n",
      "iteration:  968 \teval_rewards:  654.2919143600149\n",
      "iteration:  969 \teval_rewards:  1632.0601666371856\n",
      "iteration:  970 \teval_rewards:  546.9297138655221\n",
      "iteration:  971 \teval_rewards:  140.43172700036507\n",
      "iteration:  972 \teval_rewards:  505.1783933275028\n",
      "iteration:  973 \teval_rewards:  738.5813432600992\n",
      "iteration:  974 \teval_rewards:  1275.407676507795\n",
      "iteration:  975 \teval_rewards:  494.5683054257121\n",
      "iteration:  976 \teval_rewards:  2626.460478287207\n",
      "iteration:  977 \teval_rewards:  623.7723425596467\n",
      "iteration:  978 \teval_rewards:  1732.2319321935918\n",
      "iteration:  979 \teval_rewards:  1586.3496134924983\n",
      "iteration:  980 \teval_rewards:  555.9051968177255\n",
      "iteration:  981 \teval_rewards:  2524.5204160885473\n",
      "iteration:  982 \teval_rewards:  655.7686316052177\n",
      "iteration:  983 \teval_rewards:  887.3356955812155\n",
      "iteration:  984 \teval_rewards:  1299.6255859369298\n",
      "iteration:  985 \teval_rewards:  793.3713474434408\n",
      "iteration:  986 \teval_rewards:  945.3000969621755\n",
      "iteration:  987 \teval_rewards:  2519.752190689352\n",
      "iteration:  988 \teval_rewards:  975.9174342272017\n",
      "iteration:  989 \teval_rewards:  585.4404609282476\n",
      "iteration:  990 \teval_rewards:  1900.8090867955495\n",
      "iteration:  991 \teval_rewards:  674.6368836042983\n",
      "iteration:  992 \teval_rewards:  1626.1533715835078\n",
      "iteration:  993 \teval_rewards:  1904.268410393117\n",
      "iteration:  994 \teval_rewards:  689.2033973310778\n",
      "iteration:  995 \teval_rewards:  1211.3994192715786\n",
      "iteration:  996 \teval_rewards:  597.5984159965744\n",
      "iteration:  997 \teval_rewards:  1739.9041936172393\n",
      "iteration:  998 \teval_rewards:  616.4457237223781\n",
      "iteration:  999 \teval_rewards:  596.3846890167542\n",
      "Iter:1000| Ep_Reward:2771.943| Running_reward:1168.375| Actor_Loss:-0.129| Critic_Loss:181.223| Iter_duration:3.737| lr:[0.0001]\n",
      "iteration:  1000 \teval_rewards:  2771.9428804972085\n",
      "iteration:  1001 \teval_rewards:  684.5914434380684\n",
      "iteration:  1002 \teval_rewards:  1634.758273877401\n",
      "iteration:  1003 \teval_rewards:  833.1210180810582\n",
      "iteration:  1004 \teval_rewards:  2284.9760914348467\n",
      "iteration:  1005 \teval_rewards:  2358.8926505410905\n",
      "iteration:  1006 \teval_rewards:  516.2891231711329\n",
      "iteration:  1007 \teval_rewards:  662.2059054613583\n",
      "iteration:  1008 \teval_rewards:  2974.969063373899\n",
      "iteration:  1009 \teval_rewards:  644.2197813325796\n",
      "iteration:  1010 \teval_rewards:  640.3906790717218\n",
      "iteration:  1011 \teval_rewards:  703.011968150013\n",
      "iteration:  1012 \teval_rewards:  1539.7861719791842\n",
      "iteration:  1013 \teval_rewards:  785.5266252422257\n",
      "iteration:  1014 \teval_rewards:  858.6523843400219\n",
      "iteration:  1015 \teval_rewards:  2884.065731398655\n",
      "iteration:  1016 \teval_rewards:  1251.331911237336\n",
      "iteration:  1017 \teval_rewards:  2093.643437685275\n",
      "iteration:  1018 \teval_rewards:  2265.3225301029715\n",
      "iteration:  1019 \teval_rewards:  1706.995454373142\n",
      "iteration:  1020 \teval_rewards:  605.2811198167948\n",
      "iteration:  1021 \teval_rewards:  2618.8503426356115\n",
      "iteration:  1022 \teval_rewards:  911.5337460302701\n",
      "iteration:  1023 \teval_rewards:  899.3119640350632\n",
      "iteration:  1024 \teval_rewards:  1344.3871234696123\n",
      "iteration:  1025 \teval_rewards:  1615.8459424880461\n",
      "iteration:  1026 \teval_rewards:  1440.925915637048\n",
      "iteration:  1027 \teval_rewards:  2440.943068762158\n",
      "iteration:  1028 \teval_rewards:  1737.3118177296446\n",
      "iteration:  1029 \teval_rewards:  1722.6254963109127\n",
      "iteration:  1030 \teval_rewards:  656.6745881358414\n",
      "iteration:  1031 \teval_rewards:  975.9446117054418\n",
      "iteration:  1032 \teval_rewards:  611.9318238427528\n",
      "iteration:  1033 \teval_rewards:  2832.9045285394163\n",
      "iteration:  1034 \teval_rewards:  2510.3420027997804\n",
      "iteration:  1035 \teval_rewards:  1293.0780437080991\n",
      "iteration:  1036 \teval_rewards:  2345.275911477226\n",
      "iteration:  1037 \teval_rewards:  655.8344721937432\n",
      "iteration:  1038 \teval_rewards:  1304.46659025844\n",
      "iteration:  1039 \teval_rewards:  2604.4938891330385\n",
      "iteration:  1040 \teval_rewards:  2284.1240731570783\n",
      "iteration:  1041 \teval_rewards:  1278.7480156230208\n",
      "iteration:  1042 \teval_rewards:  338.16784309595505\n",
      "iteration:  1043 \teval_rewards:  1646.072258859769\n",
      "iteration:  1044 \teval_rewards:  1589.5358590631743\n",
      "iteration:  1045 \teval_rewards:  998.6057473473101\n",
      "iteration:  1046 \teval_rewards:  1373.828152514152\n",
      "iteration:  1047 \teval_rewards:  648.7530303437616\n",
      "iteration:  1048 \teval_rewards:  737.5925203028961\n",
      "iteration:  1049 \teval_rewards:  721.3124934873832\n",
      "iteration:  1050 \teval_rewards:  781.8674932708508\n",
      "iteration:  1051 \teval_rewards:  1813.4636465611688\n",
      "iteration:  1052 \teval_rewards:  1385.2272052894264\n",
      "iteration:  1053 \teval_rewards:  932.9007815842373\n",
      "iteration:  1054 \teval_rewards:  768.4773830114096\n",
      "iteration:  1055 \teval_rewards:  2681.4968207022\n",
      "iteration:  1056 \teval_rewards:  627.8846740809073\n",
      "iteration:  1057 \teval_rewards:  1038.119948388461\n",
      "iteration:  1058 \teval_rewards:  1317.787814941091\n",
      "iteration:  1059 \teval_rewards:  2998.1831505154823\n",
      "iteration:  1060 \teval_rewards:  1185.7822238625731\n",
      "iteration:  1061 \teval_rewards:  571.3980418741007\n",
      "iteration:  1062 \teval_rewards:  2039.7494349558183\n",
      "iteration:  1063 \teval_rewards:  2819.372695769954\n",
      "iteration:  1064 \teval_rewards:  1609.425304741525\n",
      "iteration:  1065 \teval_rewards:  496.9531276881696\n",
      "iteration:  1066 \teval_rewards:  1736.0380664961606\n",
      "iteration:  1067 \teval_rewards:  2524.225469611673\n",
      "iteration:  1068 \teval_rewards:  1621.1665379256415\n",
      "iteration:  1069 \teval_rewards:  2362.8970483086673\n",
      "iteration:  1070 \teval_rewards:  1089.8959405220423\n",
      "iteration:  1071 \teval_rewards:  1664.87832810409\n",
      "iteration:  1072 \teval_rewards:  726.4477638275847\n",
      "iteration:  1073 \teval_rewards:  1927.8978497084886\n",
      "iteration:  1074 \teval_rewards:  2839.8702104343465\n",
      "iteration:  1075 \teval_rewards:  1649.840956672697\n",
      "iteration:  1076 \teval_rewards:  3029.8665215254373\n",
      "iteration:  1077 \teval_rewards:  571.4773666568553\n",
      "iteration:  1078 \teval_rewards:  891.5989912737491\n",
      "iteration:  1079 \teval_rewards:  984.8384419012953\n",
      "iteration:  1080 \teval_rewards:  714.0417050142828\n",
      "iteration:  1081 \teval_rewards:  773.7967783017644\n",
      "iteration:  1082 \teval_rewards:  897.5631587019037\n",
      "iteration:  1083 \teval_rewards:  1096.2673963577784\n",
      "iteration:  1084 \teval_rewards:  908.8470810330339\n",
      "iteration:  1085 \teval_rewards:  1166.489295941253\n",
      "iteration:  1086 \teval_rewards:  1320.7402263440938\n",
      "iteration:  1087 \teval_rewards:  1779.0081409707946\n",
      "iteration:  1088 \teval_rewards:  609.3681138561133\n",
      "iteration:  1089 \teval_rewards:  1570.3414950420304\n",
      "iteration:  1090 \teval_rewards:  1230.6273307279898\n",
      "iteration:  1091 \teval_rewards:  1386.389393794485\n",
      "iteration:  1092 \teval_rewards:  1928.7882303445767\n",
      "iteration:  1093 \teval_rewards:  1986.4428457105018\n",
      "iteration:  1094 \teval_rewards:  695.830047331558\n",
      "iteration:  1095 \teval_rewards:  2709.1789739629166\n",
      "iteration:  1096 \teval_rewards:  1361.95183163056\n",
      "iteration:  1097 \teval_rewards:  634.7011649913205\n",
      "iteration:  1098 \teval_rewards:  471.7229222196692\n",
      "iteration:  1099 \teval_rewards:  920.6609738906106\n",
      "Iter:1100| Ep_Reward:877.908| Running_reward:1316.622| Actor_Loss:-0.021| Critic_Loss:182.784| Iter_duration:3.241| lr:[8e-05]\n",
      "iteration:  1100 \teval_rewards:  877.9076131556877\n",
      "iteration:  1101 \teval_rewards:  1290.404801285231\n",
      "iteration:  1102 \teval_rewards:  587.2091284379331\n",
      "iteration:  1103 \teval_rewards:  822.6472791684531\n",
      "iteration:  1104 \teval_rewards:  1509.5335670937181\n",
      "iteration:  1105 \teval_rewards:  535.1053765024019\n",
      "iteration:  1106 \teval_rewards:  901.1019161267911\n",
      "iteration:  1107 \teval_rewards:  1011.7936216330668\n",
      "iteration:  1108 \teval_rewards:  943.5077930043551\n",
      "iteration:  1109 \teval_rewards:  788.5641676013187\n",
      "iteration:  1110 \teval_rewards:  1000.5483649169535\n",
      "iteration:  1111 \teval_rewards:  558.5859165117055\n",
      "iteration:  1112 \teval_rewards:  621.8537506364597\n",
      "iteration:  1113 \teval_rewards:  886.9850335099117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1114 \teval_rewards:  390.28824860313546\n",
      "iteration:  1115 \teval_rewards:  2975.5123941041093\n",
      "iteration:  1116 \teval_rewards:  476.32788435549884\n",
      "iteration:  1117 \teval_rewards:  1435.927271278861\n",
      "iteration:  1118 \teval_rewards:  1094.3190416246196\n",
      "iteration:  1119 \teval_rewards:  900.980701970632\n",
      "iteration:  1120 \teval_rewards:  873.061647839036\n",
      "iteration:  1121 \teval_rewards:  644.714178030683\n",
      "iteration:  1122 \teval_rewards:  453.5628295371118\n",
      "iteration:  1123 \teval_rewards:  622.5684898571757\n",
      "iteration:  1124 \teval_rewards:  615.987071206417\n",
      "iteration:  1125 \teval_rewards:  854.8583860956161\n",
      "iteration:  1126 \teval_rewards:  2913.5909148741803\n",
      "iteration:  1127 \teval_rewards:  783.649533067914\n",
      "iteration:  1128 \teval_rewards:  561.5582865908829\n",
      "iteration:  1129 \teval_rewards:  557.6209507376399\n",
      "iteration:  1130 \teval_rewards:  2695.1944309606965\n",
      "iteration:  1131 \teval_rewards:  2290.8775188590134\n",
      "iteration:  1132 \teval_rewards:  572.0325087425513\n",
      "iteration:  1133 \teval_rewards:  858.7653806924037\n",
      "iteration:  1134 \teval_rewards:  392.3399198363614\n",
      "iteration:  1135 \teval_rewards:  1219.4035171419127\n",
      "iteration:  1136 \teval_rewards:  727.858215454622\n",
      "iteration:  1137 \teval_rewards:  1316.7299437986485\n",
      "iteration:  1138 \teval_rewards:  587.303552745395\n",
      "iteration:  1139 \teval_rewards:  569.0061002320992\n",
      "iteration:  1140 \teval_rewards:  1746.1915694643287\n",
      "iteration:  1141 \teval_rewards:  317.969016207632\n",
      "iteration:  1142 \teval_rewards:  1194.797809645294\n",
      "iteration:  1143 \teval_rewards:  556.6223802695123\n",
      "iteration:  1144 \teval_rewards:  481.4164176297934\n",
      "iteration:  1145 \teval_rewards:  1255.2514376012957\n",
      "iteration:  1146 \teval_rewards:  546.563825971446\n",
      "iteration:  1147 \teval_rewards:  3084.7192576793627\n",
      "iteration:  1148 \teval_rewards:  1457.6964087853428\n",
      "iteration:  1149 \teval_rewards:  875.4737938929029\n",
      "iteration:  1150 \teval_rewards:  586.285404838372\n",
      "iteration:  1151 \teval_rewards:  928.3607136155815\n",
      "iteration:  1152 \teval_rewards:  2585.0831306673463\n",
      "iteration:  1153 \teval_rewards:  943.0856329433778\n",
      "iteration:  1154 \teval_rewards:  847.4319227317278\n",
      "iteration:  1155 \teval_rewards:  716.5097668447938\n",
      "iteration:  1156 \teval_rewards:  1413.0737671259383\n",
      "iteration:  1157 \teval_rewards:  1215.1055517958966\n",
      "iteration:  1158 \teval_rewards:  2380.4953379182452\n",
      "iteration:  1159 \teval_rewards:  1428.0393056406097\n",
      "iteration:  1160 \teval_rewards:  618.0344372061398\n",
      "iteration:  1161 \teval_rewards:  644.4213631641841\n",
      "iteration:  1162 \teval_rewards:  612.8365734792698\n",
      "iteration:  1163 \teval_rewards:  1647.8209222220084\n",
      "iteration:  1164 \teval_rewards:  2375.2103324134337\n",
      "iteration:  1165 \teval_rewards:  594.1698203386653\n",
      "iteration:  1166 \teval_rewards:  580.8171090122243\n",
      "iteration:  1167 \teval_rewards:  1147.4991766242383\n",
      "iteration:  1168 \teval_rewards:  932.9132924144131\n",
      "iteration:  1169 \teval_rewards:  1479.3040440442724\n",
      "iteration:  1170 \teval_rewards:  666.787876455713\n",
      "iteration:  1171 \teval_rewards:  2663.573135321216\n",
      "iteration:  1172 \teval_rewards:  434.20092099752026\n",
      "iteration:  1173 \teval_rewards:  1465.6112097903413\n",
      "iteration:  1174 \teval_rewards:  1787.3324719579828\n",
      "iteration:  1175 \teval_rewards:  2016.869706726642\n",
      "iteration:  1176 \teval_rewards:  506.5722508474796\n",
      "iteration:  1177 \teval_rewards:  997.6028336434342\n",
      "iteration:  1178 \teval_rewards:  453.8584475143758\n",
      "iteration:  1179 \teval_rewards:  2989.5810100783724\n",
      "iteration:  1180 \teval_rewards:  1380.068238257206\n",
      "iteration:  1181 \teval_rewards:  639.0793723799441\n",
      "iteration:  1182 \teval_rewards:  2010.8075919211267\n",
      "iteration:  1183 \teval_rewards:  912.7265530225046\n",
      "iteration:  1184 \teval_rewards:  1557.6087617292676\n",
      "iteration:  1185 \teval_rewards:  1209.3791873801806\n",
      "iteration:  1186 \teval_rewards:  571.6580895026366\n",
      "iteration:  1187 \teval_rewards:  1455.1723412073497\n",
      "iteration:  1188 \teval_rewards:  1362.4569003032154\n",
      "iteration:  1189 \teval_rewards:  637.227435903061\n",
      "iteration:  1190 \teval_rewards:  552.5122413259144\n",
      "iteration:  1191 \teval_rewards:  608.0579704330241\n",
      "iteration:  1192 \teval_rewards:  1539.9398477380937\n",
      "iteration:  1193 \teval_rewards:  1538.3469414231754\n",
      "iteration:  1194 \teval_rewards:  610.752552274726\n",
      "iteration:  1195 \teval_rewards:  511.8324180994621\n",
      "iteration:  1196 \teval_rewards:  810.7461743981364\n",
      "iteration:  1197 \teval_rewards:  584.364065360799\n",
      "iteration:  1198 \teval_rewards:  693.7167658029357\n",
      "iteration:  1199 \teval_rewards:  1420.9897170356319\n",
      "Iter:1200| Ep_Reward:2008.261| Running_reward:1194.144| Actor_Loss:0.072| Critic_Loss:258.623| Iter_duration:3.410| lr:[5.999999999999998e-05]\n",
      "iteration:  1200 \teval_rewards:  2008.2608437225713\n",
      "iteration:  1201 \teval_rewards:  1212.9775733536226\n",
      "iteration:  1202 \teval_rewards:  518.6761582072556\n",
      "iteration:  1203 \teval_rewards:  1855.4092544576058\n",
      "iteration:  1204 \teval_rewards:  996.4483116260105\n",
      "iteration:  1205 \teval_rewards:  977.0127782082485\n",
      "iteration:  1206 \teval_rewards:  1299.130703854182\n",
      "iteration:  1207 \teval_rewards:  569.1310238360526\n",
      "iteration:  1208 \teval_rewards:  1916.500976343011\n",
      "iteration:  1209 \teval_rewards:  526.9598207255791\n",
      "iteration:  1210 \teval_rewards:  1297.5779258051202\n",
      "iteration:  1211 \teval_rewards:  571.7032877647862\n",
      "iteration:  1212 \teval_rewards:  1355.7470202153045\n",
      "iteration:  1213 \teval_rewards:  1094.8015349864186\n",
      "iteration:  1214 \teval_rewards:  467.0285549528327\n",
      "iteration:  1215 \teval_rewards:  550.6072211604956\n",
      "iteration:  1216 \teval_rewards:  589.1655651349672\n",
      "iteration:  1217 \teval_rewards:  600.9794014229013\n",
      "iteration:  1218 \teval_rewards:  234.29870193852213\n",
      "iteration:  1219 \teval_rewards:  635.204416640686\n",
      "iteration:  1220 \teval_rewards:  1265.6065216996005\n",
      "iteration:  1221 \teval_rewards:  1233.5578573849123\n",
      "iteration:  1222 \teval_rewards:  573.2055239592851\n",
      "iteration:  1223 \teval_rewards:  1886.25716608205\n",
      "iteration:  1224 \teval_rewards:  914.2123584258152\n",
      "iteration:  1225 \teval_rewards:  580.4300325500225\n",
      "iteration:  1226 \teval_rewards:  1810.3271136228143\n",
      "iteration:  1227 \teval_rewards:  1025.1239206861549\n",
      "iteration:  1228 \teval_rewards:  883.2459228679122\n",
      "iteration:  1229 \teval_rewards:  547.5817979313825\n",
      "iteration:  1230 \teval_rewards:  578.4324045007154\n",
      "iteration:  1231 \teval_rewards:  1618.2848233965778\n",
      "iteration:  1232 \teval_rewards:  1468.8337297255086\n",
      "iteration:  1233 \teval_rewards:  1281.8617826588154\n",
      "iteration:  1234 \teval_rewards:  1125.4951276236452\n",
      "iteration:  1235 \teval_rewards:  659.7999058389858\n",
      "iteration:  1236 \teval_rewards:  381.01831311013785\n",
      "iteration:  1237 \teval_rewards:  2738.587789097832\n",
      "iteration:  1238 \teval_rewards:  244.42512538213242\n",
      "iteration:  1239 \teval_rewards:  849.9763715155624\n",
      "iteration:  1240 \teval_rewards:  1898.8467045709554\n",
      "iteration:  1241 \teval_rewards:  2804.7072220682\n",
      "iteration:  1242 \teval_rewards:  786.4929119628147\n",
      "iteration:  1243 \teval_rewards:  907.59898149499\n",
      "iteration:  1244 \teval_rewards:  1091.8497307067466\n",
      "iteration:  1245 \teval_rewards:  2120.3662203910294\n",
      "iteration:  1246 \teval_rewards:  985.1743870434491\n",
      "iteration:  1247 \teval_rewards:  1832.798364599768\n",
      "iteration:  1248 \teval_rewards:  2754.2488277900698\n",
      "iteration:  1249 \teval_rewards:  580.8373544353926\n",
      "iteration:  1250 \teval_rewards:  2869.030502176541\n",
      "iteration:  1251 \teval_rewards:  621.9880239803852\n",
      "iteration:  1252 \teval_rewards:  779.3173730723997\n",
      "iteration:  1253 \teval_rewards:  3096.1098898815444\n",
      "iteration:  1254 \teval_rewards:  1638.957750697159\n",
      "iteration:  1255 \teval_rewards:  613.9414718056415\n",
      "iteration:  1256 \teval_rewards:  1573.3299796693593\n",
      "iteration:  1257 \teval_rewards:  1968.4441398477304\n",
      "iteration:  1258 \teval_rewards:  1583.103105380347\n",
      "iteration:  1259 \teval_rewards:  843.4143625415463\n",
      "iteration:  1260 \teval_rewards:  709.7125024592517\n",
      "iteration:  1261 \teval_rewards:  890.3141266729938\n",
      "iteration:  1262 \teval_rewards:  2530.3041066695832\n",
      "iteration:  1263 \teval_rewards:  542.9038909247349\n",
      "iteration:  1264 \teval_rewards:  2061.5855737378797\n",
      "iteration:  1265 \teval_rewards:  3051.9643259505806\n",
      "iteration:  1266 \teval_rewards:  480.9517779701573\n",
      "iteration:  1267 \teval_rewards:  431.68996595699326\n",
      "iteration:  1268 \teval_rewards:  1669.0299302725355\n",
      "iteration:  1269 \teval_rewards:  2506.1251564756103\n",
      "iteration:  1270 \teval_rewards:  1175.5448019218368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1271 \teval_rewards:  2510.4112005129464\n",
      "iteration:  1272 \teval_rewards:  1062.1900221486183\n",
      "iteration:  1273 \teval_rewards:  434.10638696913605\n",
      "iteration:  1274 \teval_rewards:  1622.0548236442348\n",
      "iteration:  1275 \teval_rewards:  919.0123852533609\n",
      "iteration:  1276 \teval_rewards:  2558.0660307260223\n",
      "iteration:  1277 \teval_rewards:  492.72495539947806\n",
      "iteration:  1278 \teval_rewards:  1219.2272919556792\n",
      "iteration:  1279 \teval_rewards:  1906.7329515432798\n",
      "iteration:  1280 \teval_rewards:  782.9354971568578\n",
      "iteration:  1281 \teval_rewards:  531.1370116188046\n",
      "iteration:  1282 \teval_rewards:  587.9531406054048\n",
      "iteration:  1283 \teval_rewards:  812.3936100991125\n",
      "iteration:  1284 \teval_rewards:  1059.104862145263\n",
      "iteration:  1285 \teval_rewards:  627.5585590685806\n",
      "iteration:  1286 \teval_rewards:  3138.4201091493005\n",
      "iteration:  1287 \teval_rewards:  557.2938204318643\n",
      "iteration:  1288 \teval_rewards:  590.4267152199\n",
      "iteration:  1289 \teval_rewards:  1970.18013562641\n",
      "iteration:  1290 \teval_rewards:  778.0792605462625\n",
      "iteration:  1291 \teval_rewards:  1421.718988583491\n",
      "iteration:  1292 \teval_rewards:  1163.0176934636145\n",
      "iteration:  1293 \teval_rewards:  507.9940071476854\n",
      "iteration:  1294 \teval_rewards:  791.7728771393467\n",
      "iteration:  1295 \teval_rewards:  511.61136846685815\n",
      "iteration:  1296 \teval_rewards:  1251.1933571821921\n",
      "iteration:  1297 \teval_rewards:  3054.9910788542084\n",
      "iteration:  1298 \teval_rewards:  918.3782975457999\n",
      "iteration:  1299 \teval_rewards:  948.7170405773854\n",
      "Iter:1300| Ep_Reward:637.507| Running_reward:1219.668| Actor_Loss:-0.013| Critic_Loss:153.183| Iter_duration:3.174| lr:[3.999999999999999e-05]\n",
      "iteration:  1300 \teval_rewards:  637.5066727168314\n",
      "iteration:  1301 \teval_rewards:  1495.0162239755841\n",
      "iteration:  1302 \teval_rewards:  842.7642108323695\n",
      "iteration:  1303 \teval_rewards:  1739.1199584092235\n",
      "iteration:  1304 \teval_rewards:  3106.2925636934806\n",
      "iteration:  1305 \teval_rewards:  402.16350126939625\n",
      "iteration:  1306 \teval_rewards:  885.5117738843978\n",
      "iteration:  1307 \teval_rewards:  1246.193614756612\n",
      "iteration:  1308 \teval_rewards:  631.9888861703301\n",
      "iteration:  1309 \teval_rewards:  994.7072866544804\n",
      "iteration:  1310 \teval_rewards:  1720.076597989749\n",
      "iteration:  1311 \teval_rewards:  847.2045762383933\n",
      "iteration:  1312 \teval_rewards:  1162.877836585618\n",
      "iteration:  1313 \teval_rewards:  1089.265495158093\n",
      "iteration:  1314 \teval_rewards:  564.3496070699399\n",
      "iteration:  1315 \teval_rewards:  953.2470514376135\n",
      "iteration:  1316 \teval_rewards:  2140.961973397414\n",
      "iteration:  1317 \teval_rewards:  581.7892469121833\n",
      "iteration:  1318 \teval_rewards:  1748.967279561155\n",
      "iteration:  1319 \teval_rewards:  3100.078710064144\n",
      "iteration:  1320 \teval_rewards:  620.3283485421861\n",
      "iteration:  1321 \teval_rewards:  2197.051947211731\n",
      "iteration:  1322 \teval_rewards:  2097.6048129606033\n",
      "iteration:  1323 \teval_rewards:  610.2891558155738\n",
      "iteration:  1324 \teval_rewards:  565.2407621030014\n",
      "iteration:  1325 \teval_rewards:  383.8178790044353\n",
      "iteration:  1326 \teval_rewards:  604.3872141392046\n",
      "iteration:  1327 \teval_rewards:  544.9495931325474\n",
      "iteration:  1328 \teval_rewards:  517.689608267828\n",
      "iteration:  1329 \teval_rewards:  557.1922723575716\n",
      "iteration:  1330 \teval_rewards:  2611.7432179321263\n",
      "iteration:  1331 \teval_rewards:  1381.2828073370122\n",
      "iteration:  1332 \teval_rewards:  745.5333597396716\n",
      "iteration:  1333 \teval_rewards:  1291.6474024602403\n",
      "iteration:  1334 \teval_rewards:  434.10347946425776\n",
      "iteration:  1335 \teval_rewards:  796.9575167525473\n",
      "iteration:  1336 \teval_rewards:  705.3761813562696\n",
      "iteration:  1337 \teval_rewards:  655.1735156362756\n",
      "iteration:  1338 \teval_rewards:  2211.9692434197113\n",
      "iteration:  1339 \teval_rewards:  2429.369940755966\n",
      "iteration:  1340 \teval_rewards:  1303.6050678446004\n",
      "iteration:  1341 \teval_rewards:  546.6672258846853\n",
      "iteration:  1342 \teval_rewards:  2665.9104894127618\n",
      "iteration:  1343 \teval_rewards:  1510.8608988681844\n",
      "iteration:  1344 \teval_rewards:  2137.3271492055974\n",
      "iteration:  1345 \teval_rewards:  607.0389877605588\n",
      "iteration:  1346 \teval_rewards:  1713.0017473725654\n",
      "iteration:  1347 \teval_rewards:  843.0399641771193\n",
      "iteration:  1348 \teval_rewards:  537.8010922088101\n",
      "iteration:  1349 \teval_rewards:  691.8403491277572\n",
      "iteration:  1350 \teval_rewards:  774.5077139550609\n",
      "iteration:  1351 \teval_rewards:  597.7516298294695\n",
      "iteration:  1352 \teval_rewards:  765.2524816277971\n",
      "iteration:  1353 \teval_rewards:  672.7777918866464\n",
      "iteration:  1354 \teval_rewards:  1190.1101845468704\n",
      "iteration:  1355 \teval_rewards:  1481.749486933124\n",
      "iteration:  1356 \teval_rewards:  877.3541978657021\n",
      "iteration:  1357 \teval_rewards:  3130.08461958633\n",
      "iteration:  1358 \teval_rewards:  2451.0729880929666\n",
      "iteration:  1359 \teval_rewards:  2965.0291701073907\n",
      "iteration:  1360 \teval_rewards:  1511.6615914350255\n",
      "iteration:  1361 \teval_rewards:  491.2660856075772\n",
      "iteration:  1362 \teval_rewards:  3119.7084665153448\n",
      "iteration:  1363 \teval_rewards:  992.6604752842289\n",
      "iteration:  1364 \teval_rewards:  909.87644682662\n",
      "iteration:  1365 \teval_rewards:  860.7203373387832\n",
      "iteration:  1366 \teval_rewards:  1109.3794713048355\n",
      "iteration:  1367 \teval_rewards:  3146.6239467925743\n",
      "iteration:  1368 \teval_rewards:  622.8663055299014\n",
      "iteration:  1369 \teval_rewards:  641.4124771176727\n",
      "iteration:  1370 \teval_rewards:  752.1339668974523\n",
      "iteration:  1371 \teval_rewards:  3022.9544848836645\n",
      "iteration:  1372 \teval_rewards:  1290.7720036945284\n",
      "iteration:  1373 \teval_rewards:  3065.149120305294\n",
      "iteration:  1374 \teval_rewards:  1224.9303965573936\n",
      "iteration:  1375 \teval_rewards:  516.8798200860293\n",
      "iteration:  1376 \teval_rewards:  1512.02452896766\n",
      "iteration:  1377 \teval_rewards:  750.7995880098218\n",
      "iteration:  1378 \teval_rewards:  653.5938238191579\n",
      "iteration:  1379 \teval_rewards:  3166.0987441714765\n",
      "iteration:  1380 \teval_rewards:  3030.4387012890456\n",
      "iteration:  1381 \teval_rewards:  2159.8791917544754\n",
      "iteration:  1382 \teval_rewards:  482.5340826733411\n",
      "iteration:  1383 \teval_rewards:  559.1385788111486\n",
      "iteration:  1384 \teval_rewards:  3134.388895924964\n",
      "iteration:  1385 \teval_rewards:  730.1201637010348\n",
      "iteration:  1386 \teval_rewards:  1842.412693622777\n",
      "iteration:  1387 \teval_rewards:  2036.5256548281016\n",
      "iteration:  1388 \teval_rewards:  2939.014726085423\n",
      "iteration:  1389 \teval_rewards:  1369.1905551249797\n",
      "iteration:  1390 \teval_rewards:  2094.423991892569\n",
      "iteration:  1391 \teval_rewards:  1216.894533652764\n",
      "iteration:  1392 \teval_rewards:  2299.3394206599255\n",
      "iteration:  1393 \teval_rewards:  3112.0493484233657\n",
      "iteration:  1394 \teval_rewards:  3026.367718625017\n",
      "iteration:  1395 \teval_rewards:  1393.5580181849687\n",
      "iteration:  1396 \teval_rewards:  2858.4292889062517\n",
      "iteration:  1397 \teval_rewards:  875.4982362186335\n",
      "iteration:  1398 \teval_rewards:  2179.976604130095\n",
      "iteration:  1399 \teval_rewards:  1269.4002964447582\n",
      "Iter:1400| Ep_Reward:703.226| Running_reward:1398.109| Actor_Loss:0.066| Critic_Loss:1180.378| Iter_duration:3.189| lr:[1.9999999999999995e-05]\n",
      "iteration:  1400 \teval_rewards:  703.2262710280412\n",
      "iteration:  1401 \teval_rewards:  373.0385584320485\n",
      "iteration:  1402 \teval_rewards:  306.8137796172655\n",
      "iteration:  1403 \teval_rewards:  829.5755125430907\n",
      "iteration:  1404 \teval_rewards:  657.9896419723077\n",
      "iteration:  1405 \teval_rewards:  502.5666599862758\n",
      "iteration:  1406 \teval_rewards:  2782.9613172721074\n",
      "iteration:  1407 \teval_rewards:  600.75324378047\n",
      "iteration:  1408 \teval_rewards:  1342.1455634257422\n",
      "iteration:  1409 \teval_rewards:  604.7319773344815\n",
      "iteration:  1410 \teval_rewards:  2927.5912111572056\n",
      "iteration:  1411 \teval_rewards:  1122.1382020778226\n",
      "iteration:  1412 \teval_rewards:  768.0956198633444\n",
      "iteration:  1413 \teval_rewards:  2630.659242770999\n",
      "iteration:  1414 \teval_rewards:  3154.5113139104806\n",
      "iteration:  1415 \teval_rewards:  1421.559291875078\n",
      "iteration:  1416 \teval_rewards:  501.31961287585716\n",
      "iteration:  1417 \teval_rewards:  2380.4737969697803\n",
      "iteration:  1418 \teval_rewards:  1045.9650812169775\n",
      "iteration:  1419 \teval_rewards:  3226.0153743054657\n",
      "iteration:  1420 \teval_rewards:  3183.4202415030336\n",
      "iteration:  1421 \teval_rewards:  1170.6722781988753\n",
      "iteration:  1422 \teval_rewards:  2345.4201665746345\n",
      "iteration:  1423 \teval_rewards:  1986.0711741266916\n",
      "iteration:  1424 \teval_rewards:  1161.2543408086535\n",
      "iteration:  1425 \teval_rewards:  2703.345502767102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1426 \teval_rewards:  3160.735850479412\n",
      "iteration:  1427 \teval_rewards:  2952.504719234715\n",
      "iteration:  1428 \teval_rewards:  602.7289085327159\n",
      "iteration:  1429 \teval_rewards:  1782.2917360522079\n",
      "iteration:  1430 \teval_rewards:  1224.3876669937054\n",
      "iteration:  1431 \teval_rewards:  1941.7274734581624\n",
      "iteration:  1432 \teval_rewards:  902.2012542182915\n",
      "iteration:  1433 \teval_rewards:  2152.247364837033\n",
      "iteration:  1434 \teval_rewards:  428.13359924395326\n",
      "iteration:  1435 \teval_rewards:  556.4095565673232\n",
      "iteration:  1436 \teval_rewards:  529.3590045842552\n",
      "iteration:  1437 \teval_rewards:  653.2577658904327\n",
      "iteration:  1438 \teval_rewards:  3168.5589121853427\n",
      "iteration:  1439 \teval_rewards:  698.4788929128598\n",
      "iteration:  1440 \teval_rewards:  655.9145477854546\n",
      "iteration:  1441 \teval_rewards:  558.8925848109072\n",
      "iteration:  1442 \teval_rewards:  773.491095918167\n",
      "iteration:  1443 \teval_rewards:  2168.5794281259714\n",
      "iteration:  1444 \teval_rewards:  636.9498650330106\n",
      "iteration:  1445 \teval_rewards:  1425.3712820819374\n",
      "iteration:  1446 \teval_rewards:  1720.6133060530794\n",
      "iteration:  1447 \teval_rewards:  516.450746294077\n",
      "iteration:  1448 \teval_rewards:  1571.4534637883924\n",
      "iteration:  1449 \teval_rewards:  1767.1971362241356\n",
      "iteration:  1450 \teval_rewards:  1136.1109298262193\n",
      "iteration:  1451 \teval_rewards:  1059.5375093884334\n",
      "iteration:  1452 \teval_rewards:  896.0684284031182\n",
      "iteration:  1453 \teval_rewards:  883.7401484044113\n",
      "iteration:  1454 \teval_rewards:  579.1684011136639\n",
      "iteration:  1455 \teval_rewards:  528.441595238264\n",
      "iteration:  1456 \teval_rewards:  2234.9381665153273\n",
      "iteration:  1457 \teval_rewards:  804.7629959696054\n",
      "iteration:  1458 \teval_rewards:  1605.042480100775\n",
      "iteration:  1459 \teval_rewards:  657.7350249555232\n",
      "iteration:  1460 \teval_rewards:  820.7979419461526\n",
      "iteration:  1461 \teval_rewards:  687.0745540186603\n",
      "iteration:  1462 \teval_rewards:  1341.2991300647527\n",
      "iteration:  1463 \teval_rewards:  862.0943285587495\n",
      "iteration:  1464 \teval_rewards:  2340.1923341750526\n",
      "iteration:  1465 \teval_rewards:  1869.1130319322438\n",
      "iteration:  1466 \teval_rewards:  2184.871089706912\n",
      "iteration:  1467 \teval_rewards:  2999.702522520607\n",
      "iteration:  1468 \teval_rewards:  1356.0539174514613\n",
      "iteration:  1469 \teval_rewards:  2076.383610681907\n",
      "iteration:  1470 \teval_rewards:  2960.945595308152\n",
      "iteration:  1471 \teval_rewards:  554.4621057162465\n",
      "iteration:  1472 \teval_rewards:  2516.1569799821204\n",
      "iteration:  1473 \teval_rewards:  1921.3128940063314\n",
      "iteration:  1474 \teval_rewards:  2074.93730359911\n",
      "iteration:  1475 \teval_rewards:  730.6928755180221\n",
      "iteration:  1476 \teval_rewards:  754.6885837309101\n",
      "iteration:  1477 \teval_rewards:  1170.7741688520707\n",
      "iteration:  1478 \teval_rewards:  2986.8449477270715\n",
      "iteration:  1479 \teval_rewards:  1666.415702934318\n",
      "iteration:  1480 \teval_rewards:  778.3883413292853\n",
      "iteration:  1481 \teval_rewards:  492.6817728877979\n",
      "iteration:  1482 \teval_rewards:  550.8048213501937\n",
      "iteration:  1483 \teval_rewards:  703.1162045337564\n",
      "iteration:  1484 \teval_rewards:  651.4252640227556\n",
      "iteration:  1485 \teval_rewards:  753.1973550713518\n",
      "iteration:  1486 \teval_rewards:  2853.5452201199932\n",
      "iteration:  1487 \teval_rewards:  665.6689442050576\n",
      "iteration:  1488 \teval_rewards:  2346.3519083215797\n",
      "iteration:  1489 \teval_rewards:  1355.0624437148756\n",
      "iteration:  1490 \teval_rewards:  1094.140822282519\n",
      "iteration:  1491 \teval_rewards:  851.9214783408217\n",
      "iteration:  1492 \teval_rewards:  1820.246431958667\n",
      "iteration:  1493 \teval_rewards:  743.3107364676333\n",
      "iteration:  1494 \teval_rewards:  2340.4753239800943\n",
      "iteration:  1495 \teval_rewards:  1135.181584568108\n",
      "iteration:  1496 \teval_rewards:  2914.5990679216807\n",
      "iteration:  1497 \teval_rewards:  556.5408933381789\n",
      "iteration:  1498 \teval_rewards:  514.0707948723912\n",
      "iteration:  1499 \teval_rewards:  3017.5610794621302\n",
      "Iter:1500| Ep_Reward:564.273| Running_reward:1415.242| Actor_Loss:0.196| Critic_Loss:65.737| Iter_duration:3.128| lr:[0.0]\n",
      "iteration:  1500 \teval_rewards:  564.2734512254788\n",
      "episode reward:1018.943\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ant (4 legs animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"Ant\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "n_states = test_env.observation_space.shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states:111\n",
      "action bounds:[-1.0, 1.0]\n",
      "number of actions:8\n",
      "iteration:  1 \teval_rewards:  -90.52347489201367\n",
      "iteration:  2 \teval_rewards:  -2952.4207652657738\n",
      "iteration:  3 \teval_rewards:  -203.85554127405481\n",
      "iteration:  4 \teval_rewards:  -160.10090996163487\n",
      "iteration:  5 \teval_rewards:  -59.27902447112697\n",
      "iteration:  6 \teval_rewards:  -36.3396315102584\n",
      "iteration:  7 \teval_rewards:  -85.49083393318256\n",
      "iteration:  8 \teval_rewards:  -64.82798018891248\n",
      "iteration:  9 \teval_rewards:  -134.0075425316432\n",
      "iteration:  10 \teval_rewards:  -124.30530081984408\n",
      "iteration:  11 \teval_rewards:  -40.6603289185003\n",
      "iteration:  12 \teval_rewards:  -19.222018203265144\n",
      "iteration:  13 \teval_rewards:  -34.00819165785523\n",
      "iteration:  14 \teval_rewards:  -54.90853348084213\n",
      "iteration:  15 \teval_rewards:  -178.0133566312446\n",
      "iteration:  16 \teval_rewards:  -179.0340669161161\n",
      "iteration:  17 \teval_rewards:  -136.16001405883577\n",
      "iteration:  18 \teval_rewards:  -13.833301570736507\n",
      "iteration:  19 \teval_rewards:  -144.75984712685292\n",
      "iteration:  20 \teval_rewards:  -152.47862906104626\n",
      "iteration:  21 \teval_rewards:  -21.698071783482963\n",
      "iteration:  22 \teval_rewards:  -53.76238803258007\n",
      "iteration:  23 \teval_rewards:  -3.664672799924817\n",
      "iteration:  24 \teval_rewards:  -49.17775366289902\n",
      "iteration:  25 \teval_rewards:  -198.83887160536773\n",
      "iteration:  26 \teval_rewards:  -63.753319033720665\n",
      "iteration:  27 \teval_rewards:  4.91496095283464\n",
      "iteration:  28 \teval_rewards:  -28.545512564980235\n",
      "iteration:  29 \teval_rewards:  -42.539754854161956\n",
      "iteration:  30 \teval_rewards:  -32.24599095968847\n",
      "iteration:  31 \teval_rewards:  -18.92843837419595\n",
      "iteration:  32 \teval_rewards:  -4.4113217413013155\n",
      "iteration:  33 \teval_rewards:  -30.468875092557628\n",
      "iteration:  34 \teval_rewards:  -2.1325205958659703\n",
      "iteration:  35 \teval_rewards:  -20.482703902358097\n",
      "iteration:  36 \teval_rewards:  -25.592139137336705\n",
      "iteration:  37 \teval_rewards:  -37.69158519379161\n",
      "iteration:  38 \teval_rewards:  -496.6648326994054\n",
      "iteration:  39 \teval_rewards:  -484.5729166222355\n",
      "iteration:  40 \teval_rewards:  2.0904450635161016\n",
      "iteration:  41 \teval_rewards:  -21.085229116166214\n",
      "iteration:  42 \teval_rewards:  1.494818045595908\n",
      "iteration:  43 \teval_rewards:  1.8097773822250325\n",
      "iteration:  44 \teval_rewards:  -1.91061983118438\n",
      "iteration:  45 \teval_rewards:  -1.5508627937457828\n",
      "iteration:  46 \teval_rewards:  17.30097968116895\n",
      "iteration:  47 \teval_rewards:  -13.375915658920432\n",
      "iteration:  48 \teval_rewards:  4.0173457064072915\n",
      "iteration:  49 \teval_rewards:  3.4459564783119223\n",
      "iteration:  50 \teval_rewards:  20.52409397545884\n",
      "iteration:  51 \teval_rewards:  -32.57758045478487\n",
      "iteration:  52 \teval_rewards:  -0.512095240677726\n",
      "iteration:  53 \teval_rewards:  4.776730376928295\n",
      "iteration:  54 \teval_rewards:  43.55554466148457\n",
      "iteration:  55 \teval_rewards:  94.73257574831524\n",
      "iteration:  56 \teval_rewards:  82.86271553133416\n",
      "iteration:  57 \teval_rewards:  47.5559357527366\n",
      "iteration:  58 \teval_rewards:  86.86019771294852\n",
      "iteration:  59 \teval_rewards:  19.43303856849456\n",
      "iteration:  60 \teval_rewards:  47.040431832629515\n",
      "iteration:  61 \teval_rewards:  21.052472248554317\n",
      "iteration:  62 \teval_rewards:  113.22558178154429\n",
      "iteration:  63 \teval_rewards:  134.31257910269682\n",
      "iteration:  64 \teval_rewards:  36.53438535472352\n",
      "iteration:  65 \teval_rewards:  63.14628136800735\n",
      "iteration:  66 \teval_rewards:  10.861120818457175\n",
      "iteration:  67 \teval_rewards:  124.50304508658971\n",
      "iteration:  68 \teval_rewards:  295.9116470873911\n",
      "iteration:  69 \teval_rewards:  10.851677384758858\n",
      "iteration:  70 \teval_rewards:  238.13124350457198\n",
      "iteration:  71 \teval_rewards:  7.34611748206955\n",
      "iteration:  72 \teval_rewards:  137.43583627791574\n",
      "iteration:  73 \teval_rewards:  274.6177461372474\n",
      "iteration:  74 \teval_rewards:  302.68397210930215\n",
      "iteration:  75 \teval_rewards:  310.50420316843764\n",
      "iteration:  76 \teval_rewards:  56.19961958336307\n",
      "iteration:  77 \teval_rewards:  515.7790068513884\n",
      "iteration:  78 \teval_rewards:  3.106630315558044\n",
      "iteration:  79 \teval_rewards:  345.91954381826775\n",
      "iteration:  80 \teval_rewards:  7.774805668300797\n",
      "iteration:  81 \teval_rewards:  185.4414005673129\n",
      "iteration:  82 \teval_rewards:  417.6221185846476\n",
      "iteration:  83 \teval_rewards:  430.88541951780536\n",
      "iteration:  84 \teval_rewards:  194.3577568296573\n",
      "iteration:  85 \teval_rewards:  102.03462345272537\n",
      "iteration:  86 \teval_rewards:  18.765907851272903\n",
      "iteration:  87 \teval_rewards:  543.7534083906008\n",
      "iteration:  88 \teval_rewards:  319.9954760562133\n",
      "iteration:  89 \teval_rewards:  691.7917288172134\n",
      "iteration:  90 \teval_rewards:  390.8263043557573\n",
      "iteration:  91 \teval_rewards:  90.25618692021587\n",
      "iteration:  92 \teval_rewards:  36.50418375878412\n",
      "iteration:  93 \teval_rewards:  361.67224994570876\n",
      "iteration:  94 \teval_rewards:  63.04476902687121\n",
      "iteration:  95 \teval_rewards:  508.04426998908355\n",
      "iteration:  96 \teval_rewards:  100.9101724459554\n",
      "iteration:  97 \teval_rewards:  162.9749274636296\n",
      "iteration:  98 \teval_rewards:  716.1592711968847\n",
      "iteration:  99 \teval_rewards:  463.7044295131039\n",
      "Iter:100| Ep_Reward:190.041| Running_reward:19.872| Actor_Loss:-0.061| Critic_Loss:70.695| Iter_duration:3.620| lr:[0.00028]\n",
      "iteration:  100 \teval_rewards:  190.0407500599031\n",
      "iteration:  101 \teval_rewards:  497.05266675752836\n",
      "iteration:  102 \teval_rewards:  674.8028501445195\n",
      "iteration:  103 \teval_rewards:  634.8964702580687\n",
      "iteration:  104 \teval_rewards:  198.9916851178224\n",
      "iteration:  105 \teval_rewards:  638.0568631721384\n",
      "iteration:  106 \teval_rewards:  810.7639835040644\n",
      "iteration:  107 \teval_rewards:  324.7218174205177\n",
      "iteration:  108 \teval_rewards:  592.2362229331882\n",
      "iteration:  109 \teval_rewards:  112.20226705301775\n",
      "iteration:  110 \teval_rewards:  934.9587159416285\n",
      "iteration:  111 \teval_rewards:  600.1252651205658\n",
      "iteration:  112 \teval_rewards:  613.9104612995897\n",
      "iteration:  113 \teval_rewards:  689.2826639074348\n",
      "iteration:  114 \teval_rewards:  502.32846399937046\n",
      "iteration:  115 \teval_rewards:  557.6544111211751\n",
      "iteration:  116 \teval_rewards:  854.5473057531998\n",
      "iteration:  117 \teval_rewards:  830.4025684396976\n",
      "iteration:  118 \teval_rewards:  694.1213628987687\n",
      "iteration:  119 \teval_rewards:  770.3427552787178\n",
      "iteration:  120 \teval_rewards:  758.7376981467523\n",
      "iteration:  121 \teval_rewards:  923.6579272298166\n",
      "iteration:  122 \teval_rewards:  570.1741957127396\n",
      "iteration:  123 \teval_rewards:  370.8683877694477\n",
      "iteration:  124 \teval_rewards:  812.0873117673761\n",
      "iteration:  125 \teval_rewards:  988.9609833806029\n",
      "iteration:  126 \teval_rewards:  887.8906284507443\n",
      "iteration:  127 \teval_rewards:  727.9371519588926\n",
      "iteration:  128 \teval_rewards:  726.1545524042826\n",
      "iteration:  129 \teval_rewards:  760.0347064966165\n",
      "iteration:  130 \teval_rewards:  203.1556879056351\n",
      "iteration:  131 \teval_rewards:  886.4288897239609\n",
      "iteration:  132 \teval_rewards:  1091.843253456355\n",
      "iteration:  133 \teval_rewards:  625.9009803765068\n",
      "iteration:  134 \teval_rewards:  1016.8381252905182\n",
      "iteration:  135 \teval_rewards:  571.1129494886623\n",
      "iteration:  136 \teval_rewards:  585.2722905673922\n",
      "iteration:  137 \teval_rewards:  135.68208354720298\n",
      "iteration:  138 \teval_rewards:  193.57935687268645\n",
      "iteration:  139 \teval_rewards:  1061.9711359840671\n",
      "iteration:  140 \teval_rewards:  1090.2100044394822\n",
      "iteration:  141 \teval_rewards:  1100.2549833222442\n",
      "iteration:  142 \teval_rewards:  1044.4017765052697\n",
      "iteration:  143 \teval_rewards:  239.4018340278003\n",
      "iteration:  144 \teval_rewards:  1023.0204226972646\n",
      "iteration:  145 \teval_rewards:  325.2727379743261\n",
      "iteration:  146 \teval_rewards:  229.36670652776553\n",
      "iteration:  147 \teval_rewards:  506.1514079261273\n",
      "iteration:  148 \teval_rewards:  388.4253776895007\n",
      "iteration:  149 \teval_rewards:  987.7477997518291\n",
      "iteration:  150 \teval_rewards:  820.0212272636296\n",
      "iteration:  151 \teval_rewards:  436.88216633637734\n",
      "iteration:  152 \teval_rewards:  303.6307414261664\n",
      "iteration:  153 \teval_rewards:  185.06464717761185\n",
      "iteration:  154 \teval_rewards:  1038.2407131207274\n",
      "iteration:  155 \teval_rewards:  741.0298923589113\n",
      "iteration:  156 \teval_rewards:  63.43872617578318\n",
      "iteration:  157 \teval_rewards:  882.2182313944202\n",
      "iteration:  158 \teval_rewards:  765.2421035806386\n",
      "iteration:  159 \teval_rewards:  51.62597645987156\n",
      "iteration:  160 \teval_rewards:  1216.2309613802415\n",
      "iteration:  161 \teval_rewards:  1065.7734045510442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  162 \teval_rewards:  10.444257225857921\n",
      "iteration:  163 \teval_rewards:  61.49056604995963\n",
      "iteration:  164 \teval_rewards:  402.2532444907955\n",
      "iteration:  165 \teval_rewards:  402.38713180583636\n",
      "iteration:  166 \teval_rewards:  1101.460573793279\n",
      "iteration:  167 \teval_rewards:  663.0406597882275\n",
      "iteration:  168 \teval_rewards:  1026.1478509596664\n",
      "iteration:  169 \teval_rewards:  490.2966935377651\n",
      "iteration:  170 \teval_rewards:  939.8558164846927\n",
      "iteration:  171 \teval_rewards:  2.8857032090320325\n",
      "iteration:  172 \teval_rewards:  442.27262643268676\n",
      "iteration:  173 \teval_rewards:  269.7854103038054\n",
      "iteration:  174 \teval_rewards:  673.7835590264433\n",
      "iteration:  175 \teval_rewards:  239.74352636680274\n",
      "iteration:  176 \teval_rewards:  1070.945953528168\n",
      "iteration:  177 \teval_rewards:  407.0949129559192\n",
      "iteration:  178 \teval_rewards:  694.9413631572207\n",
      "iteration:  179 \teval_rewards:  944.1239703109252\n",
      "iteration:  180 \teval_rewards:  1161.5583954653466\n",
      "iteration:  181 \teval_rewards:  597.2505085421012\n",
      "iteration:  182 \teval_rewards:  488.40031160384285\n",
      "iteration:  183 \teval_rewards:  639.610740718149\n",
      "iteration:  184 \teval_rewards:  249.10240559225645\n",
      "iteration:  185 \teval_rewards:  519.0771695666633\n",
      "iteration:  186 \teval_rewards:  111.28696933336917\n",
      "iteration:  187 \teval_rewards:  49.25288954683965\n",
      "iteration:  188 \teval_rewards:  646.2225516609604\n",
      "iteration:  189 \teval_rewards:  620.8291081100116\n",
      "iteration:  190 \teval_rewards:  1207.9887603875745\n",
      "iteration:  191 \teval_rewards:  1140.707906610792\n",
      "iteration:  192 \teval_rewards:  590.7042636417989\n",
      "iteration:  193 \teval_rewards:  689.3151204127512\n",
      "iteration:  194 \teval_rewards:  812.309134460075\n",
      "iteration:  195 \teval_rewards:  962.0299271538943\n",
      "iteration:  196 \teval_rewards:  789.9439650112873\n",
      "iteration:  197 \teval_rewards:  863.6081883194913\n",
      "iteration:  198 \teval_rewards:  881.1695649119456\n",
      "iteration:  199 \teval_rewards:  1293.398511582541\n",
      "Iter:200| Ep_Reward:1069.761| Running_reward:426.077| Actor_Loss:0.224| Critic_Loss:229.868| Iter_duration:4.140| lr:[0.00026]\n",
      "iteration:  200 \teval_rewards:  1069.7608389570137\n",
      "iteration:  201 \teval_rewards:  314.5531655148139\n",
      "iteration:  202 \teval_rewards:  31.24183342338382\n",
      "iteration:  203 \teval_rewards:  621.5989458693231\n",
      "iteration:  204 \teval_rewards:  238.37942852056148\n",
      "iteration:  205 \teval_rewards:  1104.8651413765504\n",
      "iteration:  206 \teval_rewards:  731.664873719053\n",
      "iteration:  207 \teval_rewards:  1029.0643232177638\n",
      "iteration:  208 \teval_rewards:  1215.597716320905\n",
      "iteration:  209 \teval_rewards:  863.8724635566371\n",
      "iteration:  210 \teval_rewards:  206.36589083527815\n",
      "iteration:  211 \teval_rewards:  538.0137011718349\n",
      "iteration:  212 \teval_rewards:  130.60853993723867\n",
      "iteration:  213 \teval_rewards:  763.1612013597156\n",
      "iteration:  214 \teval_rewards:  545.4555218195322\n",
      "iteration:  215 \teval_rewards:  1395.100157367255\n",
      "iteration:  216 \teval_rewards:  1310.1715864520745\n",
      "iteration:  217 \teval_rewards:  1161.7888194213224\n",
      "iteration:  218 \teval_rewards:  1283.4647919298777\n",
      "iteration:  219 \teval_rewards:  756.3733510379438\n",
      "iteration:  220 \teval_rewards:  1001.4766777730513\n",
      "iteration:  221 \teval_rewards:  861.6809408130002\n",
      "iteration:  222 \teval_rewards:  586.2994622170696\n",
      "iteration:  223 \teval_rewards:  591.9598743239926\n",
      "iteration:  224 \teval_rewards:  527.7508277452212\n",
      "iteration:  225 \teval_rewards:  613.622834723129\n",
      "iteration:  226 \teval_rewards:  1301.6938662647337\n",
      "iteration:  227 \teval_rewards:  119.69919933637955\n",
      "iteration:  228 \teval_rewards:  48.43544246978213\n",
      "iteration:  229 \teval_rewards:  626.2156438366565\n",
      "iteration:  230 \teval_rewards:  100.20017567446853\n",
      "iteration:  231 \teval_rewards:  1234.3509121307482\n",
      "iteration:  232 \teval_rewards:  313.5549611329251\n",
      "iteration:  233 \teval_rewards:  555.7495318106897\n",
      "iteration:  234 \teval_rewards:  1351.2023359294994\n",
      "iteration:  235 \teval_rewards:  1387.6600870561151\n",
      "iteration:  236 \teval_rewards:  1368.1157647511648\n",
      "iteration:  237 \teval_rewards:  766.5254617477856\n",
      "iteration:  238 \teval_rewards:  1315.0476341191777\n",
      "iteration:  239 \teval_rewards:  1458.6867703091125\n",
      "iteration:  240 \teval_rewards:  995.8866124149374\n",
      "iteration:  241 \teval_rewards:  198.9852866436083\n",
      "iteration:  242 \teval_rewards:  1301.225885163118\n",
      "iteration:  243 \teval_rewards:  232.4994014665379\n",
      "iteration:  244 \teval_rewards:  1347.752774749567\n",
      "iteration:  245 \teval_rewards:  157.51488352703512\n",
      "iteration:  246 \teval_rewards:  918.5178825744338\n",
      "iteration:  247 \teval_rewards:  340.6283541941411\n",
      "iteration:  248 \teval_rewards:  604.3185619668479\n",
      "iteration:  249 \teval_rewards:  162.63430643498668\n",
      "iteration:  250 \teval_rewards:  1359.8154201729017\n",
      "iteration:  251 \teval_rewards:  837.1841441995026\n",
      "iteration:  252 \teval_rewards:  505.64416213748444\n",
      "iteration:  253 \teval_rewards:  421.12334906687926\n",
      "iteration:  254 \teval_rewards:  800.9923901927051\n",
      "iteration:  255 \teval_rewards:  992.7751064471764\n",
      "iteration:  256 \teval_rewards:  1111.261653868667\n",
      "iteration:  257 \teval_rewards:  1011.8218493041202\n",
      "iteration:  258 \teval_rewards:  1395.5807262908497\n",
      "iteration:  259 \teval_rewards:  1324.0102860216366\n",
      "iteration:  260 \teval_rewards:  1471.6347480725128\n",
      "iteration:  261 \teval_rewards:  833.5581045408677\n",
      "iteration:  262 \teval_rewards:  134.497733322898\n",
      "iteration:  263 \teval_rewards:  250.27023653150954\n",
      "iteration:  264 \teval_rewards:  1391.120717211189\n",
      "iteration:  265 \teval_rewards:  1459.1752521874314\n",
      "iteration:  266 \teval_rewards:  1441.5693229862118\n",
      "iteration:  267 \teval_rewards:  1004.6551101314989\n",
      "iteration:  268 \teval_rewards:  1250.0135200985842\n",
      "iteration:  269 \teval_rewards:  1166.2975990767868\n",
      "iteration:  270 \teval_rewards:  956.9961063154691\n",
      "iteration:  271 \teval_rewards:  1440.3179111698792\n",
      "iteration:  272 \teval_rewards:  699.5181138998846\n",
      "iteration:  273 \teval_rewards:  1477.5594320302005\n",
      "iteration:  274 \teval_rewards:  226.95611770320676\n",
      "iteration:  275 \teval_rewards:  1568.536662949123\n",
      "iteration:  276 \teval_rewards:  1063.0253734626222\n",
      "iteration:  277 \teval_rewards:  511.5470387489315\n",
      "iteration:  278 \teval_rewards:  1461.7152981062197\n",
      "iteration:  279 \teval_rewards:  1257.183254335701\n",
      "iteration:  280 \teval_rewards:  1355.6493963266894\n",
      "iteration:  281 \teval_rewards:  164.22488035416941\n",
      "iteration:  282 \teval_rewards:  1367.6718897985875\n",
      "iteration:  283 \teval_rewards:  246.6445064679475\n",
      "iteration:  284 \teval_rewards:  79.7807425470657\n",
      "iteration:  285 \teval_rewards:  1466.3458429118007\n",
      "iteration:  286 \teval_rewards:  584.8780076734083\n",
      "iteration:  287 \teval_rewards:  1459.6888574967088\n",
      "iteration:  288 \teval_rewards:  1470.6766660994856\n",
      "iteration:  289 \teval_rewards:  517.3626593124347\n",
      "iteration:  290 \teval_rewards:  224.81494540361567\n",
      "iteration:  291 \teval_rewards:  1032.384817864622\n",
      "iteration:  292 \teval_rewards:  1145.4391260258444\n",
      "iteration:  293 \teval_rewards:  1259.9108570236958\n",
      "iteration:  294 \teval_rewards:  1551.8465415948974\n",
      "iteration:  295 \teval_rewards:  1133.4807099331633\n",
      "iteration:  296 \teval_rewards:  1477.5692202953085\n",
      "iteration:  297 \teval_rewards:  1072.7692761655676\n",
      "iteration:  298 \teval_rewards:  413.257323145263\n",
      "iteration:  299 \teval_rewards:  675.4427297161988\n",
      "Iter:300| Ep_Reward:269.724| Running_reward:720.679| Actor_Loss:0.009| Critic_Loss:167.317| Iter_duration:3.605| lr:[0.00023999999999999998]\n",
      "iteration:  300 \teval_rewards:  269.72396598852197\n",
      "iteration:  301 \teval_rewards:  37.35549686421325\n",
      "iteration:  302 \teval_rewards:  174.9171829192153\n",
      "iteration:  303 \teval_rewards:  185.0993898370868\n",
      "iteration:  304 \teval_rewards:  1570.8616213737262\n",
      "iteration:  305 \teval_rewards:  1124.60600828522\n",
      "iteration:  306 \teval_rewards:  1442.4942715440875\n",
      "iteration:  307 \teval_rewards:  586.1314717885855\n",
      "iteration:  308 \teval_rewards:  1624.1619919752338\n",
      "iteration:  309 \teval_rewards:  1501.3829953738173\n",
      "iteration:  310 \teval_rewards:  1039.9120568568699\n",
      "iteration:  311 \teval_rewards:  1611.505626656979\n",
      "iteration:  312 \teval_rewards:  822.1158909417778\n",
      "iteration:  313 \teval_rewards:  430.2592112415158\n",
      "iteration:  314 \teval_rewards:  301.081355536211\n",
      "iteration:  315 \teval_rewards:  559.6381685129131\n",
      "iteration:  316 \teval_rewards:  4.699593978895626\n",
      "iteration:  317 \teval_rewards:  90.3121955777235\n",
      "iteration:  318 \teval_rewards:  1480.4476938782047\n",
      "iteration:  319 \teval_rewards:  1526.771183277484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  320 \teval_rewards:  1134.4085821442861\n",
      "iteration:  321 \teval_rewards:  982.9430421796877\n",
      "iteration:  322 \teval_rewards:  1074.7674167789485\n",
      "iteration:  323 \teval_rewards:  512.9627836326883\n",
      "iteration:  324 \teval_rewards:  1458.7460230933395\n",
      "iteration:  325 \teval_rewards:  1552.9977858280731\n",
      "iteration:  326 \teval_rewards:  856.9511995962281\n",
      "iteration:  327 \teval_rewards:  52.69769742136406\n",
      "iteration:  328 \teval_rewards:  -1.7920742505798677\n",
      "iteration:  329 \teval_rewards:  262.92362087237444\n",
      "iteration:  330 \teval_rewards:  1540.0545491763246\n",
      "iteration:  331 \teval_rewards:  1542.2657521256867\n",
      "iteration:  332 \teval_rewards:  1423.7500966738437\n",
      "iteration:  333 \teval_rewards:  720.6463547320384\n",
      "iteration:  334 \teval_rewards:  315.23283909399595\n",
      "iteration:  335 \teval_rewards:  1651.0632655922943\n",
      "iteration:  336 \teval_rewards:  1646.2260171892801\n",
      "iteration:  337 \teval_rewards:  1815.162547544012\n",
      "iteration:  338 \teval_rewards:  1558.266296569618\n",
      "iteration:  339 \teval_rewards:  1567.1992274096858\n",
      "iteration:  340 \teval_rewards:  1625.6307658614676\n",
      "iteration:  341 \teval_rewards:  414.9349823772259\n",
      "iteration:  342 \teval_rewards:  1715.4482499294631\n",
      "iteration:  343 \teval_rewards:  454.91569225274617\n",
      "iteration:  344 \teval_rewards:  588.8793225481579\n",
      "iteration:  345 \teval_rewards:  596.495063295252\n",
      "iteration:  346 \teval_rewards:  762.5564632660938\n",
      "iteration:  347 \teval_rewards:  1705.6284738959857\n",
      "iteration:  348 \teval_rewards:  1225.5250962527853\n",
      "iteration:  349 \teval_rewards:  1584.804822897803\n",
      "iteration:  350 \teval_rewards:  654.8534494938225\n",
      "iteration:  351 \teval_rewards:  1665.8075976006908\n",
      "iteration:  352 \teval_rewards:  889.6098276475229\n",
      "iteration:  353 \teval_rewards:  920.6322836694562\n",
      "iteration:  354 \teval_rewards:  1513.8212196841732\n",
      "iteration:  355 \teval_rewards:  257.0015424550096\n",
      "iteration:  356 \teval_rewards:  1692.493709593762\n",
      "iteration:  357 \teval_rewards:  1608.3397612433293\n",
      "iteration:  358 \teval_rewards:  311.11203204970644\n",
      "iteration:  359 \teval_rewards:  902.7975245773889\n",
      "iteration:  360 \teval_rewards:  539.6786251891657\n",
      "iteration:  361 \teval_rewards:  1467.4832099944927\n",
      "iteration:  362 \teval_rewards:  1278.4362048904973\n",
      "iteration:  363 \teval_rewards:  1466.769072257281\n",
      "iteration:  364 \teval_rewards:  1416.4721194253411\n",
      "iteration:  365 \teval_rewards:  1475.1806803333075\n",
      "iteration:  366 \teval_rewards:  1384.5032320410319\n",
      "iteration:  367 \teval_rewards:  1721.0937384836334\n",
      "iteration:  368 \teval_rewards:  406.0416068863371\n",
      "iteration:  369 \teval_rewards:  512.1867577207203\n",
      "iteration:  370 \teval_rewards:  1215.557723551194\n",
      "iteration:  371 \teval_rewards:  587.0732603607372\n",
      "iteration:  372 \teval_rewards:  707.1113630958528\n",
      "iteration:  373 \teval_rewards:  250.62887987317563\n",
      "iteration:  374 \teval_rewards:  1367.1036529068995\n",
      "iteration:  375 \teval_rewards:  1062.3078702509488\n",
      "iteration:  376 \teval_rewards:  1617.460304172012\n",
      "iteration:  377 \teval_rewards:  1518.1860904137593\n",
      "iteration:  378 \teval_rewards:  1692.643418837174\n",
      "iteration:  379 \teval_rewards:  1516.4953426301993\n",
      "iteration:  380 \teval_rewards:  1203.41519401521\n",
      "iteration:  381 \teval_rewards:  1498.4094908348109\n",
      "iteration:  382 \teval_rewards:  1615.400123235475\n",
      "iteration:  383 \teval_rewards:  1181.5237341421307\n",
      "iteration:  384 \teval_rewards:  569.428837431874\n",
      "iteration:  385 \teval_rewards:  1555.320652677209\n",
      "iteration:  386 \teval_rewards:  51.0686982407597\n",
      "iteration:  387 \teval_rewards:  1135.8805716059067\n",
      "iteration:  388 \teval_rewards:  1826.665641424354\n",
      "iteration:  389 \teval_rewards:  1301.035523057831\n",
      "iteration:  390 \teval_rewards:  1624.4214428967448\n",
      "iteration:  391 \teval_rewards:  1043.4938126293982\n",
      "iteration:  392 \teval_rewards:  154.9703289425959\n",
      "iteration:  393 \teval_rewards:  1515.128211925023\n",
      "iteration:  394 \teval_rewards:  1462.7118110246204\n",
      "iteration:  395 \teval_rewards:  1598.8864152727097\n",
      "iteration:  396 \teval_rewards:  1482.2342143398844\n",
      "iteration:  397 \teval_rewards:  1193.494385328531\n",
      "iteration:  398 \teval_rewards:  1373.7832214116795\n",
      "iteration:  399 \teval_rewards:  383.4420425789265\n",
      "Iter:400| Ep_Reward:4.117| Running_reward:950.039| Actor_Loss:0.030| Critic_Loss:252.999| Iter_duration:3.586| lr:[0.00022]\n",
      "iteration:  400 \teval_rewards:  4.117495162043603\n",
      "iteration:  401 \teval_rewards:  1403.3463808166757\n",
      "iteration:  402 \teval_rewards:  1443.7963326546155\n",
      "iteration:  403 \teval_rewards:  830.4866111236227\n",
      "iteration:  404 \teval_rewards:  1510.8761893170274\n",
      "iteration:  405 \teval_rewards:  1625.724439350882\n",
      "iteration:  406 \teval_rewards:  1639.3617182104879\n",
      "iteration:  407 \teval_rewards:  144.99682647749552\n",
      "iteration:  408 \teval_rewards:  1643.196450865359\n",
      "iteration:  409 \teval_rewards:  458.8577580665416\n",
      "iteration:  410 \teval_rewards:  1508.5325641414877\n",
      "iteration:  411 \teval_rewards:  1698.0086908770656\n",
      "iteration:  412 \teval_rewards:  1581.164412075626\n",
      "iteration:  413 \teval_rewards:  1725.419315260845\n",
      "iteration:  414 \teval_rewards:  1719.7536851417651\n",
      "iteration:  415 \teval_rewards:  1707.603036456596\n",
      "iteration:  416 \teval_rewards:  1623.3182343937442\n",
      "iteration:  417 \teval_rewards:  1608.037520862522\n",
      "iteration:  418 \teval_rewards:  1515.8545557135405\n",
      "iteration:  419 \teval_rewards:  1665.3436814921672\n",
      "iteration:  420 \teval_rewards:  685.1273905167416\n",
      "iteration:  421 \teval_rewards:  1666.3730766339895\n",
      "iteration:  422 \teval_rewards:  1499.429550406753\n",
      "iteration:  423 \teval_rewards:  1711.938278960386\n",
      "iteration:  424 \teval_rewards:  1674.5226646503297\n",
      "iteration:  425 \teval_rewards:  1748.4386829310724\n",
      "iteration:  426 \teval_rewards:  1657.8866098361464\n",
      "iteration:  427 \teval_rewards:  1665.076330428424\n",
      "iteration:  428 \teval_rewards:  1596.6133222003969\n",
      "iteration:  429 \teval_rewards:  1677.595832594086\n",
      "iteration:  430 \teval_rewards:  1731.0493149777626\n",
      "iteration:  431 \teval_rewards:  1630.4203358800728\n",
      "iteration:  432 \teval_rewards:  1664.0343580081067\n",
      "iteration:  433 \teval_rewards:  1562.7307927490524\n",
      "iteration:  434 \teval_rewards:  1354.6863320367618\n",
      "iteration:  435 \teval_rewards:  1615.3602675543793\n",
      "iteration:  436 \teval_rewards:  1590.3113781455527\n",
      "iteration:  437 \teval_rewards:  325.6328661622352\n",
      "iteration:  438 \teval_rewards:  1583.653409482298\n",
      "iteration:  439 \teval_rewards:  1584.4678573487413\n",
      "iteration:  440 \teval_rewards:  878.2960395483583\n",
      "iteration:  441 \teval_rewards:  1350.6201736103608\n",
      "iteration:  442 \teval_rewards:  1673.8834911670094\n",
      "iteration:  443 \teval_rewards:  276.58465757338297\n",
      "iteration:  444 \teval_rewards:  1615.3270713105449\n",
      "iteration:  445 \teval_rewards:  1758.2025026327585\n",
      "iteration:  446 \teval_rewards:  1201.711416758256\n",
      "iteration:  447 \teval_rewards:  153.16018972744519\n",
      "iteration:  448 \teval_rewards:  1717.1438102838028\n",
      "iteration:  449 \teval_rewards:  464.61018470613385\n",
      "iteration:  450 \teval_rewards:  471.4323522785777\n",
      "iteration:  451 \teval_rewards:  1657.332531933084\n",
      "iteration:  452 \teval_rewards:  1056.9008554042764\n",
      "iteration:  453 \teval_rewards:  1576.030575091686\n",
      "iteration:  454 \teval_rewards:  1682.8591655937275\n",
      "iteration:  455 \teval_rewards:  1571.3561494074863\n",
      "iteration:  456 \teval_rewards:  1221.6161460249882\n",
      "iteration:  457 \teval_rewards:  898.3543685497475\n",
      "iteration:  458 \teval_rewards:  1754.183526306527\n",
      "iteration:  459 \teval_rewards:  497.53524843513037\n",
      "iteration:  460 \teval_rewards:  39.09016561616217\n",
      "iteration:  461 \teval_rewards:  1672.5344506397769\n",
      "iteration:  462 \teval_rewards:  1675.1001676117978\n",
      "iteration:  463 \teval_rewards:  1393.9308614089919\n",
      "iteration:  464 \teval_rewards:  1690.3694918143729\n",
      "iteration:  465 \teval_rewards:  1608.7076572685685\n",
      "iteration:  466 \teval_rewards:  1744.0443306879536\n",
      "iteration:  467 \teval_rewards:  1388.9082348994693\n",
      "iteration:  468 \teval_rewards:  1615.0658613767266\n",
      "iteration:  469 \teval_rewards:  1652.0024872800914\n",
      "iteration:  470 \teval_rewards:  1668.6540760059252\n",
      "iteration:  471 \teval_rewards:  1815.9173996178424\n",
      "iteration:  472 \teval_rewards:  1669.0802112702543\n",
      "iteration:  473 \teval_rewards:  243.97037883827335\n",
      "iteration:  474 \teval_rewards:  1815.3429436674653\n",
      "iteration:  475 \teval_rewards:  1818.249436369303\n",
      "iteration:  476 \teval_rewards:  1029.7558241396391\n",
      "iteration:  477 \teval_rewards:  1747.9638082107958\n",
      "iteration:  478 \teval_rewards:  1625.5478441793587\n",
      "iteration:  479 \teval_rewards:  1578.9553874250562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  480 \teval_rewards:  1697.9052495695353\n",
      "iteration:  481 \teval_rewards:  1671.2772230494543\n",
      "iteration:  482 \teval_rewards:  1781.9610122275674\n",
      "iteration:  483 \teval_rewards:  329.75446272270676\n",
      "iteration:  484 \teval_rewards:  1659.6301700916315\n",
      "iteration:  485 \teval_rewards:  1644.6529465680817\n",
      "iteration:  486 \teval_rewards:  1681.5220855592208\n",
      "iteration:  487 \teval_rewards:  1522.7468192673261\n",
      "iteration:  488 \teval_rewards:  1704.879789019937\n",
      "iteration:  489 \teval_rewards:  1724.8960001760736\n",
      "iteration:  490 \teval_rewards:  1632.9824829932186\n",
      "iteration:  491 \teval_rewards:  1707.0790676766164\n",
      "iteration:  492 \teval_rewards:  1642.284685278294\n",
      "iteration:  493 \teval_rewards:  138.2354673161604\n",
      "iteration:  494 \teval_rewards:  1780.7707513364662\n",
      "iteration:  495 \teval_rewards:  1567.438563702843\n",
      "iteration:  496 \teval_rewards:  1431.6624146618412\n",
      "iteration:  497 \teval_rewards:  1715.9732194299347\n",
      "iteration:  498 \teval_rewards:  1711.6830719715003\n",
      "iteration:  499 \teval_rewards:  500.32171304773465\n",
      "Iter:500| Ep_Reward:472.921| Running_reward:1234.251| Actor_Loss:-0.080| Critic_Loss:356.414| Iter_duration:4.334| lr:[0.0002]\n",
      "iteration:  500 \teval_rewards:  472.92120544245904\n",
      "iteration:  501 \teval_rewards:  1551.9772084546296\n",
      "iteration:  502 \teval_rewards:  1708.383419368783\n",
      "iteration:  503 \teval_rewards:  1709.1546752864429\n",
      "iteration:  504 \teval_rewards:  1619.7359038600334\n",
      "iteration:  505 \teval_rewards:  1699.2061176661082\n",
      "iteration:  506 \teval_rewards:  1444.7770588273065\n",
      "iteration:  507 \teval_rewards:  1655.223865753771\n",
      "iteration:  508 \teval_rewards:  1686.1579642546671\n",
      "iteration:  509 \teval_rewards:  1537.2111980816203\n",
      "iteration:  510 \teval_rewards:  333.00018641693845\n",
      "iteration:  511 \teval_rewards:  1747.028079565584\n",
      "iteration:  512 \teval_rewards:  1756.7275027053956\n",
      "iteration:  513 \teval_rewards:  1745.875848506606\n",
      "iteration:  514 \teval_rewards:  1685.842563146098\n",
      "iteration:  515 \teval_rewards:  1772.2235600907538\n",
      "iteration:  516 \teval_rewards:  1688.8112922036137\n",
      "iteration:  517 \teval_rewards:  1663.659742855893\n",
      "iteration:  518 \teval_rewards:  1737.694571391718\n",
      "iteration:  519 \teval_rewards:  1785.42422475719\n",
      "iteration:  520 \teval_rewards:  1707.2034716365376\n",
      "iteration:  521 \teval_rewards:  1819.1343557900502\n",
      "iteration:  522 \teval_rewards:  692.1430298015454\n",
      "iteration:  523 \teval_rewards:  1731.8468933190845\n",
      "iteration:  524 \teval_rewards:  1661.0934952130592\n",
      "iteration:  525 \teval_rewards:  1602.6065160449393\n",
      "iteration:  526 \teval_rewards:  1688.3517814093352\n",
      "iteration:  527 \teval_rewards:  1747.4293667197603\n",
      "iteration:  528 \teval_rewards:  1210.1724219996945\n",
      "iteration:  529 \teval_rewards:  1702.2633563059753\n",
      "iteration:  530 \teval_rewards:  1759.5045286748914\n",
      "iteration:  531 \teval_rewards:  192.14596011189084\n",
      "iteration:  532 \teval_rewards:  1688.2751324662563\n",
      "iteration:  533 \teval_rewards:  1853.7947985718222\n",
      "iteration:  534 \teval_rewards:  1714.7302677026732\n",
      "iteration:  535 \teval_rewards:  1773.552160390052\n",
      "iteration:  536 \teval_rewards:  1792.8238543438426\n",
      "iteration:  537 \teval_rewards:  1849.6325927258304\n",
      "iteration:  538 \teval_rewards:  1460.5944248629087\n",
      "iteration:  539 \teval_rewards:  1628.178246246528\n",
      "iteration:  540 \teval_rewards:  1693.7698194116697\n",
      "iteration:  541 \teval_rewards:  1781.9428795726894\n",
      "iteration:  542 \teval_rewards:  1834.899577290652\n",
      "iteration:  543 \teval_rewards:  1785.4043436655647\n",
      "iteration:  544 \teval_rewards:  1743.5312478488338\n",
      "iteration:  545 \teval_rewards:  394.83648517916714\n",
      "iteration:  546 \teval_rewards:  1624.0562809408766\n",
      "iteration:  547 \teval_rewards:  1576.9710242833894\n",
      "iteration:  548 \teval_rewards:  1647.513807435641\n",
      "iteration:  549 \teval_rewards:  1721.7563412772708\n",
      "iteration:  550 \teval_rewards:  1743.8087800138762\n",
      "iteration:  551 \teval_rewards:  1781.7859662812737\n",
      "iteration:  552 \teval_rewards:  1778.0007213422284\n",
      "iteration:  553 \teval_rewards:  1625.325662229833\n",
      "iteration:  554 \teval_rewards:  1772.8972338114686\n",
      "iteration:  555 \teval_rewards:  1749.1676283751053\n",
      "iteration:  556 \teval_rewards:  1148.5533553042742\n",
      "iteration:  557 \teval_rewards:  29.330518478637195\n",
      "iteration:  558 \teval_rewards:  1749.082023077184\n",
      "iteration:  559 \teval_rewards:  212.86887423076064\n",
      "iteration:  560 \teval_rewards:  1656.9844611151464\n",
      "iteration:  561 \teval_rewards:  433.989451673996\n",
      "iteration:  562 \teval_rewards:  728.005063433427\n",
      "iteration:  563 \teval_rewards:  79.55406499963182\n",
      "iteration:  564 \teval_rewards:  1597.9690538298946\n",
      "iteration:  565 \teval_rewards:  450.86705543671223\n",
      "iteration:  566 \teval_rewards:  1647.6358159318117\n",
      "iteration:  567 \teval_rewards:  1722.457650974831\n",
      "iteration:  568 \teval_rewards:  1657.6124524650256\n",
      "iteration:  569 \teval_rewards:  927.9191824468593\n",
      "iteration:  570 \teval_rewards:  1629.5831129881815\n",
      "iteration:  571 \teval_rewards:  1610.7159841312919\n",
      "iteration:  572 \teval_rewards:  1769.411720116835\n",
      "iteration:  573 \teval_rewards:  522.6611123151288\n",
      "iteration:  574 \teval_rewards:  240.854244603583\n",
      "iteration:  575 \teval_rewards:  1650.7610589014034\n",
      "iteration:  576 \teval_rewards:  601.4411461061582\n",
      "iteration:  577 \teval_rewards:  1719.2407615336015\n",
      "iteration:  578 \teval_rewards:  1483.7905904334052\n",
      "iteration:  579 \teval_rewards:  1621.9243264424672\n",
      "iteration:  580 \teval_rewards:  876.7175099738395\n",
      "iteration:  581 \teval_rewards:  -33.681615968573865\n",
      "iteration:  582 \teval_rewards:  9.29597830539945\n",
      "iteration:  583 \teval_rewards:  1778.29128469292\n",
      "iteration:  584 \teval_rewards:  1765.7596172687674\n",
      "iteration:  585 \teval_rewards:  330.8638235128733\n",
      "iteration:  586 \teval_rewards:  431.69122427834577\n",
      "iteration:  587 \teval_rewards:  602.7832866241121\n",
      "iteration:  588 \teval_rewards:  68.89073569745722\n",
      "iteration:  589 \teval_rewards:  417.2081772654547\n",
      "iteration:  590 \teval_rewards:  58.82731087152184\n",
      "iteration:  591 \teval_rewards:  1049.6651655449741\n",
      "iteration:  592 \teval_rewards:  1676.8759944918509\n",
      "iteration:  593 \teval_rewards:  803.1869227951124\n",
      "iteration:  594 \teval_rewards:  89.667866060961\n",
      "iteration:  595 \teval_rewards:  511.7557391372018\n",
      "iteration:  596 \teval_rewards:  865.8899162059051\n",
      "iteration:  597 \teval_rewards:  1561.3978139604844\n",
      "iteration:  598 \teval_rewards:  871.2173680658208\n",
      "iteration:  599 \teval_rewards:  1721.4169901315934\n",
      "Iter:600| Ep_Reward:1694.702| Running_reward:1240.175| Actor_Loss:-0.196| Critic_Loss:95.990| Iter_duration:4.302| lr:[0.00017999999999999998]\n",
      "iteration:  600 \teval_rewards:  1694.7018069309333\n",
      "iteration:  601 \teval_rewards:  1425.6214729052274\n",
      "iteration:  602 \teval_rewards:  173.5405048213633\n",
      "iteration:  603 \teval_rewards:  1273.902315870445\n",
      "iteration:  604 \teval_rewards:  -281.45449394867234\n",
      "iteration:  605 \teval_rewards:  247.56423831469064\n",
      "iteration:  606 \teval_rewards:  809.2282432551996\n",
      "iteration:  607 \teval_rewards:  792.4067253102056\n",
      "iteration:  608 \teval_rewards:  1674.1119279036\n",
      "iteration:  609 \teval_rewards:  1672.275408360285\n",
      "iteration:  610 \teval_rewards:  1139.2695935214913\n",
      "iteration:  611 \teval_rewards:  648.6908662437422\n",
      "iteration:  612 \teval_rewards:  1648.195027850093\n",
      "iteration:  613 \teval_rewards:  1843.6043192127822\n",
      "iteration:  614 \teval_rewards:  1655.3975945418333\n",
      "iteration:  615 \teval_rewards:  1839.6518528385063\n",
      "iteration:  616 \teval_rewards:  1551.7121600559558\n",
      "iteration:  617 \teval_rewards:  5.417380610122258\n",
      "iteration:  618 \teval_rewards:  92.90665532150275\n",
      "iteration:  619 \teval_rewards:  806.1417458808878\n",
      "iteration:  620 \teval_rewards:  806.8148270209664\n",
      "iteration:  621 \teval_rewards:  1668.2029613204425\n",
      "iteration:  622 \teval_rewards:  565.401703750078\n",
      "iteration:  623 \teval_rewards:  1740.768355235359\n",
      "iteration:  624 \teval_rewards:  1629.4550272264023\n",
      "iteration:  625 \teval_rewards:  599.9638485832337\n",
      "iteration:  626 \teval_rewards:  755.3223885011587\n",
      "iteration:  627 \teval_rewards:  -433.8743912736383\n",
      "iteration:  628 \teval_rewards:  297.7476397806443\n",
      "iteration:  629 \teval_rewards:  1617.961012697663\n",
      "iteration:  630 \teval_rewards:  531.2612145327773\n",
      "iteration:  631 \teval_rewards:  1691.7288972616777\n",
      "iteration:  632 \teval_rewards:  1762.1541659240556\n",
      "iteration:  633 \teval_rewards:  1766.8894512172894\n",
      "iteration:  634 \teval_rewards:  1757.6230720612764\n",
      "iteration:  635 \teval_rewards:  381.6551571610739\n",
      "iteration:  636 \teval_rewards:  -218.11161965190485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  637 \teval_rewards:  1737.6356885604973\n",
      "iteration:  638 \teval_rewards:  1478.54237879476\n",
      "iteration:  639 \teval_rewards:  168.76441133194348\n",
      "iteration:  640 \teval_rewards:  1646.1443061355806\n",
      "iteration:  641 \teval_rewards:  1726.8613435287873\n",
      "iteration:  642 \teval_rewards:  1675.2985515076534\n",
      "iteration:  643 \teval_rewards:  1762.9622270088255\n",
      "iteration:  644 \teval_rewards:  1319.6066573554015\n",
      "iteration:  645 \teval_rewards:  1635.2457821217365\n",
      "iteration:  646 \teval_rewards:  1783.2379334184661\n",
      "iteration:  647 \teval_rewards:  474.83300317818515\n",
      "iteration:  648 \teval_rewards:  1803.3737672728578\n",
      "iteration:  649 \teval_rewards:  1733.2098139852912\n",
      "iteration:  650 \teval_rewards:  1811.8738433521773\n",
      "iteration:  651 \teval_rewards:  1571.3644112346037\n",
      "iteration:  652 \teval_rewards:  1753.893135083931\n",
      "iteration:  653 \teval_rewards:  1581.8212163044602\n",
      "iteration:  654 \teval_rewards:  1655.2081000755145\n",
      "iteration:  655 \teval_rewards:  1744.6815387044676\n",
      "iteration:  656 \teval_rewards:  374.0491009937839\n",
      "iteration:  657 \teval_rewards:  -1620.796731420367\n",
      "iteration:  658 \teval_rewards:  1823.1174362094407\n",
      "iteration:  659 \teval_rewards:  906.4573145032024\n",
      "iteration:  660 \teval_rewards:  1286.7216956170685\n",
      "iteration:  661 \teval_rewards:  344.02172669642897\n",
      "iteration:  662 \teval_rewards:  1706.1454080383087\n",
      "iteration:  663 \teval_rewards:  1735.7806015800513\n",
      "iteration:  664 \teval_rewards:  152.23695702835818\n",
      "iteration:  665 \teval_rewards:  1077.882887593166\n",
      "iteration:  666 \teval_rewards:  1725.9386358394677\n",
      "iteration:  667 \teval_rewards:  1751.5355144734604\n",
      "iteration:  668 \teval_rewards:  1783.1770568472666\n",
      "iteration:  669 \teval_rewards:  1684.4585255027641\n",
      "iteration:  670 \teval_rewards:  450.91683369945406\n",
      "iteration:  671 \teval_rewards:  1775.25517559532\n",
      "iteration:  672 \teval_rewards:  1859.9325786733868\n",
      "iteration:  673 \teval_rewards:  210.41850367808993\n",
      "iteration:  674 \teval_rewards:  267.7012664501643\n",
      "iteration:  675 \teval_rewards:  1723.0339203324907\n",
      "iteration:  676 \teval_rewards:  1729.39407462174\n",
      "iteration:  677 \teval_rewards:  1645.2050545230202\n",
      "iteration:  678 \teval_rewards:  1688.9376759433733\n",
      "iteration:  679 \teval_rewards:  1849.3151688120115\n",
      "iteration:  680 \teval_rewards:  1764.7925614755711\n",
      "iteration:  681 \teval_rewards:  1890.0359807913612\n",
      "iteration:  682 \teval_rewards:  910.1494324184268\n",
      "iteration:  683 \teval_rewards:  262.0878415407092\n",
      "iteration:  684 \teval_rewards:  34.683002904665265\n",
      "iteration:  685 \teval_rewards:  1748.3908657350034\n",
      "iteration:  686 \teval_rewards:  1888.901921380268\n",
      "iteration:  687 \teval_rewards:  1882.5191295959864\n",
      "iteration:  688 \teval_rewards:  1812.0709034944412\n",
      "iteration:  689 \teval_rewards:  1807.3366513496896\n",
      "iteration:  690 \teval_rewards:  770.4119312443903\n",
      "iteration:  691 \teval_rewards:  1775.1711929266794\n",
      "iteration:  692 \teval_rewards:  1754.9201555045486\n",
      "iteration:  693 \teval_rewards:  1765.818908660177\n",
      "iteration:  694 \teval_rewards:  1752.5578704910888\n",
      "iteration:  695 \teval_rewards:  1750.4900359855083\n",
      "iteration:  696 \teval_rewards:  1744.188616276549\n",
      "iteration:  697 \teval_rewards:  586.5914667787063\n",
      "iteration:  698 \teval_rewards:  710.2139749967384\n",
      "iteration:  699 \teval_rewards:  544.0973919576566\n",
      "Iter:700| Ep_Reward:522.198| Running_reward:1238.514| Actor_Loss:-0.148| Critic_Loss:62.775| Iter_duration:4.332| lr:[0.00015999999999999999]\n",
      "iteration:  700 \teval_rewards:  522.19754085172\n",
      "iteration:  701 \teval_rewards:  1681.9528187796104\n",
      "iteration:  702 \teval_rewards:  1490.5403676806059\n",
      "iteration:  703 \teval_rewards:  1645.1504049519322\n",
      "iteration:  704 \teval_rewards:  4.066530054610723\n",
      "iteration:  705 \teval_rewards:  1584.7807231925062\n",
      "iteration:  706 \teval_rewards:  1742.6665798276033\n",
      "iteration:  707 \teval_rewards:  649.4338427039042\n",
      "iteration:  708 \teval_rewards:  1806.8383294699931\n",
      "iteration:  709 \teval_rewards:  1584.210499896328\n",
      "iteration:  710 \teval_rewards:  1676.9556087313488\n",
      "iteration:  711 \teval_rewards:  629.3523539378498\n",
      "iteration:  712 \teval_rewards:  1803.5322653722637\n",
      "iteration:  713 \teval_rewards:  1530.642153450574\n",
      "iteration:  714 \teval_rewards:  1662.26261407381\n",
      "iteration:  715 \teval_rewards:  1747.9924969231763\n",
      "iteration:  716 \teval_rewards:  1747.0527714587151\n",
      "iteration:  717 \teval_rewards:  536.5564573913986\n",
      "iteration:  718 \teval_rewards:  1586.4318756576615\n",
      "iteration:  719 \teval_rewards:  736.1605575609193\n",
      "iteration:  720 \teval_rewards:  1627.6612993449394\n",
      "iteration:  721 \teval_rewards:  1484.988975945331\n",
      "iteration:  722 \teval_rewards:  1169.5542395167092\n",
      "iteration:  723 \teval_rewards:  1553.3622266674129\n",
      "iteration:  724 \teval_rewards:  1159.450757170634\n",
      "iteration:  725 \teval_rewards:  1540.5340537873435\n",
      "iteration:  726 \teval_rewards:  2.493801276169842\n",
      "iteration:  727 \teval_rewards:  1048.8106959217084\n",
      "iteration:  728 \teval_rewards:  1641.7803963450733\n",
      "iteration:  729 \teval_rewards:  -89.3739065760778\n",
      "iteration:  730 \teval_rewards:  175.8034578728536\n",
      "iteration:  731 \teval_rewards:  841.8527867707393\n",
      "iteration:  732 \teval_rewards:  -248.3606267434385\n",
      "iteration:  733 \teval_rewards:  1656.8562812646085\n",
      "iteration:  734 \teval_rewards:  1660.7474508618402\n",
      "iteration:  735 \teval_rewards:  1781.9499510541243\n",
      "iteration:  736 \teval_rewards:  672.4260477758633\n",
      "iteration:  737 \teval_rewards:  1738.9046570092132\n",
      "iteration:  738 \teval_rewards:  1716.1469466257804\n",
      "iteration:  739 \teval_rewards:  1665.935258984001\n",
      "iteration:  740 \teval_rewards:  1781.0326433724051\n",
      "iteration:  741 \teval_rewards:  1820.674157824039\n",
      "iteration:  742 \teval_rewards:  185.46468126855\n",
      "iteration:  743 \teval_rewards:  1677.9725780223735\n",
      "iteration:  744 \teval_rewards:  1433.6558828238926\n",
      "iteration:  745 \teval_rewards:  190.79143857668484\n",
      "iteration:  746 \teval_rewards:  200.30241497306105\n",
      "iteration:  747 \teval_rewards:  1691.829813314445\n",
      "iteration:  748 \teval_rewards:  1743.662582074648\n",
      "iteration:  749 \teval_rewards:  308.9214596370849\n",
      "iteration:  750 \teval_rewards:  1737.5756877592357\n",
      "iteration:  751 \teval_rewards:  1712.7994663039622\n",
      "iteration:  752 \teval_rewards:  1735.9356236791962\n",
      "iteration:  753 \teval_rewards:  1816.002014301839\n",
      "iteration:  754 \teval_rewards:  1699.514753516751\n",
      "iteration:  755 \teval_rewards:  1678.094056373622\n",
      "iteration:  756 \teval_rewards:  1834.685955386571\n",
      "iteration:  757 \teval_rewards:  254.8688035193013\n",
      "iteration:  758 \teval_rewards:  723.4337988799347\n",
      "iteration:  759 \teval_rewards:  541.55839405235\n",
      "iteration:  760 \teval_rewards:  1785.0114095204617\n",
      "iteration:  761 \teval_rewards:  726.9381839546281\n",
      "iteration:  762 \teval_rewards:  1776.6520098765666\n",
      "iteration:  763 \teval_rewards:  1773.0130696843135\n",
      "iteration:  764 \teval_rewards:  1636.0870713987274\n",
      "iteration:  765 \teval_rewards:  1703.6204517634073\n",
      "iteration:  766 \teval_rewards:  1052.355224257623\n",
      "iteration:  767 \teval_rewards:  449.9522181293898\n",
      "iteration:  768 \teval_rewards:  919.5649799911339\n",
      "iteration:  769 \teval_rewards:  459.32113577121254\n",
      "iteration:  770 \teval_rewards:  1597.3264163739286\n",
      "iteration:  771 \teval_rewards:  1762.07912500134\n",
      "iteration:  772 \teval_rewards:  1751.2851454410907\n",
      "iteration:  773 \teval_rewards:  1699.4497881246075\n",
      "iteration:  774 \teval_rewards:  1735.9707237226198\n",
      "iteration:  775 \teval_rewards:  1666.1624664571095\n",
      "iteration:  776 \teval_rewards:  139.75644804538533\n",
      "iteration:  777 \teval_rewards:  762.8280255886989\n",
      "iteration:  778 \teval_rewards:  1705.8924808879383\n",
      "iteration:  779 \teval_rewards:  1485.7612057543492\n",
      "iteration:  780 \teval_rewards:  1543.3452233665473\n",
      "iteration:  781 \teval_rewards:  1598.2604891781223\n",
      "iteration:  782 \teval_rewards:  524.8246569471199\n",
      "iteration:  783 \teval_rewards:  58.587558347164325\n",
      "iteration:  784 \teval_rewards:  1796.2937560390596\n",
      "iteration:  785 \teval_rewards:  1731.5199304358437\n",
      "iteration:  786 \teval_rewards:  1663.9576814609804\n",
      "iteration:  787 \teval_rewards:  476.96123563326984\n",
      "iteration:  788 \teval_rewards:  1707.1415296667014\n",
      "iteration:  789 \teval_rewards:  1398.1639608726489\n",
      "iteration:  790 \teval_rewards:  1617.648175293078\n",
      "iteration:  791 \teval_rewards:  1111.5893346078922\n",
      "iteration:  792 \teval_rewards:  981.6847962966954\n",
      "iteration:  793 \teval_rewards:  1577.5375932759985\n",
      "iteration:  794 \teval_rewards:  1781.871847289527\n",
      "iteration:  795 \teval_rewards:  1195.075272030398\n",
      "iteration:  796 \teval_rewards:  1819.0611710030053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  797 \teval_rewards:  180.43216690459022\n",
      "iteration:  798 \teval_rewards:  1719.3733461780266\n",
      "iteration:  799 \teval_rewards:  1769.5988526906767\n",
      "Iter:800| Ep_Reward:730.702| Running_reward:1260.253| Actor_Loss:0.170| Critic_Loss:218.975| Iter_duration:4.356| lr:[0.00014]\n",
      "iteration:  800 \teval_rewards:  730.7022502519055\n",
      "iteration:  801 \teval_rewards:  -121.86190881809671\n",
      "iteration:  802 \teval_rewards:  1038.2644188409222\n",
      "iteration:  803 \teval_rewards:  936.217198696749\n",
      "iteration:  804 \teval_rewards:  1217.91582765137\n",
      "iteration:  805 \teval_rewards:  211.5054477328272\n",
      "iteration:  806 \teval_rewards:  584.3069115325052\n",
      "iteration:  807 \teval_rewards:  1644.35931563396\n",
      "iteration:  808 \teval_rewards:  1737.3283079306623\n",
      "iteration:  809 \teval_rewards:  558.1389736366655\n",
      "iteration:  810 \teval_rewards:  1745.5456310832583\n",
      "iteration:  811 \teval_rewards:  55.70642089770466\n",
      "iteration:  812 \teval_rewards:  1608.7692446834317\n",
      "iteration:  813 \teval_rewards:  1045.65576589734\n",
      "iteration:  814 \teval_rewards:  1617.7039447016668\n",
      "iteration:  815 \teval_rewards:  1666.611081173759\n",
      "iteration:  816 \teval_rewards:  1765.5057648938737\n",
      "iteration:  817 \teval_rewards:  264.61523894916724\n",
      "iteration:  818 \teval_rewards:  1768.6233789718808\n",
      "iteration:  819 \teval_rewards:  1309.1908343806547\n",
      "iteration:  820 \teval_rewards:  1758.849854071427\n",
      "iteration:  821 \teval_rewards:  1799.9488635954544\n",
      "iteration:  822 \teval_rewards:  1014.3233676328674\n",
      "iteration:  823 \teval_rewards:  1772.1762744278778\n",
      "iteration:  824 \teval_rewards:  29.9857416689702\n",
      "iteration:  825 \teval_rewards:  1233.5857849358138\n",
      "iteration:  826 \teval_rewards:  842.8105531464191\n",
      "iteration:  827 \teval_rewards:  1840.1219761654934\n",
      "iteration:  828 \teval_rewards:  1711.6325543220737\n",
      "iteration:  829 \teval_rewards:  937.238305668031\n",
      "iteration:  830 \teval_rewards:  1335.8220578917555\n",
      "iteration:  831 \teval_rewards:  685.1057265311362\n",
      "iteration:  832 \teval_rewards:  698.3957588685124\n",
      "iteration:  833 \teval_rewards:  254.88323624957633\n",
      "iteration:  834 \teval_rewards:  1769.298779381637\n",
      "iteration:  835 \teval_rewards:  1862.4140906305759\n",
      "iteration:  836 \teval_rewards:  117.17997593467706\n",
      "iteration:  837 \teval_rewards:  1053.4135040938725\n",
      "iteration:  838 \teval_rewards:  1833.9571766382305\n",
      "iteration:  839 \teval_rewards:  779.740030428537\n",
      "iteration:  840 \teval_rewards:  1840.239550322135\n",
      "iteration:  841 \teval_rewards:  1487.721488390104\n",
      "iteration:  842 \teval_rewards:  1639.475653800222\n",
      "iteration:  843 \teval_rewards:  1055.1852142997886\n",
      "iteration:  844 \teval_rewards:  916.9530955018993\n",
      "iteration:  845 \teval_rewards:  1860.0027293629169\n",
      "iteration:  846 \teval_rewards:  1790.4306634941167\n",
      "iteration:  847 \teval_rewards:  1794.2260966974004\n",
      "iteration:  848 \teval_rewards:  127.80324565945334\n",
      "iteration:  849 \teval_rewards:  -141.90205857740486\n",
      "iteration:  850 \teval_rewards:  1703.9477972393438\n",
      "iteration:  851 \teval_rewards:  1642.6513337305748\n",
      "iteration:  852 \teval_rewards:  1851.16610949676\n",
      "iteration:  853 \teval_rewards:  640.9299134860961\n",
      "iteration:  854 \teval_rewards:  1311.0180501287987\n",
      "iteration:  855 \teval_rewards:  1816.677868562471\n",
      "iteration:  856 \teval_rewards:  1807.7928105879002\n",
      "iteration:  857 \teval_rewards:  1114.0104195880951\n",
      "iteration:  858 \teval_rewards:  1850.2556944120722\n",
      "iteration:  859 \teval_rewards:  1831.3485399818658\n",
      "iteration:  860 \teval_rewards:  1751.1192986508872\n",
      "iteration:  861 \teval_rewards:  661.867999921254\n",
      "iteration:  862 \teval_rewards:  1784.2686071403864\n",
      "iteration:  863 \teval_rewards:  1765.172820278658\n",
      "iteration:  864 \teval_rewards:  1784.7515291665309\n",
      "iteration:  865 \teval_rewards:  1702.6352135814543\n",
      "iteration:  866 \teval_rewards:  1780.6711946288963\n",
      "iteration:  867 \teval_rewards:  1316.1032273282792\n",
      "iteration:  868 \teval_rewards:  431.08713656780753\n",
      "iteration:  869 \teval_rewards:  1814.855705095856\n",
      "iteration:  870 \teval_rewards:  1174.6183360918524\n",
      "iteration:  871 \teval_rewards:  175.46013981769062\n",
      "iteration:  872 \teval_rewards:  1825.9511083900493\n",
      "iteration:  873 \teval_rewards:  1791.430494618834\n",
      "iteration:  874 \teval_rewards:  1907.6725783294319\n",
      "iteration:  875 \teval_rewards:  1517.4178502970503\n",
      "iteration:  876 \teval_rewards:  1883.1598728512347\n",
      "iteration:  877 \teval_rewards:  1726.9878192829196\n",
      "iteration:  878 \teval_rewards:  186.29878103015932\n",
      "iteration:  879 \teval_rewards:  230.66895355398213\n",
      "iteration:  880 \teval_rewards:  523.9853462967626\n",
      "iteration:  881 \teval_rewards:  404.87275910763276\n",
      "iteration:  882 \teval_rewards:  1880.6975476704356\n",
      "iteration:  883 \teval_rewards:  1384.8789823456154\n",
      "iteration:  884 \teval_rewards:  1738.8306947060478\n",
      "iteration:  885 \teval_rewards:  498.1931943573446\n",
      "iteration:  886 \teval_rewards:  1748.689399370587\n",
      "iteration:  887 \teval_rewards:  1779.6747029365715\n",
      "iteration:  888 \teval_rewards:  1776.3033227952512\n",
      "iteration:  889 \teval_rewards:  893.7720546386781\n",
      "iteration:  890 \teval_rewards:  1802.9372238883784\n",
      "iteration:  891 \teval_rewards:  1845.5360319915671\n",
      "iteration:  892 \teval_rewards:  1708.054622828692\n",
      "iteration:  893 \teval_rewards:  213.01350080749202\n",
      "iteration:  894 \teval_rewards:  1869.2736400568356\n",
      "iteration:  895 \teval_rewards:  1730.3537917165286\n",
      "iteration:  896 \teval_rewards:  1847.172969405392\n",
      "iteration:  897 \teval_rewards:  1686.0768792327458\n",
      "iteration:  898 \teval_rewards:  1644.2483683404967\n",
      "iteration:  899 \teval_rewards:  1706.4372803087078\n",
      "Iter:900| Ep_Reward:1719.442| Running_reward:1306.517| Actor_Loss:-0.163| Critic_Loss:67.388| Iter_duration:4.177| lr:[0.00011999999999999999]\n",
      "iteration:  900 \teval_rewards:  1719.4415609245345\n",
      "iteration:  901 \teval_rewards:  1148.2351964126512\n",
      "iteration:  902 \teval_rewards:  1748.605149414127\n",
      "iteration:  903 \teval_rewards:  1732.1120146715443\n",
      "iteration:  904 \teval_rewards:  1809.0711874729109\n",
      "iteration:  905 \teval_rewards:  1888.0141793192242\n",
      "iteration:  906 \teval_rewards:  1772.080171352207\n",
      "iteration:  907 \teval_rewards:  1841.535229903909\n",
      "iteration:  908 \teval_rewards:  590.0334197358421\n",
      "iteration:  909 \teval_rewards:  1736.9601303018621\n",
      "iteration:  910 \teval_rewards:  933.9773220390445\n",
      "iteration:  911 \teval_rewards:  1741.2194712788196\n",
      "iteration:  912 \teval_rewards:  74.81555949596265\n",
      "iteration:  913 \teval_rewards:  1874.3830021473277\n",
      "iteration:  914 \teval_rewards:  1707.1646923860555\n",
      "iteration:  915 \teval_rewards:  598.0653942513625\n",
      "iteration:  916 \teval_rewards:  1897.8424650470665\n",
      "iteration:  917 \teval_rewards:  1848.0035558718278\n",
      "iteration:  918 \teval_rewards:  1689.0352477526085\n",
      "iteration:  919 \teval_rewards:  1964.4329238500673\n",
      "iteration:  920 \teval_rewards:  880.9199112327359\n",
      "iteration:  921 \teval_rewards:  1809.6138188491668\n",
      "iteration:  922 \teval_rewards:  1836.3235388492753\n",
      "iteration:  923 \teval_rewards:  1911.5989413510642\n",
      "iteration:  924 \teval_rewards:  433.2150196049505\n",
      "iteration:  925 \teval_rewards:  1839.6390783068932\n",
      "iteration:  926 \teval_rewards:  1737.1715128875444\n",
      "iteration:  927 \teval_rewards:  144.63623527857445\n",
      "iteration:  928 \teval_rewards:  154.25475835007006\n",
      "iteration:  929 \teval_rewards:  1868.9822295425552\n",
      "iteration:  930 \teval_rewards:  1827.5152877956382\n",
      "iteration:  931 \teval_rewards:  1912.2688901713902\n",
      "iteration:  932 \teval_rewards:  445.4235946770611\n",
      "iteration:  933 \teval_rewards:  1779.6254370522859\n",
      "iteration:  934 \teval_rewards:  1818.4880558942996\n",
      "iteration:  935 \teval_rewards:  1773.815731849415\n",
      "iteration:  936 \teval_rewards:  1753.4180126915094\n",
      "iteration:  937 \teval_rewards:  890.2773182848586\n",
      "iteration:  938 \teval_rewards:  1280.0839908301534\n",
      "iteration:  939 \teval_rewards:  1825.9780891769133\n",
      "iteration:  940 \teval_rewards:  1894.9669981463724\n",
      "iteration:  941 \teval_rewards:  1833.8098870623662\n",
      "iteration:  942 \teval_rewards:  1891.4636161473134\n",
      "iteration:  943 \teval_rewards:  1785.2506800956937\n",
      "iteration:  944 \teval_rewards:  1899.5220574715781\n",
      "iteration:  945 \teval_rewards:  882.9169653772173\n",
      "iteration:  946 \teval_rewards:  1829.0252044187682\n",
      "iteration:  947 \teval_rewards:  683.2199103540479\n",
      "iteration:  948 \teval_rewards:  1748.832698296573\n",
      "iteration:  949 \teval_rewards:  1102.5534985992956\n",
      "iteration:  950 \teval_rewards:  567.0663266147546\n",
      "iteration:  951 \teval_rewards:  875.6229511309616\n",
      "iteration:  952 \teval_rewards:  162.41896311939848\n",
      "iteration:  953 \teval_rewards:  1915.0312458111935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  954 \teval_rewards:  1928.7190704030186\n",
      "iteration:  955 \teval_rewards:  1882.019427439072\n",
      "iteration:  956 \teval_rewards:  1993.680252190299\n",
      "iteration:  957 \teval_rewards:  1822.9964807629462\n",
      "iteration:  958 \teval_rewards:  369.4701660674845\n",
      "iteration:  959 \teval_rewards:  445.2965710405668\n",
      "iteration:  960 \teval_rewards:  1370.314007825527\n",
      "iteration:  961 \teval_rewards:  1999.73147713471\n",
      "iteration:  962 \teval_rewards:  1914.2977445115366\n",
      "iteration:  963 \teval_rewards:  1943.0010623278786\n",
      "iteration:  964 \teval_rewards:  2036.9840863613406\n",
      "iteration:  965 \teval_rewards:  2051.5291944217706\n",
      "iteration:  966 \teval_rewards:  1267.993988411627\n",
      "iteration:  967 \teval_rewards:  602.2886315950731\n",
      "iteration:  968 \teval_rewards:  1809.6608670928763\n",
      "iteration:  969 \teval_rewards:  960.8275790147014\n",
      "iteration:  970 \teval_rewards:  1797.2972582242937\n",
      "iteration:  971 \teval_rewards:  821.4601295658917\n",
      "iteration:  972 \teval_rewards:  1815.7091426801835\n",
      "iteration:  973 \teval_rewards:  1980.9596335164258\n",
      "iteration:  974 \teval_rewards:  336.00752200917185\n",
      "iteration:  975 \teval_rewards:  1777.0024233604893\n",
      "iteration:  976 \teval_rewards:  2021.7324385393822\n",
      "iteration:  977 \teval_rewards:  1827.0765118927113\n",
      "iteration:  978 \teval_rewards:  1850.8931342087144\n",
      "iteration:  979 \teval_rewards:  1926.3275124964384\n",
      "iteration:  980 \teval_rewards:  597.1158223935599\n",
      "iteration:  981 \teval_rewards:  1697.7267328228907\n",
      "iteration:  982 \teval_rewards:  1857.686490223151\n",
      "iteration:  983 \teval_rewards:  1741.139366212004\n",
      "iteration:  984 \teval_rewards:  1867.3740328077602\n",
      "iteration:  985 \teval_rewards:  1841.8888689761375\n",
      "iteration:  986 \teval_rewards:  2063.1383933365446\n",
      "iteration:  987 \teval_rewards:  1840.1490070668433\n",
      "iteration:  988 \teval_rewards:  1906.434169496399\n",
      "iteration:  989 \teval_rewards:  1103.7501407431678\n",
      "iteration:  990 \teval_rewards:  1988.1957020287762\n",
      "iteration:  991 \teval_rewards:  985.8199795655864\n",
      "iteration:  992 \teval_rewards:  2009.2410980613897\n",
      "iteration:  993 \teval_rewards:  1947.1359807081485\n",
      "iteration:  994 \teval_rewards:  1878.1308932179263\n",
      "iteration:  995 \teval_rewards:  1882.2776164529666\n",
      "iteration:  996 \teval_rewards:  1852.8343282447117\n",
      "iteration:  997 \teval_rewards:  1772.1560709149517\n",
      "iteration:  998 \teval_rewards:  1875.7956323881108\n",
      "iteration:  999 \teval_rewards:  1712.294042984191\n",
      "Iter:1000| Ep_Reward:152.079| Running_reward:1446.267| Actor_Loss:-0.125| Critic_Loss:43.607| Iter_duration:3.521| lr:[0.0001]\n",
      "iteration:  1000 \teval_rewards:  152.0787826393128\n",
      "iteration:  1001 \teval_rewards:  1357.3868102407362\n",
      "iteration:  1002 \teval_rewards:  40.531058677641745\n",
      "iteration:  1003 \teval_rewards:  238.41134029913934\n",
      "iteration:  1004 \teval_rewards:  1907.655400878837\n",
      "iteration:  1005 \teval_rewards:  1890.1874731488379\n",
      "iteration:  1006 \teval_rewards:  1823.1036432948413\n",
      "iteration:  1007 \teval_rewards:  1862.3742467652203\n",
      "iteration:  1008 \teval_rewards:  1805.1203931317295\n",
      "iteration:  1009 \teval_rewards:  1215.1185272428584\n",
      "iteration:  1010 \teval_rewards:  674.0110986893762\n",
      "iteration:  1011 \teval_rewards:  1853.761690383529\n",
      "iteration:  1012 \teval_rewards:  446.3569208255996\n",
      "iteration:  1013 \teval_rewards:  1856.0078521300832\n",
      "iteration:  1014 \teval_rewards:  1679.107531737278\n",
      "iteration:  1015 \teval_rewards:  1816.9457396371017\n",
      "iteration:  1016 \teval_rewards:  1888.9483378539694\n",
      "iteration:  1017 \teval_rewards:  1932.2926691087287\n",
      "iteration:  1018 \teval_rewards:  1850.8621566035035\n",
      "iteration:  1019 \teval_rewards:  1666.2733610624855\n",
      "iteration:  1020 \teval_rewards:  742.2398901469194\n",
      "iteration:  1021 \teval_rewards:  1902.571081076648\n",
      "iteration:  1022 \teval_rewards:  1807.7584227300572\n",
      "iteration:  1023 \teval_rewards:  402.6998852335311\n",
      "iteration:  1024 \teval_rewards:  181.75802096534832\n",
      "iteration:  1025 \teval_rewards:  1449.0264375438983\n",
      "iteration:  1026 \teval_rewards:  1831.1751283479398\n",
      "iteration:  1027 \teval_rewards:  1756.9572572556492\n",
      "iteration:  1028 \teval_rewards:  1428.9965441817158\n",
      "iteration:  1029 \teval_rewards:  1814.0929989434555\n",
      "iteration:  1030 \teval_rewards:  1330.8671323035783\n",
      "iteration:  1031 \teval_rewards:  1663.9178692901316\n",
      "iteration:  1032 \teval_rewards:  1798.5246674365158\n",
      "iteration:  1033 \teval_rewards:  1679.1824391317496\n",
      "iteration:  1034 \teval_rewards:  847.906849875254\n",
      "iteration:  1035 \teval_rewards:  1652.4838588165414\n",
      "iteration:  1036 \teval_rewards:  1305.3619080636588\n",
      "iteration:  1037 \teval_rewards:  1665.0130960141726\n",
      "iteration:  1038 \teval_rewards:  1247.091430831052\n",
      "iteration:  1039 \teval_rewards:  766.0957639320611\n",
      "iteration:  1040 \teval_rewards:  1771.509139277407\n",
      "iteration:  1041 \teval_rewards:  1819.4534476410124\n",
      "iteration:  1042 \teval_rewards:  1901.3804849745447\n",
      "iteration:  1043 \teval_rewards:  1905.9078451864643\n",
      "iteration:  1044 \teval_rewards:  1896.9783387823134\n",
      "iteration:  1045 \teval_rewards:  1754.0576913632083\n",
      "iteration:  1046 \teval_rewards:  1821.6481765815379\n",
      "iteration:  1047 \teval_rewards:  580.7934958367524\n",
      "iteration:  1048 \teval_rewards:  1958.458723345296\n",
      "iteration:  1049 \teval_rewards:  25.739510141728932\n",
      "iteration:  1050 \teval_rewards:  1386.6430977263276\n",
      "iteration:  1051 \teval_rewards:  1484.0088715702125\n",
      "iteration:  1052 \teval_rewards:  1803.2710286173851\n",
      "iteration:  1053 \teval_rewards:  176.74943070833606\n",
      "iteration:  1054 \teval_rewards:  986.9309885598781\n",
      "iteration:  1055 \teval_rewards:  1831.2968947805907\n",
      "iteration:  1056 \teval_rewards:  1970.0251535729187\n",
      "iteration:  1057 \teval_rewards:  818.7051391352383\n",
      "iteration:  1058 \teval_rewards:  238.1905513389768\n",
      "iteration:  1059 \teval_rewards:  1970.4763593329485\n",
      "iteration:  1060 \teval_rewards:  1809.2743449436725\n",
      "iteration:  1061 \teval_rewards:  1834.8222389014581\n",
      "iteration:  1062 \teval_rewards:  875.8408604239759\n",
      "iteration:  1063 \teval_rewards:  1856.1187657047358\n",
      "iteration:  1064 \teval_rewards:  1956.780921081218\n",
      "iteration:  1065 \teval_rewards:  305.21299045691626\n",
      "iteration:  1066 \teval_rewards:  1826.5076862162969\n",
      "iteration:  1067 \teval_rewards:  1148.2776136219418\n",
      "iteration:  1068 \teval_rewards:  1765.5619166004005\n",
      "iteration:  1069 \teval_rewards:  377.5808851006118\n",
      "iteration:  1070 \teval_rewards:  1825.3828664104894\n",
      "iteration:  1071 \teval_rewards:  999.5292684393669\n",
      "iteration:  1072 \teval_rewards:  1062.8354991247847\n",
      "iteration:  1073 \teval_rewards:  1967.4994733481858\n",
      "iteration:  1074 \teval_rewards:  1961.6199206527995\n",
      "iteration:  1075 \teval_rewards:  1776.0002442324824\n",
      "iteration:  1076 \teval_rewards:  714.3063884444665\n",
      "iteration:  1077 \teval_rewards:  458.2233973677845\n",
      "iteration:  1078 \teval_rewards:  1943.9822555760702\n",
      "iteration:  1079 \teval_rewards:  1416.9419476171395\n",
      "iteration:  1080 \teval_rewards:  1856.6010270610886\n",
      "iteration:  1081 \teval_rewards:  1993.1861664727392\n",
      "iteration:  1082 \teval_rewards:  1905.8502374018635\n",
      "iteration:  1083 \teval_rewards:  147.70736164367477\n",
      "iteration:  1084 \teval_rewards:  1886.4568276285604\n",
      "iteration:  1085 \teval_rewards:  1314.0858150160618\n",
      "iteration:  1086 \teval_rewards:  71.77395983751862\n",
      "iteration:  1087 \teval_rewards:  786.1732286942299\n",
      "iteration:  1088 \teval_rewards:  1961.1121495729076\n",
      "iteration:  1089 \teval_rewards:  977.3341179804136\n",
      "iteration:  1090 \teval_rewards:  1920.5796440933118\n",
      "iteration:  1091 \teval_rewards:  1177.5198284596534\n",
      "iteration:  1092 \teval_rewards:  483.12428965026066\n",
      "iteration:  1093 \teval_rewards:  1162.5838325690456\n",
      "iteration:  1094 \teval_rewards:  545.0593070062084\n",
      "iteration:  1095 \teval_rewards:  1922.0610749635766\n",
      "iteration:  1096 \teval_rewards:  1821.8307114106533\n",
      "iteration:  1097 \teval_rewards:  560.2654484265398\n",
      "iteration:  1098 \teval_rewards:  1941.0531386642892\n",
      "iteration:  1099 \teval_rewards:  1336.6302216977342\n",
      "Iter:1100| Ep_Reward:1842.257| Running_reward:1401.081| Actor_Loss:0.104| Critic_Loss:297.026| Iter_duration:4.277| lr:[8e-05]\n",
      "iteration:  1100 \teval_rewards:  1842.2574170031098\n",
      "iteration:  1101 \teval_rewards:  1885.2965639686997\n",
      "iteration:  1102 \teval_rewards:  1760.9754865488283\n",
      "iteration:  1103 \teval_rewards:  520.9594057704959\n",
      "iteration:  1104 \teval_rewards:  1420.1825523212522\n",
      "iteration:  1105 \teval_rewards:  1938.1488742031472\n",
      "iteration:  1106 \teval_rewards:  1571.96254338316\n",
      "iteration:  1107 \teval_rewards:  1823.7291074672225\n",
      "iteration:  1108 \teval_rewards:  1366.533485190071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1109 \teval_rewards:  1819.3969168235685\n",
      "iteration:  1110 \teval_rewards:  1894.4949121006402\n",
      "iteration:  1111 \teval_rewards:  148.4469143190973\n",
      "iteration:  1112 \teval_rewards:  1914.8267996671623\n",
      "iteration:  1113 \teval_rewards:  1895.4057828895545\n",
      "iteration:  1114 \teval_rewards:  1858.7442610722337\n",
      "iteration:  1115 \teval_rewards:  1891.2377448486898\n",
      "iteration:  1116 \teval_rewards:  1789.6281001467767\n",
      "iteration:  1117 \teval_rewards:  1881.846617096011\n",
      "iteration:  1118 \teval_rewards:  355.3790272760983\n",
      "iteration:  1119 \teval_rewards:  1364.8623535520464\n",
      "iteration:  1120 \teval_rewards:  1810.102100986792\n",
      "iteration:  1121 \teval_rewards:  1749.1468065455638\n",
      "iteration:  1122 \teval_rewards:  1894.6750719358345\n",
      "iteration:  1123 \teval_rewards:  96.45365679829328\n",
      "iteration:  1124 \teval_rewards:  1791.831199811001\n",
      "iteration:  1125 \teval_rewards:  772.5000054017786\n",
      "iteration:  1126 \teval_rewards:  1822.9158463261747\n",
      "iteration:  1127 \teval_rewards:  1845.3601100623896\n",
      "iteration:  1128 \teval_rewards:  1812.582834665296\n",
      "iteration:  1129 \teval_rewards:  1871.0444132819678\n",
      "iteration:  1130 \teval_rewards:  1730.1929586036163\n",
      "iteration:  1131 \teval_rewards:  1854.6187351141539\n",
      "iteration:  1132 \teval_rewards:  1850.4015029779553\n",
      "iteration:  1133 \teval_rewards:  1556.2126234117793\n",
      "iteration:  1134 \teval_rewards:  1963.0718195825025\n",
      "iteration:  1135 \teval_rewards:  939.057078524739\n",
      "iteration:  1136 \teval_rewards:  1838.435134561319\n",
      "iteration:  1137 \teval_rewards:  1884.077315956715\n",
      "iteration:  1138 \teval_rewards:  1985.2545853688075\n",
      "iteration:  1139 \teval_rewards:  391.15800152205986\n",
      "iteration:  1140 \teval_rewards:  245.76662802247813\n",
      "iteration:  1141 \teval_rewards:  1720.691772396477\n",
      "iteration:  1142 \teval_rewards:  1948.266726403555\n",
      "iteration:  1143 \teval_rewards:  573.3123150290774\n",
      "iteration:  1144 \teval_rewards:  1883.3879429086383\n",
      "iteration:  1145 \teval_rewards:  645.4640187787952\n",
      "iteration:  1146 \teval_rewards:  1902.0193174938327\n",
      "iteration:  1147 \teval_rewards:  1161.1338067804402\n",
      "iteration:  1148 \teval_rewards:  1994.3971372914166\n",
      "iteration:  1149 \teval_rewards:  1950.0556593804124\n",
      "iteration:  1150 \teval_rewards:  1969.567690247488\n",
      "iteration:  1151 \teval_rewards:  1869.6358741928511\n",
      "iteration:  1152 \teval_rewards:  1897.3735354776109\n",
      "iteration:  1153 \teval_rewards:  1918.4115527570507\n",
      "iteration:  1154 \teval_rewards:  1841.7416558125099\n",
      "iteration:  1155 \teval_rewards:  1920.0063167236876\n",
      "iteration:  1156 \teval_rewards:  1849.9498494535837\n",
      "iteration:  1157 \teval_rewards:  1768.1071088211593\n",
      "iteration:  1158 \teval_rewards:  1513.9191809112779\n",
      "iteration:  1159 \teval_rewards:  1942.10987687596\n",
      "iteration:  1160 \teval_rewards:  1917.7824026966248\n",
      "iteration:  1161 \teval_rewards:  903.3931661678323\n",
      "iteration:  1162 \teval_rewards:  1855.8885314752804\n",
      "iteration:  1163 \teval_rewards:  65.6629555108057\n",
      "iteration:  1164 \teval_rewards:  1416.454269265367\n",
      "iteration:  1165 \teval_rewards:  1943.4018109455717\n",
      "iteration:  1166 \teval_rewards:  1895.7285228644894\n",
      "iteration:  1167 \teval_rewards:  2004.4376216951478\n",
      "iteration:  1168 \teval_rewards:  1810.077271178715\n",
      "iteration:  1169 \teval_rewards:  2016.4234717525549\n",
      "iteration:  1170 \teval_rewards:  1854.1353487603126\n",
      "iteration:  1171 \teval_rewards:  472.43208514747676\n",
      "iteration:  1172 \teval_rewards:  469.31663981165036\n",
      "iteration:  1173 \teval_rewards:  1879.2660628167455\n",
      "iteration:  1174 \teval_rewards:  1773.496359413306\n",
      "iteration:  1175 \teval_rewards:  1906.327140341368\n",
      "iteration:  1176 \teval_rewards:  1904.296790218324\n",
      "iteration:  1177 \teval_rewards:  1466.2681861042063\n",
      "iteration:  1178 \teval_rewards:  725.5213256859364\n",
      "iteration:  1179 \teval_rewards:  1354.9913854638494\n",
      "iteration:  1180 \teval_rewards:  2038.9363323070627\n",
      "iteration:  1181 \teval_rewards:  554.1786983083756\n",
      "iteration:  1182 \teval_rewards:  1902.3080518980908\n",
      "iteration:  1183 \teval_rewards:  1883.170836311347\n",
      "iteration:  1184 \teval_rewards:  1088.0158838404107\n",
      "iteration:  1185 \teval_rewards:  1976.9312716285174\n",
      "iteration:  1186 \teval_rewards:  1941.120301729394\n",
      "iteration:  1187 \teval_rewards:  1830.7760621995321\n",
      "iteration:  1188 \teval_rewards:  1965.2053141610988\n",
      "iteration:  1189 \teval_rewards:  1990.742576890602\n",
      "iteration:  1190 \teval_rewards:  1789.1093965131395\n",
      "iteration:  1191 \teval_rewards:  1655.5691121808452\n",
      "iteration:  1192 \teval_rewards:  1958.7043926402596\n",
      "iteration:  1193 \teval_rewards:  1982.863288927542\n",
      "iteration:  1194 \teval_rewards:  1987.0418715416388\n",
      "iteration:  1195 \teval_rewards:  862.0485545941051\n",
      "iteration:  1196 \teval_rewards:  2054.86896715041\n",
      "iteration:  1197 \teval_rewards:  2019.1298346707263\n",
      "iteration:  1198 \teval_rewards:  1938.5430420225769\n",
      "iteration:  1199 \teval_rewards:  973.1032426310707\n",
      "Iter:1200| Ep_Reward:1964.427| Running_reward:1531.658| Actor_Loss:-0.055| Critic_Loss:338.660| Iter_duration:4.193| lr:[5.999999999999998e-05]\n",
      "iteration:  1200 \teval_rewards:  1964.4269045952337\n",
      "iteration:  1201 \teval_rewards:  1797.716301038077\n",
      "iteration:  1202 \teval_rewards:  629.6013747542853\n",
      "iteration:  1203 \teval_rewards:  1886.777222643068\n",
      "iteration:  1204 \teval_rewards:  2074.989962678192\n",
      "iteration:  1205 \teval_rewards:  1845.6176708744995\n",
      "iteration:  1206 \teval_rewards:  1894.1183145550392\n",
      "iteration:  1207 \teval_rewards:  2033.8082923788124\n",
      "iteration:  1208 \teval_rewards:  1929.9100274959233\n",
      "iteration:  1209 \teval_rewards:  1905.6563829441495\n",
      "iteration:  1210 \teval_rewards:  2010.0422719433727\n",
      "iteration:  1211 \teval_rewards:  1964.726061882572\n",
      "iteration:  1212 \teval_rewards:  329.2728400892067\n",
      "iteration:  1213 \teval_rewards:  1878.9269292241138\n",
      "iteration:  1214 \teval_rewards:  928.9303017375667\n",
      "iteration:  1215 \teval_rewards:  1016.1610234680987\n",
      "iteration:  1216 \teval_rewards:  941.4317892083031\n",
      "iteration:  1217 \teval_rewards:  1992.1492699161586\n",
      "iteration:  1218 \teval_rewards:  1858.178574328895\n",
      "iteration:  1219 \teval_rewards:  1960.6126162445287\n",
      "iteration:  1220 \teval_rewards:  2123.388314501228\n",
      "iteration:  1221 \teval_rewards:  2002.5475814659794\n",
      "iteration:  1222 \teval_rewards:  1986.8386116269598\n",
      "iteration:  1223 \teval_rewards:  848.0417287548638\n",
      "iteration:  1224 \teval_rewards:  1991.0258248600876\n",
      "iteration:  1225 \teval_rewards:  1953.9528195945575\n",
      "iteration:  1226 \teval_rewards:  1094.2210810443266\n",
      "iteration:  1227 \teval_rewards:  1875.4239288885392\n",
      "iteration:  1228 \teval_rewards:  1992.3873313626632\n",
      "iteration:  1229 \teval_rewards:  1859.7968396072986\n",
      "iteration:  1230 \teval_rewards:  1886.7332484388148\n",
      "iteration:  1231 \teval_rewards:  1891.48204497274\n",
      "iteration:  1232 \teval_rewards:  160.54097776721892\n",
      "iteration:  1233 \teval_rewards:  1924.6444126818958\n",
      "iteration:  1234 \teval_rewards:  2013.3851506546816\n",
      "iteration:  1235 \teval_rewards:  1906.1126795623675\n",
      "iteration:  1236 \teval_rewards:  1985.8811723165647\n",
      "iteration:  1237 \teval_rewards:  2028.5123608096471\n",
      "iteration:  1238 \teval_rewards:  2031.4869118063398\n",
      "iteration:  1239 \teval_rewards:  2058.506455162562\n",
      "iteration:  1240 \teval_rewards:  1903.4364010692502\n",
      "iteration:  1241 \teval_rewards:  2016.8645195049744\n",
      "iteration:  1242 \teval_rewards:  2055.44854833071\n",
      "iteration:  1243 \teval_rewards:  1973.9729411961043\n",
      "iteration:  1244 \teval_rewards:  1919.4490385505135\n",
      "iteration:  1245 \teval_rewards:  1929.3961148391477\n",
      "iteration:  1246 \teval_rewards:  1933.167505424053\n",
      "iteration:  1247 \teval_rewards:  1851.7054319169943\n",
      "iteration:  1248 \teval_rewards:  1099.7866559245772\n",
      "iteration:  1249 \teval_rewards:  1973.624146723625\n",
      "iteration:  1250 \teval_rewards:  1923.9831460635237\n",
      "iteration:  1251 \teval_rewards:  1967.09482246517\n",
      "iteration:  1252 \teval_rewards:  1984.9628106746113\n",
      "iteration:  1253 \teval_rewards:  1884.6434607157125\n",
      "iteration:  1254 \teval_rewards:  1985.641395941233\n",
      "iteration:  1255 \teval_rewards:  1827.4473160826285\n",
      "iteration:  1256 \teval_rewards:  1978.4209479066683\n",
      "iteration:  1257 \teval_rewards:  648.4624374940377\n",
      "iteration:  1258 \teval_rewards:  1979.5074423850897\n",
      "iteration:  1259 \teval_rewards:  1969.880507685742\n",
      "iteration:  1260 \teval_rewards:  1800.4219262494353\n",
      "iteration:  1261 \teval_rewards:  1973.562141476182\n",
      "iteration:  1262 \teval_rewards:  1999.2563457880174\n",
      "iteration:  1263 \teval_rewards:  1906.1157726759852\n",
      "iteration:  1264 \teval_rewards:  1864.0886299871913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1265 \teval_rewards:  1968.293223645107\n",
      "iteration:  1266 \teval_rewards:  1911.76399435223\n",
      "iteration:  1267 \teval_rewards:  1887.892449605728\n",
      "iteration:  1268 \teval_rewards:  1952.1714903046704\n",
      "iteration:  1269 \teval_rewards:  685.1085880535222\n",
      "iteration:  1270 \teval_rewards:  1952.1522252892257\n",
      "iteration:  1271 \teval_rewards:  1935.3009643388464\n",
      "iteration:  1272 \teval_rewards:  383.3593309356715\n",
      "iteration:  1273 \teval_rewards:  1958.1182850722716\n",
      "iteration:  1274 \teval_rewards:  1994.2606885853227\n",
      "iteration:  1275 \teval_rewards:  2046.498402807399\n",
      "iteration:  1276 \teval_rewards:  2045.2702729948073\n",
      "iteration:  1277 \teval_rewards:  1774.8710950938225\n",
      "iteration:  1278 \teval_rewards:  276.52093238485475\n",
      "iteration:  1279 \teval_rewards:  2013.7026376690828\n",
      "iteration:  1280 \teval_rewards:  305.77601170274204\n",
      "iteration:  1281 \teval_rewards:  1915.8899400818323\n",
      "iteration:  1282 \teval_rewards:  2013.8206320514696\n",
      "iteration:  1283 \teval_rewards:  1985.3225390396829\n",
      "iteration:  1284 \teval_rewards:  1924.971347662329\n",
      "iteration:  1285 \teval_rewards:  1928.092380987301\n",
      "iteration:  1286 \teval_rewards:  2012.4358308905473\n",
      "iteration:  1287 \teval_rewards:  688.6783789275935\n",
      "iteration:  1288 \teval_rewards:  1921.9968835883165\n",
      "iteration:  1289 \teval_rewards:  850.8641686938842\n",
      "iteration:  1290 \teval_rewards:  537.8792980420832\n",
      "iteration:  1291 \teval_rewards:  1922.8591597142204\n",
      "iteration:  1292 \teval_rewards:  547.3942289094949\n",
      "iteration:  1293 \teval_rewards:  2089.536601846545\n",
      "iteration:  1294 \teval_rewards:  1923.5877961970614\n",
      "iteration:  1295 \teval_rewards:  356.2727585592848\n",
      "iteration:  1296 \teval_rewards:  1829.8219899224482\n",
      "iteration:  1297 \teval_rewards:  2001.2065782871446\n",
      "iteration:  1298 \teval_rewards:  1998.3784805258347\n",
      "iteration:  1299 \teval_rewards:  1964.2641208804498\n",
      "Iter:1300| Ep_Reward:2009.448| Running_reward:1632.458| Actor_Loss:0.034| Critic_Loss:661.805| Iter_duration:4.219| lr:[3.999999999999999e-05]\n",
      "iteration:  1300 \teval_rewards:  2009.4480409816301\n",
      "iteration:  1301 \teval_rewards:  1601.9867148337237\n",
      "iteration:  1302 \teval_rewards:  1987.2312156478479\n",
      "iteration:  1303 \teval_rewards:  1949.0538117403285\n",
      "iteration:  1304 \teval_rewards:  1842.6037025083597\n",
      "iteration:  1305 \teval_rewards:  1964.813611396507\n",
      "iteration:  1306 \teval_rewards:  1961.7828810084154\n",
      "iteration:  1307 \teval_rewards:  601.3809909528792\n",
      "iteration:  1308 \teval_rewards:  2007.2871865355635\n",
      "iteration:  1309 \teval_rewards:  1834.5585450894737\n",
      "iteration:  1310 \teval_rewards:  1992.064692883324\n",
      "iteration:  1311 \teval_rewards:  418.6907831801317\n",
      "iteration:  1312 \teval_rewards:  1973.4144339492366\n",
      "iteration:  1313 \teval_rewards:  1957.6544147937698\n",
      "iteration:  1314 \teval_rewards:  1508.595823349193\n",
      "iteration:  1315 \teval_rewards:  536.7303813959893\n",
      "iteration:  1316 \teval_rewards:  1877.623013222768\n",
      "iteration:  1317 \teval_rewards:  1969.364869695514\n",
      "iteration:  1318 \teval_rewards:  1979.9476404152747\n",
      "iteration:  1319 \teval_rewards:  1984.9016377761227\n",
      "iteration:  1320 \teval_rewards:  2015.392120507559\n",
      "iteration:  1321 \teval_rewards:  593.0303847899025\n",
      "iteration:  1322 \teval_rewards:  1962.58312197712\n",
      "iteration:  1323 \teval_rewards:  2047.4210017672146\n",
      "iteration:  1324 \teval_rewards:  1977.994978919636\n",
      "iteration:  1325 \teval_rewards:  2013.5473483759288\n",
      "iteration:  1326 \teval_rewards:  1990.1755130019828\n",
      "iteration:  1327 \teval_rewards:  1884.0952487276081\n",
      "iteration:  1328 \teval_rewards:  1902.308191644552\n",
      "iteration:  1329 \teval_rewards:  1947.348417728841\n",
      "iteration:  1330 \teval_rewards:  1963.046175225778\n",
      "iteration:  1331 \teval_rewards:  2056.7054111843627\n",
      "iteration:  1332 \teval_rewards:  2031.533877275774\n",
      "iteration:  1333 \teval_rewards:  1937.7679648789851\n",
      "iteration:  1334 \teval_rewards:  180.30017922427942\n",
      "iteration:  1335 \teval_rewards:  1970.473859084992\n",
      "iteration:  1336 \teval_rewards:  1847.2922382698034\n",
      "iteration:  1337 \teval_rewards:  944.5972788738447\n",
      "iteration:  1338 \teval_rewards:  1984.5638025756284\n",
      "iteration:  1339 \teval_rewards:  2160.935208254592\n",
      "iteration:  1340 \teval_rewards:  1946.8681348490059\n",
      "iteration:  1341 \teval_rewards:  1947.7249775354403\n",
      "iteration:  1342 \teval_rewards:  2032.2921025210649\n",
      "iteration:  1343 \teval_rewards:  2052.193597297929\n",
      "iteration:  1344 \teval_rewards:  2049.117051030282\n",
      "iteration:  1345 \teval_rewards:  1952.8696546978272\n",
      "iteration:  1346 \teval_rewards:  2042.7053537304866\n",
      "iteration:  1347 \teval_rewards:  58.043417168999056\n",
      "iteration:  1348 \teval_rewards:  2014.3257851572776\n",
      "iteration:  1349 \teval_rewards:  1317.3927457687655\n",
      "iteration:  1350 \teval_rewards:  1973.6271616789545\n",
      "iteration:  1351 \teval_rewards:  468.61217729496275\n",
      "iteration:  1352 \teval_rewards:  2079.492691803741\n",
      "iteration:  1353 \teval_rewards:  2004.1713729314286\n",
      "iteration:  1354 \teval_rewards:  2106.180557644034\n",
      "iteration:  1355 \teval_rewards:  1127.703640382166\n",
      "iteration:  1356 \teval_rewards:  2058.595260569568\n",
      "iteration:  1357 \teval_rewards:  2046.4108296868264\n",
      "iteration:  1358 \teval_rewards:  911.3281018308302\n",
      "iteration:  1359 \teval_rewards:  1481.917524179125\n",
      "iteration:  1360 \teval_rewards:  1993.853597679855\n",
      "iteration:  1361 \teval_rewards:  1990.8004693078476\n",
      "iteration:  1362 \teval_rewards:  1294.9218255339406\n",
      "iteration:  1363 \teval_rewards:  2148.1894906252173\n",
      "iteration:  1364 \teval_rewards:  986.976867501025\n",
      "iteration:  1365 \teval_rewards:  2008.7147183916468\n",
      "iteration:  1366 \teval_rewards:  1923.0610864402124\n",
      "iteration:  1367 \teval_rewards:  2078.978506786517\n",
      "iteration:  1368 \teval_rewards:  1942.2049377271476\n",
      "iteration:  1369 \teval_rewards:  843.1994594190132\n",
      "iteration:  1370 \teval_rewards:  2022.2679894501061\n",
      "iteration:  1371 \teval_rewards:  1952.6048279486633\n",
      "iteration:  1372 \teval_rewards:  2009.1852325078048\n",
      "iteration:  1373 \teval_rewards:  1897.7701058904604\n",
      "iteration:  1374 \teval_rewards:  1968.5142768157646\n",
      "iteration:  1375 \teval_rewards:  1969.7773725956727\n",
      "iteration:  1376 \teval_rewards:  1075.3899194692729\n",
      "iteration:  1377 \teval_rewards:  1986.4392254537852\n",
      "iteration:  1378 \teval_rewards:  2057.6516598311337\n",
      "iteration:  1379 \teval_rewards:  2094.0117019395902\n",
      "iteration:  1380 \teval_rewards:  2047.4830897513832\n",
      "iteration:  1381 \teval_rewards:  2134.8774201146275\n",
      "iteration:  1382 \teval_rewards:  2031.3322488876263\n",
      "iteration:  1383 \teval_rewards:  2043.9360700437278\n",
      "iteration:  1384 \teval_rewards:  2113.314126482268\n",
      "iteration:  1385 \teval_rewards:  1980.5835172823527\n",
      "iteration:  1386 \teval_rewards:  2012.091526316535\n",
      "iteration:  1387 \teval_rewards:  307.1968476160333\n",
      "iteration:  1388 \teval_rewards:  1920.2730224611162\n",
      "iteration:  1389 \teval_rewards:  1849.9330366865502\n",
      "iteration:  1390 \teval_rewards:  1104.4976965130134\n",
      "iteration:  1391 \teval_rewards:  1894.1900653284074\n",
      "iteration:  1392 \teval_rewards:  1901.0044138020355\n",
      "iteration:  1393 \teval_rewards:  1934.0174074132933\n",
      "iteration:  1394 \teval_rewards:  2018.7845594527466\n",
      "iteration:  1395 \teval_rewards:  1989.6209047645866\n",
      "iteration:  1396 \teval_rewards:  2067.6127385643053\n",
      "iteration:  1397 \teval_rewards:  2074.9682408423428\n",
      "iteration:  1398 \teval_rewards:  1976.9555750138431\n",
      "iteration:  1399 \teval_rewards:  1507.5230007792693\n",
      "Iter:1400| Ep_Reward:2029.888| Running_reward:1723.914| Actor_Loss:-0.177| Critic_Loss:51.284| Iter_duration:4.306| lr:[1.9999999999999995e-05]\n",
      "iteration:  1400 \teval_rewards:  2029.887720934137\n",
      "iteration:  1401 \teval_rewards:  2085.9003955488306\n",
      "iteration:  1402 \teval_rewards:  2033.5696821057513\n",
      "iteration:  1403 \teval_rewards:  1881.7387799361434\n",
      "iteration:  1404 \teval_rewards:  2134.5419957179033\n",
      "iteration:  1405 \teval_rewards:  2096.0924078910803\n",
      "iteration:  1406 \teval_rewards:  2139.2731641634045\n",
      "iteration:  1407 \teval_rewards:  2136.8749098257526\n",
      "iteration:  1408 \teval_rewards:  1991.7621340327505\n",
      "iteration:  1409 \teval_rewards:  1492.1562521688688\n",
      "iteration:  1410 \teval_rewards:  1542.9954259561737\n",
      "iteration:  1411 \teval_rewards:  2013.7600004473215\n",
      "iteration:  1412 \teval_rewards:  202.86866261467407\n",
      "iteration:  1413 \teval_rewards:  1945.2871078915634\n",
      "iteration:  1414 \teval_rewards:  2218.593584115384\n",
      "iteration:  1415 \teval_rewards:  1748.3455575687142\n",
      "iteration:  1416 \teval_rewards:  618.4921528609743\n",
      "iteration:  1417 \teval_rewards:  2099.7005312012593\n",
      "iteration:  1418 \teval_rewards:  925.2476376643217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1419 \teval_rewards:  2121.7868741054076\n",
      "iteration:  1420 \teval_rewards:  1063.2637431351814\n",
      "iteration:  1421 \teval_rewards:  1218.7394872005293\n",
      "iteration:  1422 \teval_rewards:  2122.4932810086875\n",
      "iteration:  1423 \teval_rewards:  1619.1418706894235\n",
      "iteration:  1424 \teval_rewards:  2116.911593238912\n",
      "iteration:  1425 \teval_rewards:  194.3591131673447\n",
      "iteration:  1426 \teval_rewards:  838.0639398769742\n",
      "iteration:  1427 \teval_rewards:  2026.9629206257237\n",
      "iteration:  1428 \teval_rewards:  2103.699507975877\n",
      "iteration:  1429 \teval_rewards:  2063.4790213600336\n",
      "iteration:  1430 \teval_rewards:  1586.1735798782847\n",
      "iteration:  1431 \teval_rewards:  1603.202805034602\n",
      "iteration:  1432 \teval_rewards:  2066.6874331337685\n",
      "iteration:  1433 \teval_rewards:  2156.943929199391\n",
      "iteration:  1434 \teval_rewards:  2076.5957534022114\n",
      "iteration:  1435 \teval_rewards:  2065.8130271411983\n",
      "iteration:  1436 \teval_rewards:  2169.746008107667\n",
      "iteration:  1437 \teval_rewards:  2133.871353414147\n",
      "iteration:  1438 \teval_rewards:  2155.595291671156\n",
      "iteration:  1439 \teval_rewards:  763.8049540815418\n",
      "iteration:  1440 \teval_rewards:  2086.0594898311424\n",
      "iteration:  1441 \teval_rewards:  2015.454457546849\n",
      "iteration:  1442 \teval_rewards:  2214.5105676122876\n",
      "iteration:  1443 \teval_rewards:  2130.6172238477934\n",
      "iteration:  1444 \teval_rewards:  2159.10210458353\n",
      "iteration:  1445 \teval_rewards:  2183.823763490667\n",
      "iteration:  1446 \teval_rewards:  989.0756584708481\n",
      "iteration:  1447 \teval_rewards:  1695.736163843169\n",
      "iteration:  1448 \teval_rewards:  2154.1609660495933\n",
      "iteration:  1449 \teval_rewards:  2016.80616023935\n",
      "iteration:  1450 \teval_rewards:  2178.2039157602076\n",
      "iteration:  1451 \teval_rewards:  2109.493594788598\n",
      "iteration:  1452 \teval_rewards:  2100.004797663677\n",
      "iteration:  1453 \teval_rewards:  2097.4886347755037\n",
      "iteration:  1454 \teval_rewards:  2169.1698517572086\n",
      "iteration:  1455 \teval_rewards:  2116.8976065116312\n",
      "iteration:  1456 \teval_rewards:  729.2840823610725\n",
      "iteration:  1457 \teval_rewards:  1795.142807405771\n",
      "iteration:  1458 \teval_rewards:  2130.859883188489\n",
      "iteration:  1459 \teval_rewards:  829.2776454387983\n",
      "iteration:  1460 \teval_rewards:  413.2098753429143\n",
      "iteration:  1461 \teval_rewards:  503.3288300079958\n",
      "iteration:  1462 \teval_rewards:  615.1843113558035\n",
      "iteration:  1463 \teval_rewards:  774.6052775121935\n",
      "iteration:  1464 \teval_rewards:  495.168920635683\n",
      "iteration:  1465 \teval_rewards:  1843.707771423975\n",
      "iteration:  1466 \teval_rewards:  1104.5982782344922\n",
      "iteration:  1467 \teval_rewards:  2124.353462879995\n",
      "iteration:  1468 \teval_rewards:  2129.6428183220946\n",
      "iteration:  1469 \teval_rewards:  1472.7445013950137\n",
      "iteration:  1470 \teval_rewards:  1921.6095085171296\n",
      "iteration:  1471 \teval_rewards:  2054.5745545442073\n",
      "iteration:  1472 \teval_rewards:  2158.469096083365\n",
      "iteration:  1473 \teval_rewards:  50.54230130108329\n",
      "iteration:  1474 \teval_rewards:  2099.252438766982\n",
      "iteration:  1475 \teval_rewards:  2088.8928164558106\n",
      "iteration:  1476 \teval_rewards:  1206.7173095322537\n",
      "iteration:  1477 \teval_rewards:  874.4355861619998\n",
      "iteration:  1478 \teval_rewards:  2100.789627204059\n",
      "iteration:  1479 \teval_rewards:  1448.6284347440358\n",
      "iteration:  1480 \teval_rewards:  2189.6505004729515\n",
      "iteration:  1481 \teval_rewards:  2101.5949180602365\n",
      "iteration:  1482 \teval_rewards:  1210.5688573472794\n",
      "iteration:  1483 \teval_rewards:  2070.5412397986674\n",
      "iteration:  1484 \teval_rewards:  2268.028319209897\n",
      "iteration:  1485 \teval_rewards:  873.4557992887258\n",
      "iteration:  1486 \teval_rewards:  2029.0625632673657\n",
      "iteration:  1487 \teval_rewards:  570.0207883344312\n",
      "iteration:  1488 \teval_rewards:  2053.5382103643647\n",
      "iteration:  1489 \teval_rewards:  2085.968812577411\n",
      "iteration:  1490 \teval_rewards:  2100.165184214129\n",
      "iteration:  1491 \teval_rewards:  2198.414376644607\n",
      "iteration:  1492 \teval_rewards:  2044.2187517529999\n",
      "iteration:  1493 \teval_rewards:  2186.9208416223228\n",
      "iteration:  1494 \teval_rewards:  2143.0989626059636\n",
      "iteration:  1495 \teval_rewards:  2125.882615922274\n",
      "iteration:  1496 \teval_rewards:  2265.913974822586\n",
      "iteration:  1497 \teval_rewards:  2147.7596519417093\n",
      "iteration:  1498 \teval_rewards:  2124.0843163880477\n",
      "iteration:  1499 \teval_rewards:  1213.671543827705\n",
      "Iter:1500| Ep_Reward:2102.864| Running_reward:1731.887| Actor_Loss:0.329| Critic_Loss:891.369| Iter_duration:4.262| lr:[0.0]\n",
      "iteration:  1500 \teval_rewards:  2102.8635861536477\n",
      "episode reward:2153.372\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME + \"-v2\")\n",
    "\n",
    "agent = Agent(n_states=n_states,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FetchPickAndPlace\n",
    "\n",
    "The FetchPickAndPlace has some state output which different from the other environment.\n",
    "It contains dictionary of environment in the form of:\n",
    "- observation: the joint degree position and speed\n",
    "- desired_goal: the target position that robot need to go\n",
    "- achieved_goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "3\n",
      "[ 1.34193265e+00  7.49100375e-01  5.34722720e-01  1.19599421e+00\n",
      "  8.00557661e-01  4.24702091e-01 -1.45938432e-01  5.14572867e-02\n",
      " -1.10020629e-01  2.91834773e-06 -4.72661656e-08 -3.85214084e-07\n",
      "  5.92637053e-07  1.12208536e-13 -7.74656889e-06 -7.65027248e-08\n",
      "  4.92570535e-05  1.88857148e-07 -2.90549459e-07 -1.18156686e-18\n",
      "  7.73934983e-06  7.18103404e-08 -2.42928780e-06  4.93607091e-07\n",
      "  1.70999820e-07]\n",
      "[1.19599421 0.80055766 0.42470209]\n",
      "[1.45979337 0.7254639  0.60492436]\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"FetchPickAndPlace\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v1\")\n",
    "\n",
    "n_states = test_env.observation_space[\"observation\"].shape[0]\n",
    "n_achieveds = test_env.observation_space[\"achieved_goal\"].shape[0]\n",
    "n_goals = test_env.observation_space[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]\n",
    "\n",
    "print(n_states)\n",
    "print(n_goals)\n",
    "\n",
    "env_dict = test_env.reset()\n",
    "\n",
    "state = env_dict[\"observation\"]\n",
    "achieved_goal = env_dict[\"achieved_goal\"]\n",
    "desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "print(state)\n",
    "print(achieved_goal)\n",
    "print(desired_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify the environment dict to be the 1d array of state which contatain sate observation and desired goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3419e+00,  7.4910e-01,  5.3472e-01,  1.1960e+00,  8.0056e-01,\n",
      "         4.2470e-01, -1.4594e-01,  5.1457e-02, -1.1002e-01,  2.9183e-06,\n",
      "        -4.7266e-08, -3.8521e-07,  5.9264e-07,  1.1221e-13, -7.7466e-06,\n",
      "        -7.6503e-08,  4.9257e-05,  1.8886e-07, -2.9055e-07, -1.1816e-18,\n",
      "         7.7393e-06,  7.1810e-08, -2.4293e-06,  4.9361e-07,  1.7100e-07,\n",
      "         1.4598e+00,  7.2546e-01,  6.0492e-01])\n"
     ]
    }
   ],
   "source": [
    "state = env_dict[\"observation\"]\n",
    "achieved_goal = env_dict[\"achieved_goal\"]\n",
    "desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "#state = np.expand_dims(state, axis=0)\n",
    "#goal = np.expand_dims(desired_goal, axis=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = np.append(state, desired_goal)\n",
    "    x = from_numpy(x).float()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_state(env_dict):\n",
    "    state = env_dict[\"observation\"]\n",
    "    achieved_goal = env_dict[\"achieved_goal\"]\n",
    "    desired_goal = env_dict[\"desired_goal\"]\n",
    "\n",
    "    return np.append(state, desired_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(agent, env, state_rms, action_bounds):\n",
    "    total_rewards = 0\n",
    "    s = env.reset()\n",
    "    s = set_state(s)\n",
    "    done = False\n",
    "    while not done:\n",
    "        s = np.clip((s - state_rms.mean) / (state_rms.var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "        dist = agent.choose_dist(s)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        # action = np.clip(action, action_bounds[0], action_bounds[1])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = set_state(next_state)\n",
    "        # env.render()\n",
    "        s = next_state\n",
    "        total_rewards += reward\n",
    "    # env.close()\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, env, test_env, env_name, n_iterations, agent, epochs, mini_batch_size, epsilon, horizon):\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.test_env = test_env\n",
    "        self.agent = agent\n",
    "        self.epsilon = epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.state_rms = RunningMeanStd(shape=(self.agent.n_states,))\n",
    "\n",
    "        self.running_reward = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_mini_batch(mini_batch_size, states, actions, returns, advs, values, log_probs):\n",
    "        full_batch_size = len(states)\n",
    "        for _ in range(full_batch_size // mini_batch_size):\n",
    "            indices = np.random.randint(0, full_batch_size, mini_batch_size)\n",
    "            yield states[indices], actions[indices], returns[indices], advs[indices], values[indices],\\\n",
    "                  log_probs[indices]\n",
    "\n",
    "    def train(self, states, actions, advs, values, log_probs):\n",
    "\n",
    "        values = np.vstack(values[:-1])\n",
    "        log_probs = np.vstack(log_probs)\n",
    "        returns = advs + values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        actions = np.vstack(actions)\n",
    "        for epoch in range(self.epochs):\n",
    "            for state, action, return_, adv, old_value, old_log_prob in self.choose_mini_batch(self.mini_batch_size,\n",
    "                                                                                               states, actions, returns,\n",
    "                                                                                               advs, values, log_probs):\n",
    "                state = torch.Tensor(state).to(self.agent.device)\n",
    "                action = torch.Tensor(action).to(self.agent.device)\n",
    "                return_ = torch.Tensor(return_).to(self.agent.device)\n",
    "                adv = torch.Tensor(adv).to(self.agent.device)\n",
    "                old_value = torch.Tensor(old_value).to(self.agent.device)\n",
    "                old_log_prob = torch.Tensor(old_log_prob).to(self.agent.device)\n",
    "\n",
    "                value = self.agent.critic(state)\n",
    "                # clipped_value = old_value + torch.clamp(value - old_value, -self.epsilon, self.epsilon)\n",
    "                # clipped_v_loss = (clipped_value - return_).pow(2)\n",
    "                # unclipped_v_loss = (value - return_).pow(2)\n",
    "                # critic_loss = 0.5 * torch.max(clipped_v_loss, unclipped_v_loss).mean()\n",
    "                critic_loss = self.agent.critic_loss(value, return_)\n",
    "\n",
    "                new_log_prob = self.calculate_log_probs(self.agent.current_policy, state, action)\n",
    "\n",
    "                ratio = (new_log_prob - old_log_prob).exp()\n",
    "                actor_loss = self.compute_actor_loss(ratio, adv)\n",
    "\n",
    "                self.agent.optimize(actor_loss, critic_loss)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def step(self):\n",
    "        state = self.env.reset()\n",
    "        state = set_state(state)\n",
    "        for iteration in range(1, 1 + self.n_iterations):\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            dones = []\n",
    "\n",
    "            self.start_time = time.time()\n",
    "            for t in range(self.horizon):\n",
    "                # self.state_rms.update(state)\n",
    "                state = np.clip((state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "                dist = self.agent.choose_dist(state)\n",
    "                action = dist.sample()\n",
    "                # action = np.clip(action, self.agent.action_bounds[0], self.agent.action_bounds[1])\n",
    "                log_prob = dist.log_prob(action).cpu()\n",
    "                action = action.cpu().numpy()[0]\n",
    "                value = self.agent.get_value(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = set_state(next_state)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                dones.append(done)\n",
    "\n",
    "                if done:\n",
    "                    state = self.env.reset()\n",
    "                    state = set_state(state)\n",
    "                else:\n",
    "                    state = next_state\n",
    "            # self.state_rms.update(next_state)\n",
    "            next_state = np.clip((next_state - self.state_rms.mean) / (self.state_rms.var ** 0.5 + 1e-8), -5, 5)\n",
    "            next_value = self.agent.get_value(next_state) * (1 - done)\n",
    "            values.append(next_value)\n",
    "\n",
    "            advs = self.get_gae(rewards, values, dones)\n",
    "            states = np.vstack(states)\n",
    "            actor_loss, critic_loss = self.train(states, actions, advs, values, log_probs)\n",
    "            # self.agent.set_weights()\n",
    "            self.agent.schedule_lr()\n",
    "            eval_rewards = evaluate_model(self.agent, self.test_env, self.state_rms, self.agent.action_bounds)\n",
    "            self.state_rms.update(states)\n",
    "            self.print_logs(iteration, actor_loss, critic_loss, eval_rewards)\n",
    "            print(\"iteration: \", iteration, \"\\teval_rewards: \", eval_rewards)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "\n",
    "        advs = []\n",
    "        gae = 0\n",
    "\n",
    "        dones.append(0)\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + gamma * (values[step + 1]) * (1 - dones[step]) - values[step]\n",
    "            gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "            advs.append(gae)\n",
    "\n",
    "        advs.reverse()\n",
    "        return np.vstack(advs)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_log_probs(model, states, actions):\n",
    "        policy_distribution = model(states)\n",
    "        return policy_distribution.log_prob(actions)\n",
    "\n",
    "    def compute_actor_loss(self, ratio, adv):\n",
    "        pg_loss1 = adv * ratio\n",
    "        pg_loss2 = adv * torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "        loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "        return loss\n",
    "\n",
    "    def print_logs(self, iteration, actor_loss, critic_loss, eval_rewards):\n",
    "        if iteration == 1:\n",
    "            self.running_reward = eval_rewards\n",
    "        else:\n",
    "            self.running_reward = self.running_reward * 0.99 + eval_rewards * 0.01\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iter:{iteration}| \"\n",
    "                  f\"Ep_Reward:{eval_rewards:.3f}| \"\n",
    "                  f\"Running_reward:{self.running_reward:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Iter_duration:{time.time() - self.start_time:.3f}| \"\n",
    "                  f\"lr:{self.agent.actor_scheduler.get_last_lr()}\")\n",
    "            self.agent.save_weights(iteration, self.state_rms)\n",
    "\n",
    "        with SummaryWriter(self.env_name + \"/logs\") as writer:\n",
    "            writer.add_scalar(\"Episode running reward\", self.running_reward, iteration)\n",
    "            writer.add_scalar(\"Episode reward\", eval_rewards, iteration)\n",
    "            writer.add_scalar(\"Actor loss\", actor_loss, iteration)\n",
    "            writer.add_scalar(\"Critic loss\", critic_loss, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n"
     ]
    }
   ],
   "source": [
    "GlfwContext(offscreen=True)\n",
    "\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, env_name, max_episode=1):\n",
    "        self.env = env\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        _, self.state_rms_mean, self.state_rms_var = self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = torch.device(process_device)\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.VideoWriter = cv2.VideoWriter(env_name + \".avi\", self.fourcc, 50.0, (250, 250))\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        for _ in range(self.max_episode):\n",
    "            s = self.env.reset()\n",
    "            s = set_state(s)\n",
    "            episode_reward = 0\n",
    "            for _ in range(self.env._max_episode_steps):\n",
    "                s = np.clip((s - self.state_rms_mean) / (self.state_rms_var ** 0.5 + 1e-8), -5.0, 5.0)\n",
    "                dist = self.agent.choose_dist(s)\n",
    "                action = dist.sample().cpu().numpy()[0]\n",
    "                s_, r, done, _ = self.env.step(action)\n",
    "                s_ = set_state(s_)\n",
    "                episode_reward += r\n",
    "                if done:\n",
    "                    break\n",
    "                s = s_\n",
    "                # self.env.render(mode=\"human\")\n",
    "                # self.env.viewer.cam.type = const.CAMERA_FIXED\n",
    "                # self.env.viewer.cam.fixedcamid = 0\n",
    "                # time.sleep(0.03)\n",
    "                I = self.env.render(mode='rgb_array')\n",
    "                I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "                I = cv2.resize(I, (250, 250))\n",
    "                self.VideoWriter.write(I)\n",
    "                # cv2.imshow(\"env\", I)\n",
    "                # cv2.waitKey(10)\n",
    "            print(f\"episode reward:{episode_reward:3.3f}\")\n",
    "        self.env.close()\n",
    "        self.VideoWriter.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FetchPickAndPlace\"\n",
    "TRAIN_FLAG = True\n",
    "test_env = gym.make(ENV_NAME + \"-v1\")\n",
    "\n",
    "n_states = test_env.observation_space[\"observation\"].shape[0]\n",
    "n_achieveds = test_env.observation_space[\"achieved_goal\"].shape[0]\n",
    "n_goals = test_env.observation_space[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "n_actions = test_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to use the environment without changing loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states:25\n",
      "action bounds:[-1.0, 1.0]\n",
      "number of actions:4\n",
      "iteration:  1 \teval_rewards:  -50.0\n",
      "iteration:  2 \teval_rewards:  -50.0\n",
      "iteration:  3 \teval_rewards:  -50.0\n",
      "iteration:  4 \teval_rewards:  -50.0\n",
      "iteration:  5 \teval_rewards:  0.0\n",
      "iteration:  6 \teval_rewards:  -50.0\n",
      "iteration:  7 \teval_rewards:  -50.0\n",
      "iteration:  8 \teval_rewards:  -50.0\n",
      "iteration:  9 \teval_rewards:  -50.0\n",
      "iteration:  10 \teval_rewards:  -50.0\n",
      "iteration:  11 \teval_rewards:  -50.0\n",
      "iteration:  12 \teval_rewards:  -50.0\n",
      "iteration:  13 \teval_rewards:  -50.0\n",
      "iteration:  14 \teval_rewards:  -50.0\n",
      "iteration:  15 \teval_rewards:  -50.0\n",
      "iteration:  16 \teval_rewards:  -50.0\n",
      "iteration:  17 \teval_rewards:  -50.0\n",
      "iteration:  18 \teval_rewards:  0.0\n",
      "iteration:  19 \teval_rewards:  -50.0\n",
      "iteration:  20 \teval_rewards:  -50.0\n",
      "iteration:  21 \teval_rewards:  -50.0\n",
      "iteration:  22 \teval_rewards:  -50.0\n",
      "iteration:  23 \teval_rewards:  -50.0\n",
      "iteration:  24 \teval_rewards:  -50.0\n",
      "iteration:  25 \teval_rewards:  -50.0\n",
      "iteration:  26 \teval_rewards:  -50.0\n",
      "iteration:  27 \teval_rewards:  -50.0\n",
      "iteration:  28 \teval_rewards:  -50.0\n",
      "iteration:  29 \teval_rewards:  -50.0\n",
      "iteration:  30 \teval_rewards:  -50.0\n",
      "iteration:  31 \teval_rewards:  -50.0\n",
      "iteration:  32 \teval_rewards:  -50.0\n",
      "iteration:  33 \teval_rewards:  -50.0\n",
      "iteration:  34 \teval_rewards:  -50.0\n",
      "iteration:  35 \teval_rewards:  -50.0\n",
      "iteration:  36 \teval_rewards:  -50.0\n",
      "iteration:  37 \teval_rewards:  -50.0\n",
      "iteration:  38 \teval_rewards:  -50.0\n",
      "iteration:  39 \teval_rewards:  -50.0\n",
      "iteration:  40 \teval_rewards:  -50.0\n",
      "iteration:  41 \teval_rewards:  -50.0\n",
      "iteration:  42 \teval_rewards:  -50.0\n",
      "iteration:  43 \teval_rewards:  -50.0\n",
      "iteration:  44 \teval_rewards:  -50.0\n",
      "iteration:  45 \teval_rewards:  -50.0\n",
      "iteration:  46 \teval_rewards:  -50.0\n",
      "iteration:  47 \teval_rewards:  0.0\n",
      "iteration:  48 \teval_rewards:  -50.0\n",
      "iteration:  49 \teval_rewards:  -50.0\n",
      "iteration:  50 \teval_rewards:  -50.0\n",
      "iteration:  51 \teval_rewards:  -50.0\n",
      "iteration:  52 \teval_rewards:  -50.0\n",
      "iteration:  53 \teval_rewards:  -50.0\n",
      "iteration:  54 \teval_rewards:  -50.0\n",
      "iteration:  55 \teval_rewards:  -50.0\n",
      "iteration:  56 \teval_rewards:  -50.0\n",
      "iteration:  57 \teval_rewards:  -50.0\n",
      "iteration:  58 \teval_rewards:  -50.0\n",
      "iteration:  59 \teval_rewards:  -50.0\n",
      "iteration:  60 \teval_rewards:  -50.0\n",
      "iteration:  61 \teval_rewards:  -50.0\n",
      "iteration:  62 \teval_rewards:  -50.0\n",
      "iteration:  63 \teval_rewards:  -50.0\n",
      "iteration:  64 \teval_rewards:  -50.0\n",
      "iteration:  65 \teval_rewards:  -50.0\n",
      "iteration:  66 \teval_rewards:  -50.0\n",
      "iteration:  67 \teval_rewards:  -50.0\n",
      "iteration:  68 \teval_rewards:  -50.0\n",
      "iteration:  69 \teval_rewards:  -50.0\n",
      "iteration:  70 \teval_rewards:  -50.0\n",
      "iteration:  71 \teval_rewards:  -50.0\n",
      "iteration:  72 \teval_rewards:  -50.0\n",
      "iteration:  73 \teval_rewards:  0.0\n",
      "iteration:  74 \teval_rewards:  -50.0\n",
      "iteration:  75 \teval_rewards:  -50.0\n",
      "iteration:  76 \teval_rewards:  -50.0\n",
      "iteration:  77 \teval_rewards:  -50.0\n",
      "iteration:  78 \teval_rewards:  -50.0\n",
      "iteration:  79 \teval_rewards:  -50.0\n",
      "iteration:  80 \teval_rewards:  -50.0\n",
      "iteration:  81 \teval_rewards:  -50.0\n",
      "iteration:  82 \teval_rewards:  -50.0\n",
      "iteration:  83 \teval_rewards:  -50.0\n",
      "iteration:  84 \teval_rewards:  -50.0\n",
      "iteration:  85 \teval_rewards:  0.0\n",
      "iteration:  86 \teval_rewards:  -50.0\n",
      "iteration:  87 \teval_rewards:  -50.0\n",
      "iteration:  88 \teval_rewards:  -50.0\n",
      "iteration:  89 \teval_rewards:  -50.0\n",
      "iteration:  90 \teval_rewards:  -50.0\n",
      "iteration:  91 \teval_rewards:  -50.0\n",
      "iteration:  92 \teval_rewards:  -50.0\n",
      "iteration:  93 \teval_rewards:  -50.0\n",
      "iteration:  94 \teval_rewards:  -50.0\n",
      "iteration:  95 \teval_rewards:  -50.0\n",
      "iteration:  96 \teval_rewards:  0.0\n",
      "iteration:  97 \teval_rewards:  -50.0\n",
      "iteration:  98 \teval_rewards:  -50.0\n",
      "iteration:  99 \teval_rewards:  -50.0\n",
      "Iter:100| Ep_Reward:-50.000| Running_reward:-48.003| Actor_Loss:-0.211| Critic_Loss:10.345| Iter_duration:5.433| lr:[0.00028]\n",
      "iteration:  100 \teval_rewards:  -50.0\n",
      "iteration:  101 \teval_rewards:  -50.0\n",
      "iteration:  102 \teval_rewards:  -50.0\n",
      "iteration:  103 \teval_rewards:  0.0\n",
      "iteration:  104 \teval_rewards:  -50.0\n",
      "iteration:  105 \teval_rewards:  -50.0\n",
      "iteration:  106 \teval_rewards:  -50.0\n",
      "iteration:  107 \teval_rewards:  -50.0\n",
      "iteration:  108 \teval_rewards:  -50.0\n",
      "iteration:  109 \teval_rewards:  -50.0\n",
      "iteration:  110 \teval_rewards:  -50.0\n",
      "iteration:  111 \teval_rewards:  -50.0\n",
      "iteration:  112 \teval_rewards:  -50.0\n",
      "iteration:  113 \teval_rewards:  -50.0\n",
      "iteration:  114 \teval_rewards:  -50.0\n",
      "iteration:  115 \teval_rewards:  -50.0\n",
      "iteration:  116 \teval_rewards:  -50.0\n",
      "iteration:  117 \teval_rewards:  -50.0\n",
      "iteration:  118 \teval_rewards:  -50.0\n",
      "iteration:  119 \teval_rewards:  -50.0\n",
      "iteration:  120 \teval_rewards:  -50.0\n",
      "iteration:  121 \teval_rewards:  -50.0\n",
      "iteration:  122 \teval_rewards:  -50.0\n",
      "iteration:  123 \teval_rewards:  -50.0\n",
      "iteration:  124 \teval_rewards:  -50.0\n",
      "iteration:  125 \teval_rewards:  -50.0\n",
      "iteration:  126 \teval_rewards:  -50.0\n",
      "iteration:  127 \teval_rewards:  -50.0\n",
      "iteration:  128 \teval_rewards:  -50.0\n",
      "iteration:  129 \teval_rewards:  -50.0\n",
      "iteration:  130 \teval_rewards:  -50.0\n",
      "iteration:  131 \teval_rewards:  -50.0\n",
      "iteration:  132 \teval_rewards:  -50.0\n",
      "iteration:  133 \teval_rewards:  -50.0\n",
      "iteration:  134 \teval_rewards:  -50.0\n",
      "iteration:  135 \teval_rewards:  -50.0\n",
      "iteration:  136 \teval_rewards:  0.0\n",
      "iteration:  137 \teval_rewards:  -50.0\n",
      "iteration:  138 \teval_rewards:  -50.0\n",
      "iteration:  139 \teval_rewards:  -50.0\n",
      "iteration:  140 \teval_rewards:  -50.0\n",
      "iteration:  141 \teval_rewards:  -50.0\n",
      "iteration:  142 \teval_rewards:  -50.0\n",
      "iteration:  143 \teval_rewards:  -50.0\n",
      "iteration:  144 \teval_rewards:  -50.0\n",
      "iteration:  145 \teval_rewards:  -50.0\n",
      "iteration:  146 \teval_rewards:  -50.0\n",
      "iteration:  147 \teval_rewards:  -50.0\n",
      "iteration:  148 \teval_rewards:  -50.0\n",
      "iteration:  149 \teval_rewards:  -50.0\n",
      "iteration:  150 \teval_rewards:  -50.0\n",
      "iteration:  151 \teval_rewards:  -50.0\n",
      "iteration:  152 \teval_rewards:  -50.0\n",
      "iteration:  153 \teval_rewards:  -50.0\n",
      "iteration:  154 \teval_rewards:  -50.0\n",
      "iteration:  155 \teval_rewards:  -50.0\n",
      "iteration:  156 \teval_rewards:  -50.0\n",
      "iteration:  157 \teval_rewards:  -50.0\n",
      "iteration:  158 \teval_rewards:  -50.0\n",
      "iteration:  159 \teval_rewards:  -50.0\n",
      "iteration:  160 \teval_rewards:  -50.0\n",
      "iteration:  161 \teval_rewards:  -50.0\n",
      "iteration:  162 \teval_rewards:  -50.0\n",
      "iteration:  163 \teval_rewards:  -50.0\n",
      "iteration:  164 \teval_rewards:  -50.0\n",
      "iteration:  165 \teval_rewards:  -50.0\n",
      "iteration:  166 \teval_rewards:  -50.0\n",
      "iteration:  167 \teval_rewards:  -50.0\n",
      "iteration:  168 \teval_rewards:  -50.0\n",
      "iteration:  169 \teval_rewards:  -50.0\n",
      "iteration:  170 \teval_rewards:  -50.0\n",
      "iteration:  171 \teval_rewards:  -50.0\n",
      "iteration:  172 \teval_rewards:  -50.0\n",
      "iteration:  173 \teval_rewards:  -50.0\n",
      "iteration:  174 \teval_rewards:  -50.0\n",
      "iteration:  175 \teval_rewards:  -50.0\n",
      "iteration:  176 \teval_rewards:  -50.0\n",
      "iteration:  177 \teval_rewards:  -50.0\n",
      "iteration:  178 \teval_rewards:  -50.0\n",
      "iteration:  179 \teval_rewards:  -50.0\n",
      "iteration:  180 \teval_rewards:  -50.0\n",
      "iteration:  181 \teval_rewards:  -50.0\n",
      "iteration:  182 \teval_rewards:  -50.0\n",
      "iteration:  183 \teval_rewards:  -50.0\n",
      "iteration:  184 \teval_rewards:  -50.0\n",
      "iteration:  185 \teval_rewards:  -50.0\n",
      "iteration:  186 \teval_rewards:  -50.0\n",
      "iteration:  187 \teval_rewards:  -50.0\n",
      "iteration:  188 \teval_rewards:  -50.0\n",
      "iteration:  189 \teval_rewards:  -50.0\n",
      "iteration:  190 \teval_rewards:  -50.0\n",
      "iteration:  191 \teval_rewards:  -50.0\n",
      "iteration:  192 \teval_rewards:  -50.0\n",
      "iteration:  193 \teval_rewards:  -50.0\n",
      "iteration:  194 \teval_rewards:  -50.0\n",
      "iteration:  195 \teval_rewards:  -50.0\n",
      "iteration:  196 \teval_rewards:  -50.0\n",
      "iteration:  197 \teval_rewards:  -50.0\n",
      "iteration:  198 \teval_rewards:  -50.0\n",
      "iteration:  199 \teval_rewards:  -50.0\n",
      "Iter:200| Ep_Reward:-50.000| Running_reward:-48.818| Actor_Loss:0.302| Critic_Loss:1.094| Iter_duration:5.559| lr:[0.00026]\n",
      "iteration:  200 \teval_rewards:  -50.0\n",
      "iteration:  201 \teval_rewards:  -50.0\n",
      "iteration:  202 \teval_rewards:  -50.0\n",
      "iteration:  203 \teval_rewards:  -50.0\n",
      "iteration:  204 \teval_rewards:  -50.0\n",
      "iteration:  205 \teval_rewards:  -50.0\n",
      "iteration:  206 \teval_rewards:  -50.0\n",
      "iteration:  207 \teval_rewards:  -50.0\n",
      "iteration:  208 \teval_rewards:  -50.0\n",
      "iteration:  209 \teval_rewards:  -50.0\n",
      "iteration:  210 \teval_rewards:  -50.0\n",
      "iteration:  211 \teval_rewards:  -50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  212 \teval_rewards:  -50.0\n",
      "iteration:  213 \teval_rewards:  -50.0\n",
      "iteration:  214 \teval_rewards:  -50.0\n",
      "iteration:  215 \teval_rewards:  -50.0\n",
      "iteration:  216 \teval_rewards:  0.0\n",
      "iteration:  217 \teval_rewards:  -50.0\n",
      "iteration:  218 \teval_rewards:  -50.0\n",
      "iteration:  219 \teval_rewards:  -50.0\n",
      "iteration:  220 \teval_rewards:  -50.0\n",
      "iteration:  221 \teval_rewards:  -50.0\n",
      "iteration:  222 \teval_rewards:  -50.0\n",
      "iteration:  223 \teval_rewards:  -50.0\n",
      "iteration:  224 \teval_rewards:  -50.0\n",
      "iteration:  225 \teval_rewards:  -50.0\n",
      "iteration:  226 \teval_rewards:  -50.0\n",
      "iteration:  227 \teval_rewards:  -50.0\n",
      "iteration:  228 \teval_rewards:  0.0\n",
      "iteration:  229 \teval_rewards:  -50.0\n",
      "iteration:  230 \teval_rewards:  -50.0\n",
      "iteration:  231 \teval_rewards:  -50.0\n",
      "iteration:  232 \teval_rewards:  -50.0\n",
      "iteration:  233 \teval_rewards:  -50.0\n",
      "iteration:  234 \teval_rewards:  -50.0\n",
      "iteration:  235 \teval_rewards:  -50.0\n",
      "iteration:  236 \teval_rewards:  -50.0\n",
      "iteration:  237 \teval_rewards:  -50.0\n",
      "iteration:  238 \teval_rewards:  -50.0\n",
      "iteration:  239 \teval_rewards:  -50.0\n",
      "iteration:  240 \teval_rewards:  -50.0\n",
      "iteration:  241 \teval_rewards:  -50.0\n",
      "iteration:  242 \teval_rewards:  0.0\n",
      "iteration:  243 \teval_rewards:  -50.0\n",
      "iteration:  244 \teval_rewards:  -50.0\n",
      "iteration:  245 \teval_rewards:  -50.0\n",
      "iteration:  246 \teval_rewards:  -50.0\n",
      "iteration:  247 \teval_rewards:  -50.0\n",
      "iteration:  248 \teval_rewards:  -50.0\n",
      "iteration:  249 \teval_rewards:  -50.0\n",
      "iteration:  250 \teval_rewards:  -50.0\n",
      "iteration:  251 \teval_rewards:  -50.0\n",
      "iteration:  252 \teval_rewards:  -50.0\n",
      "iteration:  253 \teval_rewards:  -50.0\n",
      "iteration:  254 \teval_rewards:  -50.0\n",
      "iteration:  255 \teval_rewards:  -50.0\n",
      "iteration:  256 \teval_rewards:  -50.0\n",
      "iteration:  257 \teval_rewards:  -50.0\n",
      "iteration:  258 \teval_rewards:  -50.0\n",
      "iteration:  259 \teval_rewards:  -50.0\n",
      "iteration:  260 \teval_rewards:  -50.0\n",
      "iteration:  261 \teval_rewards:  -50.0\n",
      "iteration:  262 \teval_rewards:  -50.0\n",
      "iteration:  263 \teval_rewards:  -50.0\n",
      "iteration:  264 \teval_rewards:  -50.0\n",
      "iteration:  265 \teval_rewards:  -50.0\n",
      "iteration:  266 \teval_rewards:  -50.0\n",
      "iteration:  267 \teval_rewards:  -50.0\n",
      "iteration:  268 \teval_rewards:  -50.0\n",
      "iteration:  269 \teval_rewards:  -50.0\n",
      "iteration:  270 \teval_rewards:  -50.0\n",
      "iteration:  271 \teval_rewards:  -50.0\n",
      "iteration:  272 \teval_rewards:  -50.0\n",
      "iteration:  273 \teval_rewards:  -50.0\n",
      "iteration:  274 \teval_rewards:  -50.0\n",
      "iteration:  275 \teval_rewards:  -50.0\n",
      "iteration:  276 \teval_rewards:  -50.0\n",
      "iteration:  277 \teval_rewards:  -50.0\n",
      "iteration:  278 \teval_rewards:  -50.0\n",
      "iteration:  279 \teval_rewards:  -50.0\n",
      "iteration:  280 \teval_rewards:  -50.0\n",
      "iteration:  281 \teval_rewards:  -50.0\n",
      "iteration:  282 \teval_rewards:  -50.0\n",
      "iteration:  283 \teval_rewards:  -50.0\n",
      "iteration:  284 \teval_rewards:  -50.0\n",
      "iteration:  285 \teval_rewards:  -50.0\n",
      "iteration:  286 \teval_rewards:  -50.0\n",
      "iteration:  287 \teval_rewards:  -50.0\n",
      "iteration:  288 \teval_rewards:  -50.0\n",
      "iteration:  289 \teval_rewards:  -50.0\n",
      "iteration:  290 \teval_rewards:  -50.0\n",
      "iteration:  291 \teval_rewards:  -50.0\n",
      "iteration:  292 \teval_rewards:  -50.0\n",
      "iteration:  293 \teval_rewards:  -50.0\n",
      "iteration:  294 \teval_rewards:  -50.0\n",
      "iteration:  295 \teval_rewards:  -50.0\n",
      "iteration:  296 \teval_rewards:  -50.0\n",
      "iteration:  297 \teval_rewards:  -50.0\n",
      "iteration:  298 \teval_rewards:  -50.0\n",
      "iteration:  299 \teval_rewards:  0.0\n",
      "Iter:300| Ep_Reward:-50.000| Running_reward:-48.336| Actor_Loss:0.052| Critic_Loss:3.096| Iter_duration:5.551| lr:[0.00023999999999999998]\n",
      "iteration:  300 \teval_rewards:  -50.0\n",
      "iteration:  301 \teval_rewards:  -50.0\n",
      "iteration:  302 \teval_rewards:  -50.0\n",
      "iteration:  303 \teval_rewards:  -50.0\n",
      "iteration:  304 \teval_rewards:  -50.0\n",
      "iteration:  305 \teval_rewards:  -50.0\n",
      "iteration:  306 \teval_rewards:  -50.0\n",
      "iteration:  307 \teval_rewards:  -50.0\n",
      "iteration:  308 \teval_rewards:  -50.0\n",
      "iteration:  309 \teval_rewards:  -50.0\n",
      "iteration:  310 \teval_rewards:  -50.0\n",
      "iteration:  311 \teval_rewards:  -50.0\n",
      "iteration:  312 \teval_rewards:  -50.0\n",
      "iteration:  313 \teval_rewards:  -50.0\n",
      "iteration:  314 \teval_rewards:  0.0\n",
      "iteration:  315 \teval_rewards:  -50.0\n",
      "iteration:  316 \teval_rewards:  -50.0\n",
      "iteration:  317 \teval_rewards:  -50.0\n",
      "iteration:  318 \teval_rewards:  -50.0\n",
      "iteration:  319 \teval_rewards:  -50.0\n",
      "iteration:  320 \teval_rewards:  -50.0\n",
      "iteration:  321 \teval_rewards:  -50.0\n",
      "iteration:  322 \teval_rewards:  -50.0\n",
      "iteration:  323 \teval_rewards:  -50.0\n",
      "iteration:  324 \teval_rewards:  -50.0\n",
      "iteration:  325 \teval_rewards:  -50.0\n",
      "iteration:  326 \teval_rewards:  -50.0\n",
      "iteration:  327 \teval_rewards:  -50.0\n",
      "iteration:  328 \teval_rewards:  -50.0\n",
      "iteration:  329 \teval_rewards:  -50.0\n",
      "iteration:  330 \teval_rewards:  -50.0\n",
      "iteration:  331 \teval_rewards:  -50.0\n",
      "iteration:  332 \teval_rewards:  -50.0\n",
      "iteration:  333 \teval_rewards:  -50.0\n",
      "iteration:  334 \teval_rewards:  -50.0\n",
      "iteration:  335 \teval_rewards:  -50.0\n",
      "iteration:  336 \teval_rewards:  -50.0\n",
      "iteration:  337 \teval_rewards:  -50.0\n",
      "iteration:  338 \teval_rewards:  -50.0\n",
      "iteration:  339 \teval_rewards:  -50.0\n",
      "iteration:  340 \teval_rewards:  -50.0\n",
      "iteration:  341 \teval_rewards:  -50.0\n",
      "iteration:  342 \teval_rewards:  -50.0\n",
      "iteration:  343 \teval_rewards:  -50.0\n",
      "iteration:  344 \teval_rewards:  -50.0\n",
      "iteration:  345 \teval_rewards:  -50.0\n",
      "iteration:  346 \teval_rewards:  -50.0\n",
      "iteration:  347 \teval_rewards:  -50.0\n",
      "iteration:  348 \teval_rewards:  -50.0\n",
      "iteration:  349 \teval_rewards:  -50.0\n",
      "iteration:  350 \teval_rewards:  -50.0\n",
      "iteration:  351 \teval_rewards:  -50.0\n",
      "iteration:  352 \teval_rewards:  -50.0\n",
      "iteration:  353 \teval_rewards:  -50.0\n",
      "iteration:  354 \teval_rewards:  -50.0\n",
      "iteration:  355 \teval_rewards:  -50.0\n",
      "iteration:  356 \teval_rewards:  0.0\n",
      "iteration:  357 \teval_rewards:  -50.0\n",
      "iteration:  358 \teval_rewards:  -50.0\n",
      "iteration:  359 \teval_rewards:  -50.0\n",
      "iteration:  360 \teval_rewards:  -50.0\n",
      "iteration:  361 \teval_rewards:  -50.0\n",
      "iteration:  362 \teval_rewards:  -50.0\n",
      "iteration:  363 \teval_rewards:  -50.0\n",
      "iteration:  364 \teval_rewards:  -50.0\n",
      "iteration:  365 \teval_rewards:  -50.0\n",
      "iteration:  366 \teval_rewards:  -50.0\n",
      "iteration:  367 \teval_rewards:  -50.0\n",
      "iteration:  368 \teval_rewards:  -50.0\n",
      "iteration:  369 \teval_rewards:  -50.0\n",
      "iteration:  370 \teval_rewards:  -50.0\n",
      "iteration:  371 \teval_rewards:  -50.0\n",
      "iteration:  372 \teval_rewards:  -50.0\n",
      "iteration:  373 \teval_rewards:  -50.0\n",
      "iteration:  374 \teval_rewards:  -50.0\n",
      "iteration:  375 \teval_rewards:  -50.0\n",
      "iteration:  376 \teval_rewards:  -50.0\n",
      "iteration:  377 \teval_rewards:  -50.0\n",
      "iteration:  378 \teval_rewards:  -50.0\n",
      "iteration:  379 \teval_rewards:  -50.0\n",
      "iteration:  380 \teval_rewards:  0.0\n",
      "iteration:  381 \teval_rewards:  -50.0\n",
      "iteration:  382 \teval_rewards:  -50.0\n",
      "iteration:  383 \teval_rewards:  -50.0\n",
      "iteration:  384 \teval_rewards:  -50.0\n",
      "iteration:  385 \teval_rewards:  -50.0\n",
      "iteration:  386 \teval_rewards:  -50.0\n",
      "iteration:  387 \teval_rewards:  -50.0\n",
      "iteration:  388 \teval_rewards:  -50.0\n",
      "iteration:  389 \teval_rewards:  -50.0\n",
      "iteration:  390 \teval_rewards:  -50.0\n",
      "iteration:  391 \teval_rewards:  -50.0\n",
      "iteration:  392 \teval_rewards:  -50.0\n",
      "iteration:  393 \teval_rewards:  -50.0\n",
      "iteration:  394 \teval_rewards:  -50.0\n",
      "iteration:  395 \teval_rewards:  -50.0\n",
      "iteration:  396 \teval_rewards:  -50.0\n",
      "iteration:  397 \teval_rewards:  -50.0\n",
      "iteration:  398 \teval_rewards:  -50.0\n",
      "iteration:  399 \teval_rewards:  -50.0\n",
      "Iter:400| Ep_Reward:-50.000| Running_reward:-48.450| Actor_Loss:0.101| Critic_Loss:0.472| Iter_duration:5.615| lr:[0.00022]\n",
      "iteration:  400 \teval_rewards:  -50.0\n",
      "iteration:  401 \teval_rewards:  -50.0\n",
      "iteration:  402 \teval_rewards:  -50.0\n",
      "iteration:  403 \teval_rewards:  -50.0\n",
      "iteration:  404 \teval_rewards:  -50.0\n",
      "iteration:  405 \teval_rewards:  -50.0\n",
      "iteration:  406 \teval_rewards:  -50.0\n",
      "iteration:  407 \teval_rewards:  -50.0\n",
      "iteration:  408 \teval_rewards:  -50.0\n",
      "iteration:  409 \teval_rewards:  -50.0\n",
      "iteration:  410 \teval_rewards:  -50.0\n",
      "iteration:  411 \teval_rewards:  -50.0\n",
      "iteration:  412 \teval_rewards:  -50.0\n",
      "iteration:  413 \teval_rewards:  -50.0\n",
      "iteration:  414 \teval_rewards:  -50.0\n",
      "iteration:  415 \teval_rewards:  -50.0\n",
      "iteration:  416 \teval_rewards:  -50.0\n",
      "iteration:  417 \teval_rewards:  -50.0\n",
      "iteration:  418 \teval_rewards:  -50.0\n",
      "iteration:  419 \teval_rewards:  -50.0\n",
      "iteration:  420 \teval_rewards:  -50.0\n",
      "iteration:  421 \teval_rewards:  -50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  422 \teval_rewards:  -50.0\n",
      "iteration:  423 \teval_rewards:  -50.0\n",
      "iteration:  424 \teval_rewards:  -50.0\n",
      "iteration:  425 \teval_rewards:  -50.0\n",
      "iteration:  426 \teval_rewards:  -50.0\n",
      "iteration:  427 \teval_rewards:  -50.0\n",
      "iteration:  428 \teval_rewards:  -50.0\n",
      "iteration:  429 \teval_rewards:  -50.0\n",
      "iteration:  430 \teval_rewards:  -50.0\n",
      "iteration:  431 \teval_rewards:  -50.0\n",
      "iteration:  432 \teval_rewards:  -50.0\n",
      "iteration:  433 \teval_rewards:  -50.0\n",
      "iteration:  434 \teval_rewards:  -50.0\n",
      "iteration:  435 \teval_rewards:  -50.0\n",
      "iteration:  436 \teval_rewards:  -50.0\n",
      "iteration:  437 \teval_rewards:  -50.0\n",
      "iteration:  438 \teval_rewards:  -50.0\n",
      "iteration:  439 \teval_rewards:  -50.0\n",
      "iteration:  440 \teval_rewards:  -50.0\n",
      "iteration:  441 \teval_rewards:  -50.0\n",
      "iteration:  442 \teval_rewards:  -50.0\n",
      "iteration:  443 \teval_rewards:  -50.0\n",
      "iteration:  444 \teval_rewards:  -50.0\n",
      "iteration:  445 \teval_rewards:  -50.0\n",
      "iteration:  446 \teval_rewards:  -50.0\n",
      "iteration:  447 \teval_rewards:  0.0\n",
      "iteration:  448 \teval_rewards:  -50.0\n",
      "iteration:  449 \teval_rewards:  0.0\n",
      "iteration:  450 \teval_rewards:  -50.0\n",
      "iteration:  451 \teval_rewards:  -50.0\n",
      "iteration:  452 \teval_rewards:  -50.0\n",
      "iteration:  453 \teval_rewards:  -50.0\n",
      "iteration:  454 \teval_rewards:  -50.0\n",
      "iteration:  455 \teval_rewards:  -50.0\n",
      "iteration:  456 \teval_rewards:  -50.0\n",
      "iteration:  457 \teval_rewards:  -50.0\n",
      "iteration:  458 \teval_rewards:  -50.0\n",
      "iteration:  459 \teval_rewards:  -50.0\n",
      "iteration:  460 \teval_rewards:  -50.0\n",
      "iteration:  461 \teval_rewards:  -50.0\n",
      "iteration:  462 \teval_rewards:  -50.0\n",
      "iteration:  463 \teval_rewards:  -50.0\n",
      "iteration:  464 \teval_rewards:  -50.0\n",
      "iteration:  465 \teval_rewards:  -50.0\n",
      "iteration:  466 \teval_rewards:  -50.0\n",
      "iteration:  467 \teval_rewards:  -50.0\n",
      "iteration:  468 \teval_rewards:  -50.0\n",
      "iteration:  469 \teval_rewards:  -50.0\n",
      "iteration:  470 \teval_rewards:  -50.0\n",
      "iteration:  471 \teval_rewards:  -50.0\n",
      "iteration:  472 \teval_rewards:  -50.0\n",
      "iteration:  473 \teval_rewards:  -50.0\n",
      "iteration:  474 \teval_rewards:  -50.0\n",
      "iteration:  475 \teval_rewards:  -50.0\n",
      "iteration:  476 \teval_rewards:  -50.0\n",
      "iteration:  477 \teval_rewards:  -50.0\n",
      "iteration:  478 \teval_rewards:  -50.0\n",
      "iteration:  479 \teval_rewards:  -50.0\n",
      "iteration:  480 \teval_rewards:  -50.0\n",
      "iteration:  481 \teval_rewards:  -50.0\n",
      "iteration:  482 \teval_rewards:  -50.0\n",
      "iteration:  483 \teval_rewards:  -50.0\n",
      "iteration:  484 \teval_rewards:  -50.0\n",
      "iteration:  485 \teval_rewards:  -50.0\n",
      "iteration:  486 \teval_rewards:  -50.0\n",
      "iteration:  487 \teval_rewards:  -50.0\n",
      "iteration:  488 \teval_rewards:  -50.0\n",
      "iteration:  489 \teval_rewards:  -50.0\n",
      "iteration:  490 \teval_rewards:  -50.0\n",
      "iteration:  491 \teval_rewards:  -50.0\n",
      "iteration:  492 \teval_rewards:  -50.0\n",
      "iteration:  493 \teval_rewards:  -50.0\n",
      "iteration:  494 \teval_rewards:  -50.0\n",
      "iteration:  495 \teval_rewards:  -50.0\n",
      "iteration:  496 \teval_rewards:  -50.0\n",
      "iteration:  497 \teval_rewards:  -50.0\n",
      "iteration:  498 \teval_rewards:  -50.0\n",
      "iteration:  499 \teval_rewards:  -50.0\n",
      "Iter:500| Ep_Reward:-50.000| Running_reward:-48.840| Actor_Loss:-0.026| Critic_Loss:1.238| Iter_duration:5.526| lr:[0.0002]\n",
      "iteration:  500 \teval_rewards:  -50.0\n",
      "iteration:  501 \teval_rewards:  -50.0\n",
      "iteration:  502 \teval_rewards:  -50.0\n",
      "iteration:  503 \teval_rewards:  -50.0\n",
      "iteration:  504 \teval_rewards:  -50.0\n",
      "iteration:  505 \teval_rewards:  -50.0\n",
      "iteration:  506 \teval_rewards:  -50.0\n",
      "iteration:  507 \teval_rewards:  -50.0\n",
      "iteration:  508 \teval_rewards:  -50.0\n",
      "iteration:  509 \teval_rewards:  -50.0\n",
      "iteration:  510 \teval_rewards:  -50.0\n",
      "iteration:  511 \teval_rewards:  -50.0\n",
      "iteration:  512 \teval_rewards:  -50.0\n",
      "iteration:  513 \teval_rewards:  -50.0\n",
      "iteration:  514 \teval_rewards:  -50.0\n",
      "iteration:  515 \teval_rewards:  -50.0\n",
      "iteration:  516 \teval_rewards:  -50.0\n",
      "iteration:  517 \teval_rewards:  -50.0\n",
      "iteration:  518 \teval_rewards:  0.0\n",
      "iteration:  519 \teval_rewards:  -50.0\n",
      "iteration:  520 \teval_rewards:  -50.0\n",
      "iteration:  521 \teval_rewards:  -50.0\n",
      "iteration:  522 \teval_rewards:  -50.0\n",
      "iteration:  523 \teval_rewards:  -50.0\n",
      "iteration:  524 \teval_rewards:  -50.0\n",
      "iteration:  525 \teval_rewards:  -50.0\n",
      "iteration:  526 \teval_rewards:  -50.0\n",
      "iteration:  527 \teval_rewards:  -50.0\n",
      "iteration:  528 \teval_rewards:  -50.0\n",
      "iteration:  529 \teval_rewards:  -50.0\n",
      "iteration:  530 \teval_rewards:  -50.0\n",
      "iteration:  531 \teval_rewards:  -50.0\n",
      "iteration:  532 \teval_rewards:  -50.0\n",
      "iteration:  533 \teval_rewards:  -50.0\n",
      "iteration:  534 \teval_rewards:  -50.0\n",
      "iteration:  535 \teval_rewards:  -50.0\n",
      "iteration:  536 \teval_rewards:  -50.0\n",
      "iteration:  537 \teval_rewards:  -50.0\n",
      "iteration:  538 \teval_rewards:  -50.0\n",
      "iteration:  539 \teval_rewards:  -50.0\n",
      "iteration:  540 \teval_rewards:  -50.0\n",
      "iteration:  541 \teval_rewards:  -50.0\n",
      "iteration:  542 \teval_rewards:  -50.0\n",
      "iteration:  543 \teval_rewards:  -50.0\n",
      "iteration:  544 \teval_rewards:  -50.0\n",
      "iteration:  545 \teval_rewards:  -50.0\n",
      "iteration:  546 \teval_rewards:  -50.0\n",
      "iteration:  547 \teval_rewards:  -50.0\n",
      "iteration:  548 \teval_rewards:  -50.0\n",
      "iteration:  549 \teval_rewards:  -50.0\n",
      "iteration:  550 \teval_rewards:  -50.0\n",
      "iteration:  551 \teval_rewards:  -50.0\n",
      "iteration:  552 \teval_rewards:  -50.0\n",
      "iteration:  553 \teval_rewards:  -50.0\n",
      "iteration:  554 \teval_rewards:  -50.0\n",
      "iteration:  555 \teval_rewards:  -50.0\n",
      "iteration:  556 \teval_rewards:  -50.0\n",
      "iteration:  557 \teval_rewards:  -50.0\n",
      "iteration:  558 \teval_rewards:  -50.0\n",
      "iteration:  559 \teval_rewards:  -50.0\n",
      "iteration:  560 \teval_rewards:  -50.0\n",
      "iteration:  561 \teval_rewards:  -50.0\n",
      "iteration:  562 \teval_rewards:  -50.0\n",
      "iteration:  563 \teval_rewards:  -50.0\n",
      "iteration:  564 \teval_rewards:  -50.0\n",
      "iteration:  565 \teval_rewards:  -50.0\n",
      "iteration:  566 \teval_rewards:  -50.0\n",
      "iteration:  567 \teval_rewards:  -50.0\n",
      "iteration:  568 \teval_rewards:  -50.0\n",
      "iteration:  569 \teval_rewards:  -50.0\n",
      "iteration:  570 \teval_rewards:  -50.0\n",
      "iteration:  571 \teval_rewards:  -50.0\n",
      "iteration:  572 \teval_rewards:  -50.0\n",
      "iteration:  573 \teval_rewards:  -50.0\n",
      "iteration:  574 \teval_rewards:  -50.0\n",
      "iteration:  575 \teval_rewards:  -50.0\n",
      "iteration:  576 \teval_rewards:  -50.0\n",
      "iteration:  577 \teval_rewards:  -50.0\n",
      "iteration:  578 \teval_rewards:  -50.0\n",
      "iteration:  579 \teval_rewards:  -50.0\n",
      "iteration:  580 \teval_rewards:  -50.0\n",
      "iteration:  581 \teval_rewards:  -50.0\n",
      "iteration:  582 \teval_rewards:  -50.0\n",
      "iteration:  583 \teval_rewards:  -50.0\n",
      "iteration:  584 \teval_rewards:  -50.0\n",
      "iteration:  585 \teval_rewards:  -50.0\n",
      "iteration:  586 \teval_rewards:  -50.0\n",
      "iteration:  587 \teval_rewards:  -50.0\n",
      "iteration:  588 \teval_rewards:  -50.0\n",
      "iteration:  589 \teval_rewards:  -50.0\n",
      "iteration:  590 \teval_rewards:  -50.0\n",
      "iteration:  591 \teval_rewards:  -50.0\n",
      "iteration:  592 \teval_rewards:  0.0\n",
      "iteration:  593 \teval_rewards:  -50.0\n",
      "iteration:  594 \teval_rewards:  -50.0\n",
      "iteration:  595 \teval_rewards:  -50.0\n",
      "iteration:  596 \teval_rewards:  -50.0\n",
      "iteration:  597 \teval_rewards:  -50.0\n",
      "iteration:  598 \teval_rewards:  -50.0\n",
      "iteration:  599 \teval_rewards:  0.0\n",
      "Iter:600| Ep_Reward:-50.000| Running_reward:-48.400| Actor_Loss:-0.207| Critic_Loss:0.685| Iter_duration:5.473| lr:[0.00017999999999999998]\n",
      "iteration:  600 \teval_rewards:  -50.0\n",
      "iteration:  601 \teval_rewards:  -50.0\n",
      "iteration:  602 \teval_rewards:  -50.0\n",
      "iteration:  603 \teval_rewards:  -50.0\n",
      "iteration:  604 \teval_rewards:  -50.0\n",
      "iteration:  605 \teval_rewards:  -50.0\n",
      "iteration:  606 \teval_rewards:  -50.0\n",
      "iteration:  607 \teval_rewards:  -50.0\n",
      "iteration:  608 \teval_rewards:  -50.0\n",
      "iteration:  609 \teval_rewards:  -50.0\n",
      "iteration:  610 \teval_rewards:  -50.0\n",
      "iteration:  611 \teval_rewards:  -50.0\n",
      "iteration:  612 \teval_rewards:  -50.0\n",
      "iteration:  613 \teval_rewards:  -50.0\n",
      "iteration:  614 \teval_rewards:  -50.0\n",
      "iteration:  615 \teval_rewards:  -50.0\n",
      "iteration:  616 \teval_rewards:  -50.0\n",
      "iteration:  617 \teval_rewards:  -50.0\n",
      "iteration:  618 \teval_rewards:  -50.0\n",
      "iteration:  619 \teval_rewards:  -50.0\n",
      "iteration:  620 \teval_rewards:  -50.0\n",
      "iteration:  621 \teval_rewards:  -50.0\n",
      "iteration:  622 \teval_rewards:  -50.0\n",
      "iteration:  623 \teval_rewards:  -50.0\n",
      "iteration:  624 \teval_rewards:  -50.0\n",
      "iteration:  625 \teval_rewards:  -50.0\n",
      "iteration:  626 \teval_rewards:  -50.0\n",
      "iteration:  627 \teval_rewards:  -50.0\n",
      "iteration:  628 \teval_rewards:  -50.0\n",
      "iteration:  629 \teval_rewards:  -50.0\n",
      "iteration:  630 \teval_rewards:  -50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  631 \teval_rewards:  -50.0\n",
      "iteration:  632 \teval_rewards:  -50.0\n",
      "iteration:  633 \teval_rewards:  -50.0\n",
      "iteration:  634 \teval_rewards:  -50.0\n",
      "iteration:  635 \teval_rewards:  -50.0\n",
      "iteration:  636 \teval_rewards:  -50.0\n",
      "iteration:  637 \teval_rewards:  -50.0\n",
      "iteration:  638 \teval_rewards:  -50.0\n",
      "iteration:  639 \teval_rewards:  -50.0\n",
      "iteration:  640 \teval_rewards:  -50.0\n",
      "iteration:  641 \teval_rewards:  -50.0\n",
      "iteration:  642 \teval_rewards:  -50.0\n",
      "iteration:  643 \teval_rewards:  -50.0\n",
      "iteration:  644 \teval_rewards:  -50.0\n",
      "iteration:  645 \teval_rewards:  -50.0\n",
      "iteration:  646 \teval_rewards:  -50.0\n",
      "iteration:  647 \teval_rewards:  -50.0\n",
      "iteration:  648 \teval_rewards:  -50.0\n",
      "iteration:  649 \teval_rewards:  -50.0\n",
      "iteration:  650 \teval_rewards:  -50.0\n",
      "iteration:  651 \teval_rewards:  -50.0\n",
      "iteration:  652 \teval_rewards:  -50.0\n",
      "iteration:  653 \teval_rewards:  -50.0\n",
      "iteration:  654 \teval_rewards:  -50.0\n",
      "iteration:  655 \teval_rewards:  -50.0\n",
      "iteration:  656 \teval_rewards:  -50.0\n",
      "iteration:  657 \teval_rewards:  -50.0\n",
      "iteration:  658 \teval_rewards:  -50.0\n",
      "iteration:  659 \teval_rewards:  -50.0\n",
      "iteration:  660 \teval_rewards:  -50.0\n",
      "iteration:  661 \teval_rewards:  -50.0\n",
      "iteration:  662 \teval_rewards:  -50.0\n",
      "iteration:  663 \teval_rewards:  -50.0\n",
      "iteration:  664 \teval_rewards:  -50.0\n",
      "iteration:  665 \teval_rewards:  -50.0\n",
      "iteration:  666 \teval_rewards:  -50.0\n",
      "iteration:  667 \teval_rewards:  -50.0\n",
      "iteration:  668 \teval_rewards:  -50.0\n",
      "iteration:  669 \teval_rewards:  -50.0\n",
      "iteration:  670 \teval_rewards:  -50.0\n",
      "iteration:  671 \teval_rewards:  -50.0\n",
      "iteration:  672 \teval_rewards:  -50.0\n",
      "iteration:  673 \teval_rewards:  -50.0\n",
      "iteration:  674 \teval_rewards:  -50.0\n",
      "iteration:  675 \teval_rewards:  -50.0\n",
      "iteration:  676 \teval_rewards:  -50.0\n",
      "iteration:  677 \teval_rewards:  -50.0\n",
      "iteration:  678 \teval_rewards:  -50.0\n",
      "iteration:  679 \teval_rewards:  -50.0\n",
      "iteration:  680 \teval_rewards:  -50.0\n",
      "iteration:  681 \teval_rewards:  -50.0\n",
      "iteration:  682 \teval_rewards:  -50.0\n",
      "iteration:  683 \teval_rewards:  -50.0\n",
      "iteration:  684 \teval_rewards:  -50.0\n",
      "iteration:  685 \teval_rewards:  -50.0\n",
      "iteration:  686 \teval_rewards:  -50.0\n",
      "iteration:  687 \teval_rewards:  -50.0\n",
      "iteration:  688 \teval_rewards:  -50.0\n",
      "iteration:  689 \teval_rewards:  -50.0\n",
      "iteration:  690 \teval_rewards:  -50.0\n",
      "iteration:  691 \teval_rewards:  -50.0\n",
      "iteration:  692 \teval_rewards:  -50.0\n",
      "iteration:  693 \teval_rewards:  -50.0\n",
      "iteration:  694 \teval_rewards:  -50.0\n",
      "iteration:  695 \teval_rewards:  -50.0\n",
      "iteration:  696 \teval_rewards:  -50.0\n",
      "iteration:  697 \teval_rewards:  0.0\n",
      "iteration:  698 \teval_rewards:  -50.0\n",
      "iteration:  699 \teval_rewards:  -50.0\n",
      "Iter:700| Ep_Reward:-50.000| Running_reward:-48.929| Actor_Loss:-0.164| Critic_Loss:0.524| Iter_duration:5.437| lr:[0.00015999999999999999]\n",
      "iteration:  700 \teval_rewards:  -50.0\n",
      "iteration:  701 \teval_rewards:  -50.0\n",
      "iteration:  702 \teval_rewards:  -50.0\n",
      "iteration:  703 \teval_rewards:  0.0\n",
      "iteration:  704 \teval_rewards:  -50.0\n",
      "iteration:  705 \teval_rewards:  -50.0\n",
      "iteration:  706 \teval_rewards:  -50.0\n",
      "iteration:  707 \teval_rewards:  -50.0\n",
      "iteration:  708 \teval_rewards:  -50.0\n",
      "iteration:  709 \teval_rewards:  -50.0\n",
      "iteration:  710 \teval_rewards:  -50.0\n",
      "iteration:  711 \teval_rewards:  -50.0\n",
      "iteration:  712 \teval_rewards:  -50.0\n",
      "iteration:  713 \teval_rewards:  -50.0\n",
      "iteration:  714 \teval_rewards:  -50.0\n",
      "iteration:  715 \teval_rewards:  -50.0\n",
      "iteration:  716 \teval_rewards:  -50.0\n",
      "iteration:  717 \teval_rewards:  -50.0\n",
      "iteration:  718 \teval_rewards:  -50.0\n",
      "iteration:  719 \teval_rewards:  -50.0\n",
      "iteration:  720 \teval_rewards:  -50.0\n",
      "iteration:  721 \teval_rewards:  -50.0\n",
      "iteration:  722 \teval_rewards:  -50.0\n",
      "iteration:  723 \teval_rewards:  -50.0\n",
      "iteration:  724 \teval_rewards:  -50.0\n",
      "iteration:  725 \teval_rewards:  -50.0\n",
      "iteration:  726 \teval_rewards:  -50.0\n",
      "iteration:  727 \teval_rewards:  -50.0\n",
      "iteration:  728 \teval_rewards:  -50.0\n",
      "iteration:  729 \teval_rewards:  -50.0\n",
      "iteration:  730 \teval_rewards:  -50.0\n",
      "iteration:  731 \teval_rewards:  -50.0\n",
      "iteration:  732 \teval_rewards:  -50.0\n",
      "iteration:  733 \teval_rewards:  -50.0\n",
      "iteration:  734 \teval_rewards:  -50.0\n",
      "iteration:  735 \teval_rewards:  -50.0\n",
      "iteration:  736 \teval_rewards:  -50.0\n",
      "iteration:  737 \teval_rewards:  -50.0\n",
      "iteration:  738 \teval_rewards:  -50.0\n",
      "iteration:  739 \teval_rewards:  -50.0\n",
      "iteration:  740 \teval_rewards:  -50.0\n",
      "iteration:  741 \teval_rewards:  -50.0\n",
      "iteration:  742 \teval_rewards:  -50.0\n",
      "iteration:  743 \teval_rewards:  -50.0\n",
      "iteration:  744 \teval_rewards:  -50.0\n",
      "iteration:  745 \teval_rewards:  -50.0\n",
      "iteration:  746 \teval_rewards:  -50.0\n",
      "iteration:  747 \teval_rewards:  -50.0\n",
      "iteration:  748 \teval_rewards:  -50.0\n",
      "iteration:  749 \teval_rewards:  -50.0\n",
      "iteration:  750 \teval_rewards:  -50.0\n",
      "iteration:  751 \teval_rewards:  -50.0\n",
      "iteration:  752 \teval_rewards:  -50.0\n",
      "iteration:  753 \teval_rewards:  -50.0\n",
      "iteration:  754 \teval_rewards:  -50.0\n",
      "iteration:  755 \teval_rewards:  -50.0\n",
      "iteration:  756 \teval_rewards:  0.0\n",
      "iteration:  757 \teval_rewards:  -50.0\n",
      "iteration:  758 \teval_rewards:  -50.0\n",
      "iteration:  759 \teval_rewards:  -50.0\n",
      "iteration:  760 \teval_rewards:  -50.0\n",
      "iteration:  761 \teval_rewards:  0.0\n",
      "iteration:  762 \teval_rewards:  -50.0\n",
      "iteration:  763 \teval_rewards:  -50.0\n",
      "iteration:  764 \teval_rewards:  -50.0\n",
      "iteration:  765 \teval_rewards:  -50.0\n",
      "iteration:  766 \teval_rewards:  -50.0\n",
      "iteration:  767 \teval_rewards:  -50.0\n",
      "iteration:  768 \teval_rewards:  -50.0\n",
      "iteration:  769 \teval_rewards:  -50.0\n",
      "iteration:  770 \teval_rewards:  -50.0\n",
      "iteration:  771 \teval_rewards:  -50.0\n",
      "iteration:  772 \teval_rewards:  -50.0\n",
      "iteration:  773 \teval_rewards:  -50.0\n",
      "iteration:  774 \teval_rewards:  -50.0\n",
      "iteration:  775 \teval_rewards:  -50.0\n",
      "iteration:  776 \teval_rewards:  -50.0\n",
      "iteration:  777 \teval_rewards:  -50.0\n",
      "iteration:  778 \teval_rewards:  -50.0\n",
      "iteration:  779 \teval_rewards:  -50.0\n",
      "iteration:  780 \teval_rewards:  -50.0\n",
      "iteration:  781 \teval_rewards:  -50.0\n",
      "iteration:  782 \teval_rewards:  -50.0\n",
      "iteration:  783 \teval_rewards:  -50.0\n",
      "iteration:  784 \teval_rewards:  0.0\n",
      "iteration:  785 \teval_rewards:  -50.0\n",
      "iteration:  786 \teval_rewards:  0.0\n",
      "iteration:  787 \teval_rewards:  -50.0\n",
      "iteration:  788 \teval_rewards:  -50.0\n",
      "iteration:  789 \teval_rewards:  -50.0\n",
      "iteration:  790 \teval_rewards:  -50.0\n",
      "iteration:  791 \teval_rewards:  -50.0\n",
      "iteration:  792 \teval_rewards:  -50.0\n",
      "iteration:  793 \teval_rewards:  -50.0\n",
      "iteration:  794 \teval_rewards:  -50.0\n",
      "iteration:  795 \teval_rewards:  -50.0\n",
      "iteration:  796 \teval_rewards:  -50.0\n",
      "iteration:  797 \teval_rewards:  0.0\n",
      "iteration:  798 \teval_rewards:  -50.0\n",
      "iteration:  799 \teval_rewards:  -50.0\n",
      "Iter:800| Ep_Reward:-50.000| Running_reward:-47.415| Actor_Loss:-0.081| Critic_Loss:0.394| Iter_duration:5.448| lr:[0.00014]\n",
      "iteration:  800 \teval_rewards:  -50.0\n",
      "iteration:  801 \teval_rewards:  -50.0\n",
      "iteration:  802 \teval_rewards:  -50.0\n",
      "iteration:  803 \teval_rewards:  -50.0\n",
      "iteration:  804 \teval_rewards:  -50.0\n",
      "iteration:  805 \teval_rewards:  -50.0\n",
      "iteration:  806 \teval_rewards:  -50.0\n",
      "iteration:  807 \teval_rewards:  -50.0\n",
      "iteration:  808 \teval_rewards:  -50.0\n",
      "iteration:  809 \teval_rewards:  -50.0\n",
      "iteration:  810 \teval_rewards:  -50.0\n",
      "iteration:  811 \teval_rewards:  -50.0\n",
      "iteration:  812 \teval_rewards:  -50.0\n",
      "iteration:  813 \teval_rewards:  -50.0\n",
      "iteration:  814 \teval_rewards:  -50.0\n",
      "iteration:  815 \teval_rewards:  -50.0\n",
      "iteration:  816 \teval_rewards:  -50.0\n",
      "iteration:  817 \teval_rewards:  -50.0\n",
      "iteration:  818 \teval_rewards:  -50.0\n",
      "iteration:  819 \teval_rewards:  -50.0\n",
      "iteration:  820 \teval_rewards:  -50.0\n",
      "iteration:  821 \teval_rewards:  -50.0\n",
      "iteration:  822 \teval_rewards:  -50.0\n",
      "iteration:  823 \teval_rewards:  -50.0\n",
      "iteration:  824 \teval_rewards:  -50.0\n",
      "iteration:  825 \teval_rewards:  -50.0\n",
      "iteration:  826 \teval_rewards:  -50.0\n",
      "iteration:  827 \teval_rewards:  -50.0\n",
      "iteration:  828 \teval_rewards:  -50.0\n",
      "iteration:  829 \teval_rewards:  -50.0\n",
      "iteration:  830 \teval_rewards:  -50.0\n",
      "iteration:  831 \teval_rewards:  -50.0\n",
      "iteration:  832 \teval_rewards:  -50.0\n",
      "iteration:  833 \teval_rewards:  -50.0\n",
      "iteration:  834 \teval_rewards:  -50.0\n",
      "iteration:  835 \teval_rewards:  -50.0\n",
      "iteration:  836 \teval_rewards:  -50.0\n",
      "iteration:  837 \teval_rewards:  -50.0\n",
      "iteration:  838 \teval_rewards:  0.0\n",
      "iteration:  839 \teval_rewards:  -50.0\n",
      "iteration:  840 \teval_rewards:  -50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  841 \teval_rewards:  -50.0\n",
      "iteration:  842 \teval_rewards:  -50.0\n",
      "iteration:  843 \teval_rewards:  -50.0\n",
      "iteration:  844 \teval_rewards:  -50.0\n",
      "iteration:  845 \teval_rewards:  -50.0\n",
      "iteration:  846 \teval_rewards:  -50.0\n",
      "iteration:  847 \teval_rewards:  -50.0\n",
      "iteration:  848 \teval_rewards:  -50.0\n",
      "iteration:  849 \teval_rewards:  -50.0\n",
      "iteration:  850 \teval_rewards:  -50.0\n",
      "iteration:  851 \teval_rewards:  -50.0\n",
      "iteration:  852 \teval_rewards:  0.0\n",
      "iteration:  853 \teval_rewards:  -50.0\n",
      "iteration:  854 \teval_rewards:  -50.0\n",
      "iteration:  855 \teval_rewards:  -50.0\n",
      "iteration:  856 \teval_rewards:  -50.0\n",
      "iteration:  857 \teval_rewards:  -50.0\n",
      "iteration:  858 \teval_rewards:  -50.0\n",
      "iteration:  859 \teval_rewards:  -50.0\n",
      "iteration:  860 \teval_rewards:  -50.0\n",
      "iteration:  861 \teval_rewards:  -50.0\n",
      "iteration:  862 \teval_rewards:  -50.0\n",
      "iteration:  863 \teval_rewards:  -50.0\n",
      "iteration:  864 \teval_rewards:  -50.0\n",
      "iteration:  865 \teval_rewards:  -50.0\n",
      "iteration:  866 \teval_rewards:  -50.0\n",
      "iteration:  867 \teval_rewards:  -50.0\n",
      "iteration:  868 \teval_rewards:  -50.0\n",
      "iteration:  869 \teval_rewards:  -50.0\n",
      "iteration:  870 \teval_rewards:  -50.0\n",
      "iteration:  871 \teval_rewards:  -50.0\n",
      "iteration:  872 \teval_rewards:  -50.0\n",
      "iteration:  873 \teval_rewards:  -50.0\n",
      "iteration:  874 \teval_rewards:  -50.0\n",
      "iteration:  875 \teval_rewards:  -50.0\n",
      "iteration:  876 \teval_rewards:  -50.0\n",
      "iteration:  877 \teval_rewards:  -50.0\n",
      "iteration:  878 \teval_rewards:  -50.0\n",
      "iteration:  879 \teval_rewards:  -50.0\n",
      "iteration:  880 \teval_rewards:  -50.0\n",
      "iteration:  881 \teval_rewards:  -50.0\n",
      "iteration:  882 \teval_rewards:  -50.0\n",
      "iteration:  883 \teval_rewards:  -50.0\n",
      "iteration:  884 \teval_rewards:  -50.0\n",
      "iteration:  885 \teval_rewards:  -50.0\n",
      "iteration:  886 \teval_rewards:  -50.0\n",
      "iteration:  887 \teval_rewards:  -50.0\n",
      "iteration:  888 \teval_rewards:  -50.0\n",
      "iteration:  889 \teval_rewards:  -50.0\n",
      "iteration:  890 \teval_rewards:  -50.0\n",
      "iteration:  891 \teval_rewards:  -50.0\n",
      "iteration:  892 \teval_rewards:  -50.0\n",
      "iteration:  893 \teval_rewards:  -50.0\n",
      "iteration:  894 \teval_rewards:  -50.0\n",
      "iteration:  895 \teval_rewards:  -50.0\n",
      "iteration:  896 \teval_rewards:  -50.0\n",
      "iteration:  897 \teval_rewards:  -50.0\n",
      "iteration:  898 \teval_rewards:  -50.0\n",
      "iteration:  899 \teval_rewards:  -50.0\n",
      "Iter:900| Ep_Reward:-50.000| Running_reward:-48.477| Actor_Loss:0.072| Critic_Loss:0.192| Iter_duration:5.555| lr:[0.00011999999999999999]\n",
      "iteration:  900 \teval_rewards:  -50.0\n",
      "iteration:  901 \teval_rewards:  -50.0\n",
      "iteration:  902 \teval_rewards:  -50.0\n",
      "iteration:  903 \teval_rewards:  -50.0\n",
      "iteration:  904 \teval_rewards:  -50.0\n",
      "iteration:  905 \teval_rewards:  -50.0\n",
      "iteration:  906 \teval_rewards:  -50.0\n",
      "iteration:  907 \teval_rewards:  -50.0\n",
      "iteration:  908 \teval_rewards:  -50.0\n",
      "iteration:  909 \teval_rewards:  0.0\n",
      "iteration:  910 \teval_rewards:  -50.0\n",
      "iteration:  911 \teval_rewards:  -50.0\n",
      "iteration:  912 \teval_rewards:  -50.0\n",
      "iteration:  913 \teval_rewards:  -50.0\n",
      "iteration:  914 \teval_rewards:  -50.0\n",
      "iteration:  915 \teval_rewards:  -50.0\n",
      "iteration:  916 \teval_rewards:  -50.0\n",
      "iteration:  917 \teval_rewards:  -50.0\n",
      "iteration:  918 \teval_rewards:  -50.0\n",
      "iteration:  919 \teval_rewards:  -50.0\n",
      "iteration:  920 \teval_rewards:  -50.0\n",
      "iteration:  921 \teval_rewards:  -50.0\n",
      "iteration:  922 \teval_rewards:  -50.0\n",
      "iteration:  923 \teval_rewards:  -50.0\n",
      "iteration:  924 \teval_rewards:  -50.0\n",
      "iteration:  925 \teval_rewards:  0.0\n",
      "iteration:  926 \teval_rewards:  -50.0\n",
      "iteration:  927 \teval_rewards:  -50.0\n",
      "iteration:  928 \teval_rewards:  -50.0\n",
      "iteration:  929 \teval_rewards:  -50.0\n",
      "iteration:  930 \teval_rewards:  -50.0\n",
      "iteration:  931 \teval_rewards:  -50.0\n",
      "iteration:  932 \teval_rewards:  -50.0\n",
      "iteration:  933 \teval_rewards:  -50.0\n",
      "iteration:  934 \teval_rewards:  -50.0\n",
      "iteration:  935 \teval_rewards:  -50.0\n",
      "iteration:  936 \teval_rewards:  -50.0\n",
      "iteration:  937 \teval_rewards:  -50.0\n",
      "iteration:  938 \teval_rewards:  -50.0\n",
      "iteration:  939 \teval_rewards:  -50.0\n",
      "iteration:  940 \teval_rewards:  -50.0\n",
      "iteration:  941 \teval_rewards:  -50.0\n",
      "iteration:  942 \teval_rewards:  -50.0\n",
      "iteration:  943 \teval_rewards:  -50.0\n",
      "iteration:  944 \teval_rewards:  -50.0\n",
      "iteration:  945 \teval_rewards:  -50.0\n",
      "iteration:  946 \teval_rewards:  -50.0\n",
      "iteration:  947 \teval_rewards:  -50.0\n",
      "iteration:  948 \teval_rewards:  -50.0\n",
      "iteration:  949 \teval_rewards:  -50.0\n",
      "iteration:  950 \teval_rewards:  -50.0\n",
      "iteration:  951 \teval_rewards:  -50.0\n",
      "iteration:  952 \teval_rewards:  -50.0\n",
      "iteration:  953 \teval_rewards:  -50.0\n",
      "iteration:  954 \teval_rewards:  -50.0\n",
      "iteration:  955 \teval_rewards:  -50.0\n",
      "iteration:  956 \teval_rewards:  -50.0\n",
      "iteration:  957 \teval_rewards:  -50.0\n",
      "iteration:  958 \teval_rewards:  -50.0\n",
      "iteration:  959 \teval_rewards:  -50.0\n",
      "iteration:  960 \teval_rewards:  -50.0\n",
      "iteration:  961 \teval_rewards:  -50.0\n",
      "iteration:  962 \teval_rewards:  -50.0\n",
      "iteration:  963 \teval_rewards:  -50.0\n",
      "iteration:  964 \teval_rewards:  -50.0\n",
      "iteration:  965 \teval_rewards:  -50.0\n",
      "iteration:  966 \teval_rewards:  -50.0\n",
      "iteration:  967 \teval_rewards:  -50.0\n",
      "iteration:  968 \teval_rewards:  -50.0\n",
      "iteration:  969 \teval_rewards:  -50.0\n",
      "iteration:  970 \teval_rewards:  -50.0\n",
      "iteration:  971 \teval_rewards:  -50.0\n",
      "iteration:  972 \teval_rewards:  -50.0\n",
      "iteration:  973 \teval_rewards:  -50.0\n",
      "iteration:  974 \teval_rewards:  -50.0\n",
      "iteration:  975 \teval_rewards:  -50.0\n",
      "iteration:  976 \teval_rewards:  -50.0\n",
      "iteration:  977 \teval_rewards:  -50.0\n",
      "iteration:  978 \teval_rewards:  0.0\n",
      "iteration:  979 \teval_rewards:  -50.0\n",
      "iteration:  980 \teval_rewards:  -50.0\n",
      "iteration:  981 \teval_rewards:  0.0\n",
      "iteration:  982 \teval_rewards:  -50.0\n",
      "iteration:  983 \teval_rewards:  -50.0\n",
      "iteration:  984 \teval_rewards:  -50.0\n",
      "iteration:  985 \teval_rewards:  -50.0\n",
      "iteration:  986 \teval_rewards:  -50.0\n",
      "iteration:  987 \teval_rewards:  -50.0\n",
      "iteration:  988 \teval_rewards:  -50.0\n",
      "iteration:  989 \teval_rewards:  -50.0\n",
      "iteration:  990 \teval_rewards:  -50.0\n",
      "iteration:  991 \teval_rewards:  -50.0\n",
      "iteration:  992 \teval_rewards:  -50.0\n",
      "iteration:  993 \teval_rewards:  -50.0\n",
      "iteration:  994 \teval_rewards:  -50.0\n",
      "iteration:  995 \teval_rewards:  -50.0\n",
      "iteration:  996 \teval_rewards:  -50.0\n",
      "iteration:  997 \teval_rewards:  -50.0\n",
      "iteration:  998 \teval_rewards:  -50.0\n",
      "iteration:  999 \teval_rewards:  -50.0\n",
      "Iter:1000| Ep_Reward:-50.000| Running_reward:-48.193| Actor_Loss:-0.144| Critic_Loss:0.265| Iter_duration:5.658| lr:[0.0001]\n",
      "iteration:  1000 \teval_rewards:  -50.0\n",
      "iteration:  1001 \teval_rewards:  0.0\n",
      "iteration:  1002 \teval_rewards:  -50.0\n",
      "iteration:  1003 \teval_rewards:  -50.0\n",
      "iteration:  1004 \teval_rewards:  -50.0\n",
      "iteration:  1005 \teval_rewards:  -50.0\n",
      "iteration:  1006 \teval_rewards:  -50.0\n",
      "iteration:  1007 \teval_rewards:  -50.0\n",
      "iteration:  1008 \teval_rewards:  -50.0\n",
      "iteration:  1009 \teval_rewards:  -50.0\n",
      "iteration:  1010 \teval_rewards:  -50.0\n",
      "iteration:  1011 \teval_rewards:  -50.0\n",
      "iteration:  1012 \teval_rewards:  -50.0\n",
      "iteration:  1013 \teval_rewards:  -50.0\n",
      "iteration:  1014 \teval_rewards:  -50.0\n",
      "iteration:  1015 \teval_rewards:  -50.0\n",
      "iteration:  1016 \teval_rewards:  -50.0\n",
      "iteration:  1017 \teval_rewards:  -50.0\n",
      "iteration:  1018 \teval_rewards:  -50.0\n",
      "iteration:  1019 \teval_rewards:  -50.0\n",
      "iteration:  1020 \teval_rewards:  -50.0\n",
      "iteration:  1021 \teval_rewards:  -50.0\n",
      "iteration:  1022 \teval_rewards:  -50.0\n",
      "iteration:  1023 \teval_rewards:  -50.0\n",
      "iteration:  1024 \teval_rewards:  -50.0\n",
      "iteration:  1025 \teval_rewards:  -50.0\n",
      "iteration:  1026 \teval_rewards:  -50.0\n",
      "iteration:  1027 \teval_rewards:  -50.0\n",
      "iteration:  1028 \teval_rewards:  -50.0\n",
      "iteration:  1029 \teval_rewards:  -50.0\n",
      "iteration:  1030 \teval_rewards:  -50.0\n",
      "iteration:  1031 \teval_rewards:  -50.0\n",
      "iteration:  1032 \teval_rewards:  -50.0\n",
      "iteration:  1033 \teval_rewards:  -50.0\n",
      "iteration:  1034 \teval_rewards:  -50.0\n",
      "iteration:  1035 \teval_rewards:  -50.0\n",
      "iteration:  1036 \teval_rewards:  -50.0\n",
      "iteration:  1037 \teval_rewards:  -50.0\n",
      "iteration:  1038 \teval_rewards:  -50.0\n",
      "iteration:  1039 \teval_rewards:  -50.0\n",
      "iteration:  1040 \teval_rewards:  -50.0\n",
      "iteration:  1041 \teval_rewards:  -50.0\n",
      "iteration:  1042 \teval_rewards:  -50.0\n",
      "iteration:  1043 \teval_rewards:  -50.0\n",
      "iteration:  1044 \teval_rewards:  -50.0\n",
      "iteration:  1045 \teval_rewards:  -50.0\n",
      "iteration:  1046 \teval_rewards:  -50.0\n",
      "iteration:  1047 \teval_rewards:  -50.0\n",
      "iteration:  1048 \teval_rewards:  -50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1049 \teval_rewards:  -50.0\n",
      "iteration:  1050 \teval_rewards:  -50.0\n",
      "iteration:  1051 \teval_rewards:  -50.0\n",
      "iteration:  1052 \teval_rewards:  -50.0\n",
      "iteration:  1053 \teval_rewards:  -50.0\n",
      "iteration:  1054 \teval_rewards:  0.0\n",
      "iteration:  1055 \teval_rewards:  -50.0\n",
      "iteration:  1056 \teval_rewards:  -50.0\n",
      "iteration:  1057 \teval_rewards:  0.0\n",
      "iteration:  1058 \teval_rewards:  -50.0\n",
      "iteration:  1059 \teval_rewards:  -50.0\n",
      "iteration:  1060 \teval_rewards:  -50.0\n",
      "iteration:  1061 \teval_rewards:  -50.0\n",
      "iteration:  1062 \teval_rewards:  -50.0\n",
      "iteration:  1063 \teval_rewards:  -50.0\n",
      "iteration:  1064 \teval_rewards:  -50.0\n",
      "iteration:  1065 \teval_rewards:  0.0\n",
      "iteration:  1066 \teval_rewards:  -50.0\n",
      "iteration:  1067 \teval_rewards:  -50.0\n",
      "iteration:  1068 \teval_rewards:  -50.0\n",
      "iteration:  1069 \teval_rewards:  -50.0\n",
      "iteration:  1070 \teval_rewards:  -50.0\n",
      "iteration:  1071 \teval_rewards:  -50.0\n",
      "iteration:  1072 \teval_rewards:  -50.0\n",
      "iteration:  1073 \teval_rewards:  -50.0\n",
      "iteration:  1074 \teval_rewards:  -50.0\n",
      "iteration:  1075 \teval_rewards:  -50.0\n",
      "iteration:  1076 \teval_rewards:  -50.0\n",
      "iteration:  1077 \teval_rewards:  -50.0\n",
      "iteration:  1078 \teval_rewards:  -50.0\n",
      "iteration:  1079 \teval_rewards:  -50.0\n",
      "iteration:  1080 \teval_rewards:  -50.0\n",
      "iteration:  1081 \teval_rewards:  -50.0\n",
      "iteration:  1082 \teval_rewards:  -50.0\n",
      "iteration:  1083 \teval_rewards:  -50.0\n",
      "iteration:  1084 \teval_rewards:  -50.0\n",
      "iteration:  1085 \teval_rewards:  -50.0\n",
      "iteration:  1086 \teval_rewards:  -50.0\n",
      "iteration:  1087 \teval_rewards:  -50.0\n",
      "iteration:  1088 \teval_rewards:  -50.0\n",
      "iteration:  1089 \teval_rewards:  -50.0\n",
      "iteration:  1090 \teval_rewards:  -50.0\n",
      "iteration:  1091 \teval_rewards:  -50.0\n",
      "iteration:  1092 \teval_rewards:  -50.0\n",
      "iteration:  1093 \teval_rewards:  -50.0\n",
      "iteration:  1094 \teval_rewards:  -50.0\n",
      "iteration:  1095 \teval_rewards:  -50.0\n",
      "iteration:  1096 \teval_rewards:  -50.0\n",
      "iteration:  1097 \teval_rewards:  -50.0\n",
      "iteration:  1098 \teval_rewards:  -50.0\n",
      "iteration:  1099 \teval_rewards:  -50.0\n",
      "Iter:1100| Ep_Reward:-50.000| Running_reward:-48.163| Actor_Loss:0.015| Critic_Loss:0.254| Iter_duration:5.807| lr:[8e-05]\n",
      "iteration:  1100 \teval_rewards:  -50.0\n",
      "iteration:  1101 \teval_rewards:  0.0\n",
      "iteration:  1102 \teval_rewards:  -50.0\n",
      "iteration:  1103 \teval_rewards:  -50.0\n",
      "iteration:  1104 \teval_rewards:  -50.0\n",
      "iteration:  1105 \teval_rewards:  -50.0\n",
      "iteration:  1106 \teval_rewards:  -50.0\n",
      "iteration:  1107 \teval_rewards:  -50.0\n",
      "iteration:  1108 \teval_rewards:  -50.0\n",
      "iteration:  1109 \teval_rewards:  -50.0\n",
      "iteration:  1110 \teval_rewards:  -50.0\n",
      "iteration:  1111 \teval_rewards:  -50.0\n",
      "iteration:  1112 \teval_rewards:  -50.0\n",
      "iteration:  1113 \teval_rewards:  -50.0\n",
      "iteration:  1114 \teval_rewards:  -50.0\n",
      "iteration:  1115 \teval_rewards:  -50.0\n",
      "iteration:  1116 \teval_rewards:  -50.0\n",
      "iteration:  1117 \teval_rewards:  -50.0\n",
      "iteration:  1118 \teval_rewards:  -50.0\n",
      "iteration:  1119 \teval_rewards:  -50.0\n",
      "iteration:  1120 \teval_rewards:  -50.0\n",
      "iteration:  1121 \teval_rewards:  -50.0\n",
      "iteration:  1122 \teval_rewards:  -50.0\n",
      "iteration:  1123 \teval_rewards:  -50.0\n",
      "iteration:  1124 \teval_rewards:  -50.0\n",
      "iteration:  1125 \teval_rewards:  -50.0\n",
      "iteration:  1126 \teval_rewards:  -50.0\n",
      "iteration:  1127 \teval_rewards:  -50.0\n",
      "iteration:  1128 \teval_rewards:  -50.0\n",
      "iteration:  1129 \teval_rewards:  -50.0\n",
      "iteration:  1130 \teval_rewards:  -50.0\n",
      "iteration:  1131 \teval_rewards:  -50.0\n",
      "iteration:  1132 \teval_rewards:  -50.0\n",
      "iteration:  1133 \teval_rewards:  -50.0\n",
      "iteration:  1134 \teval_rewards:  -50.0\n",
      "iteration:  1135 \teval_rewards:  -50.0\n",
      "iteration:  1136 \teval_rewards:  -50.0\n",
      "iteration:  1137 \teval_rewards:  0.0\n",
      "iteration:  1138 \teval_rewards:  -50.0\n",
      "iteration:  1139 \teval_rewards:  0.0\n",
      "iteration:  1140 \teval_rewards:  -50.0\n",
      "iteration:  1141 \teval_rewards:  -50.0\n",
      "iteration:  1142 \teval_rewards:  -50.0\n",
      "iteration:  1143 \teval_rewards:  -50.0\n",
      "iteration:  1144 \teval_rewards:  -50.0\n",
      "iteration:  1145 \teval_rewards:  -50.0\n",
      "iteration:  1146 \teval_rewards:  -50.0\n",
      "iteration:  1147 \teval_rewards:  -50.0\n",
      "iteration:  1148 \teval_rewards:  -50.0\n",
      "iteration:  1149 \teval_rewards:  -50.0\n",
      "iteration:  1150 \teval_rewards:  -50.0\n",
      "iteration:  1151 \teval_rewards:  -50.0\n",
      "iteration:  1152 \teval_rewards:  -50.0\n",
      "iteration:  1153 \teval_rewards:  -50.0\n",
      "iteration:  1154 \teval_rewards:  -50.0\n",
      "iteration:  1155 \teval_rewards:  -50.0\n",
      "iteration:  1156 \teval_rewards:  -50.0\n",
      "iteration:  1157 \teval_rewards:  -50.0\n",
      "iteration:  1158 \teval_rewards:  -50.0\n",
      "iteration:  1159 \teval_rewards:  -50.0\n",
      "iteration:  1160 \teval_rewards:  -50.0\n",
      "iteration:  1161 \teval_rewards:  -50.0\n",
      "iteration:  1162 \teval_rewards:  -50.0\n",
      "iteration:  1163 \teval_rewards:  -50.0\n",
      "iteration:  1164 \teval_rewards:  -50.0\n",
      "iteration:  1165 \teval_rewards:  -50.0\n",
      "iteration:  1166 \teval_rewards:  -50.0\n",
      "iteration:  1167 \teval_rewards:  -50.0\n",
      "iteration:  1168 \teval_rewards:  -50.0\n",
      "iteration:  1169 \teval_rewards:  -50.0\n",
      "iteration:  1170 \teval_rewards:  -50.0\n",
      "iteration:  1171 \teval_rewards:  -50.0\n",
      "iteration:  1172 \teval_rewards:  -50.0\n",
      "iteration:  1173 \teval_rewards:  -50.0\n",
      "iteration:  1174 \teval_rewards:  -50.0\n",
      "iteration:  1175 \teval_rewards:  -50.0\n",
      "iteration:  1176 \teval_rewards:  -50.0\n",
      "iteration:  1177 \teval_rewards:  -50.0\n",
      "iteration:  1178 \teval_rewards:  -50.0\n",
      "iteration:  1179 \teval_rewards:  -50.0\n",
      "iteration:  1180 \teval_rewards:  -50.0\n",
      "iteration:  1181 \teval_rewards:  -50.0\n",
      "iteration:  1182 \teval_rewards:  -50.0\n",
      "iteration:  1183 \teval_rewards:  -50.0\n",
      "iteration:  1184 \teval_rewards:  -50.0\n",
      "iteration:  1185 \teval_rewards:  -50.0\n",
      "iteration:  1186 \teval_rewards:  -50.0\n",
      "iteration:  1187 \teval_rewards:  -50.0\n",
      "iteration:  1188 \teval_rewards:  -50.0\n",
      "iteration:  1189 \teval_rewards:  -50.0\n",
      "iteration:  1190 \teval_rewards:  -50.0\n",
      "iteration:  1191 \teval_rewards:  -50.0\n",
      "iteration:  1192 \teval_rewards:  -50.0\n",
      "iteration:  1193 \teval_rewards:  -50.0\n",
      "iteration:  1194 \teval_rewards:  -50.0\n",
      "iteration:  1195 \teval_rewards:  -50.0\n",
      "iteration:  1196 \teval_rewards:  -50.0\n",
      "iteration:  1197 \teval_rewards:  -50.0\n",
      "iteration:  1198 \teval_rewards:  -50.0\n",
      "iteration:  1199 \teval_rewards:  0.0\n",
      "Iter:1200| Ep_Reward:-50.000| Running_reward:-48.111| Actor_Loss:-0.237| Critic_Loss:0.301| Iter_duration:5.585| lr:[5.999999999999998e-05]\n",
      "iteration:  1200 \teval_rewards:  -50.0\n",
      "iteration:  1201 \teval_rewards:  -50.0\n",
      "iteration:  1202 \teval_rewards:  -50.0\n",
      "iteration:  1203 \teval_rewards:  -50.0\n",
      "iteration:  1204 \teval_rewards:  -50.0\n",
      "iteration:  1205 \teval_rewards:  -50.0\n",
      "iteration:  1206 \teval_rewards:  -50.0\n",
      "iteration:  1207 \teval_rewards:  -50.0\n",
      "iteration:  1208 \teval_rewards:  -50.0\n",
      "iteration:  1209 \teval_rewards:  -50.0\n",
      "iteration:  1210 \teval_rewards:  -50.0\n",
      "iteration:  1211 \teval_rewards:  -50.0\n",
      "iteration:  1212 \teval_rewards:  -50.0\n",
      "iteration:  1213 \teval_rewards:  -50.0\n",
      "iteration:  1214 \teval_rewards:  -50.0\n",
      "iteration:  1215 \teval_rewards:  -50.0\n",
      "iteration:  1216 \teval_rewards:  -50.0\n",
      "iteration:  1217 \teval_rewards:  -50.0\n",
      "iteration:  1218 \teval_rewards:  -50.0\n",
      "iteration:  1219 \teval_rewards:  -50.0\n",
      "iteration:  1220 \teval_rewards:  -50.0\n",
      "iteration:  1221 \teval_rewards:  -50.0\n",
      "iteration:  1222 \teval_rewards:  -50.0\n",
      "iteration:  1223 \teval_rewards:  -50.0\n",
      "iteration:  1224 \teval_rewards:  -50.0\n",
      "iteration:  1225 \teval_rewards:  -50.0\n",
      "iteration:  1226 \teval_rewards:  -50.0\n",
      "iteration:  1227 \teval_rewards:  -50.0\n",
      "iteration:  1228 \teval_rewards:  -50.0\n",
      "iteration:  1229 \teval_rewards:  -50.0\n",
      "iteration:  1230 \teval_rewards:  -50.0\n",
      "iteration:  1231 \teval_rewards:  -50.0\n",
      "iteration:  1232 \teval_rewards:  -50.0\n",
      "iteration:  1233 \teval_rewards:  -50.0\n",
      "iteration:  1234 \teval_rewards:  -50.0\n",
      "iteration:  1235 \teval_rewards:  -50.0\n",
      "iteration:  1236 \teval_rewards:  -50.0\n",
      "iteration:  1237 \teval_rewards:  -50.0\n",
      "iteration:  1238 \teval_rewards:  -50.0\n",
      "iteration:  1239 \teval_rewards:  -50.0\n",
      "iteration:  1240 \teval_rewards:  -50.0\n",
      "iteration:  1241 \teval_rewards:  -50.0\n",
      "iteration:  1242 \teval_rewards:  -50.0\n",
      "iteration:  1243 \teval_rewards:  -50.0\n",
      "iteration:  1244 \teval_rewards:  -50.0\n",
      "iteration:  1245 \teval_rewards:  -50.0\n",
      "iteration:  1246 \teval_rewards:  -50.0\n",
      "iteration:  1247 \teval_rewards:  0.0\n",
      "iteration:  1248 \teval_rewards:  -50.0\n",
      "iteration:  1249 \teval_rewards:  -50.0\n",
      "iteration:  1250 \teval_rewards:  -50.0\n",
      "iteration:  1251 \teval_rewards:  -50.0\n",
      "iteration:  1252 \teval_rewards:  -50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1253 \teval_rewards:  -50.0\n",
      "iteration:  1254 \teval_rewards:  -50.0\n",
      "iteration:  1255 \teval_rewards:  -50.0\n",
      "iteration:  1256 \teval_rewards:  -50.0\n",
      "iteration:  1257 \teval_rewards:  -50.0\n",
      "iteration:  1258 \teval_rewards:  -50.0\n",
      "iteration:  1259 \teval_rewards:  -50.0\n",
      "iteration:  1260 \teval_rewards:  -50.0\n",
      "iteration:  1261 \teval_rewards:  -50.0\n",
      "iteration:  1262 \teval_rewards:  -50.0\n",
      "iteration:  1263 \teval_rewards:  -50.0\n",
      "iteration:  1264 \teval_rewards:  -50.0\n",
      "iteration:  1265 \teval_rewards:  -50.0\n",
      "iteration:  1266 \teval_rewards:  -50.0\n",
      "iteration:  1267 \teval_rewards:  -50.0\n",
      "iteration:  1268 \teval_rewards:  -50.0\n",
      "iteration:  1269 \teval_rewards:  -50.0\n",
      "iteration:  1270 \teval_rewards:  -50.0\n",
      "iteration:  1271 \teval_rewards:  -50.0\n",
      "iteration:  1272 \teval_rewards:  -50.0\n",
      "iteration:  1273 \teval_rewards:  -50.0\n",
      "iteration:  1274 \teval_rewards:  -50.0\n",
      "iteration:  1275 \teval_rewards:  -50.0\n",
      "iteration:  1276 \teval_rewards:  -50.0\n",
      "iteration:  1277 \teval_rewards:  -50.0\n",
      "iteration:  1278 \teval_rewards:  -50.0\n",
      "iteration:  1279 \teval_rewards:  -50.0\n",
      "iteration:  1280 \teval_rewards:  -50.0\n",
      "iteration:  1281 \teval_rewards:  -50.0\n",
      "iteration:  1282 \teval_rewards:  -50.0\n",
      "iteration:  1283 \teval_rewards:  0.0\n",
      "iteration:  1284 \teval_rewards:  -50.0\n",
      "iteration:  1285 \teval_rewards:  -50.0\n",
      "iteration:  1286 \teval_rewards:  -50.0\n",
      "iteration:  1287 \teval_rewards:  -50.0\n",
      "iteration:  1288 \teval_rewards:  -50.0\n",
      "iteration:  1289 \teval_rewards:  -50.0\n",
      "iteration:  1290 \teval_rewards:  -50.0\n",
      "iteration:  1291 \teval_rewards:  -50.0\n",
      "iteration:  1292 \teval_rewards:  -50.0\n",
      "iteration:  1293 \teval_rewards:  -50.0\n",
      "iteration:  1294 \teval_rewards:  -50.0\n",
      "iteration:  1295 \teval_rewards:  -50.0\n",
      "iteration:  1296 \teval_rewards:  -50.0\n",
      "iteration:  1297 \teval_rewards:  -50.0\n",
      "iteration:  1298 \teval_rewards:  -50.0\n",
      "iteration:  1299 \teval_rewards:  -50.0\n",
      "Iter:1300| Ep_Reward:-50.000| Running_reward:-48.594| Actor_Loss:0.014| Critic_Loss:0.242| Iter_duration:5.462| lr:[3.999999999999999e-05]\n",
      "iteration:  1300 \teval_rewards:  -50.0\n",
      "iteration:  1301 \teval_rewards:  -50.0\n",
      "iteration:  1302 \teval_rewards:  -50.0\n",
      "iteration:  1303 \teval_rewards:  -50.0\n",
      "iteration:  1304 \teval_rewards:  -50.0\n",
      "iteration:  1305 \teval_rewards:  -50.0\n",
      "iteration:  1306 \teval_rewards:  -50.0\n",
      "iteration:  1307 \teval_rewards:  -50.0\n",
      "iteration:  1308 \teval_rewards:  -50.0\n",
      "iteration:  1309 \teval_rewards:  -50.0\n",
      "iteration:  1310 \teval_rewards:  -50.0\n",
      "iteration:  1311 \teval_rewards:  -50.0\n",
      "iteration:  1312 \teval_rewards:  -50.0\n",
      "iteration:  1313 \teval_rewards:  -50.0\n",
      "iteration:  1314 \teval_rewards:  -50.0\n",
      "iteration:  1315 \teval_rewards:  -50.0\n",
      "iteration:  1316 \teval_rewards:  -50.0\n",
      "iteration:  1317 \teval_rewards:  -50.0\n",
      "iteration:  1318 \teval_rewards:  -50.0\n",
      "iteration:  1319 \teval_rewards:  -50.0\n",
      "iteration:  1320 \teval_rewards:  -50.0\n",
      "iteration:  1321 \teval_rewards:  -50.0\n",
      "iteration:  1322 \teval_rewards:  -50.0\n",
      "iteration:  1323 \teval_rewards:  -50.0\n",
      "iteration:  1324 \teval_rewards:  -50.0\n",
      "iteration:  1325 \teval_rewards:  -50.0\n",
      "iteration:  1326 \teval_rewards:  -50.0\n",
      "iteration:  1327 \teval_rewards:  -50.0\n",
      "iteration:  1328 \teval_rewards:  -50.0\n",
      "iteration:  1329 \teval_rewards:  -50.0\n",
      "iteration:  1330 \teval_rewards:  -50.0\n",
      "iteration:  1331 \teval_rewards:  -50.0\n",
      "iteration:  1332 \teval_rewards:  -50.0\n",
      "iteration:  1333 \teval_rewards:  -50.0\n",
      "iteration:  1334 \teval_rewards:  0.0\n",
      "iteration:  1335 \teval_rewards:  -50.0\n",
      "iteration:  1336 \teval_rewards:  -50.0\n",
      "iteration:  1337 \teval_rewards:  -50.0\n",
      "iteration:  1338 \teval_rewards:  -50.0\n",
      "iteration:  1339 \teval_rewards:  -50.0\n",
      "iteration:  1340 \teval_rewards:  -50.0\n",
      "iteration:  1341 \teval_rewards:  -50.0\n",
      "iteration:  1342 \teval_rewards:  0.0\n",
      "iteration:  1343 \teval_rewards:  -50.0\n",
      "iteration:  1344 \teval_rewards:  -50.0\n",
      "iteration:  1345 \teval_rewards:  -50.0\n",
      "iteration:  1346 \teval_rewards:  -50.0\n",
      "iteration:  1347 \teval_rewards:  -50.0\n",
      "iteration:  1348 \teval_rewards:  -50.0\n",
      "iteration:  1349 \teval_rewards:  -50.0\n",
      "iteration:  1350 \teval_rewards:  -50.0\n",
      "iteration:  1351 \teval_rewards:  -50.0\n",
      "iteration:  1352 \teval_rewards:  -50.0\n",
      "iteration:  1353 \teval_rewards:  -50.0\n",
      "iteration:  1354 \teval_rewards:  -50.0\n",
      "iteration:  1355 \teval_rewards:  -50.0\n",
      "iteration:  1356 \teval_rewards:  -50.0\n",
      "iteration:  1357 \teval_rewards:  -50.0\n",
      "iteration:  1358 \teval_rewards:  -50.0\n",
      "iteration:  1359 \teval_rewards:  -50.0\n",
      "iteration:  1360 \teval_rewards:  -50.0\n",
      "iteration:  1361 \teval_rewards:  -50.0\n",
      "iteration:  1362 \teval_rewards:  -50.0\n",
      "iteration:  1363 \teval_rewards:  -50.0\n",
      "iteration:  1364 \teval_rewards:  -50.0\n",
      "iteration:  1365 \teval_rewards:  -50.0\n",
      "iteration:  1366 \teval_rewards:  -50.0\n",
      "iteration:  1367 \teval_rewards:  -50.0\n",
      "iteration:  1368 \teval_rewards:  -50.0\n",
      "iteration:  1369 \teval_rewards:  -50.0\n",
      "iteration:  1370 \teval_rewards:  -50.0\n",
      "iteration:  1371 \teval_rewards:  -50.0\n",
      "iteration:  1372 \teval_rewards:  -50.0\n",
      "iteration:  1373 \teval_rewards:  -50.0\n",
      "iteration:  1374 \teval_rewards:  -50.0\n",
      "iteration:  1375 \teval_rewards:  -50.0\n",
      "iteration:  1376 \teval_rewards:  -50.0\n",
      "iteration:  1377 \teval_rewards:  -50.0\n",
      "iteration:  1378 \teval_rewards:  -50.0\n",
      "iteration:  1379 \teval_rewards:  -50.0\n",
      "iteration:  1380 \teval_rewards:  -50.0\n",
      "iteration:  1381 \teval_rewards:  -50.0\n",
      "iteration:  1382 \teval_rewards:  -50.0\n",
      "iteration:  1383 \teval_rewards:  -50.0\n",
      "iteration:  1384 \teval_rewards:  -50.0\n",
      "iteration:  1385 \teval_rewards:  0.0\n",
      "iteration:  1386 \teval_rewards:  -50.0\n",
      "iteration:  1387 \teval_rewards:  -50.0\n",
      "iteration:  1388 \teval_rewards:  -50.0\n",
      "iteration:  1389 \teval_rewards:  -50.0\n",
      "iteration:  1390 \teval_rewards:  -50.0\n",
      "iteration:  1391 \teval_rewards:  -50.0\n",
      "iteration:  1392 \teval_rewards:  -50.0\n",
      "iteration:  1393 \teval_rewards:  0.0\n",
      "iteration:  1394 \teval_rewards:  -50.0\n",
      "iteration:  1395 \teval_rewards:  -50.0\n",
      "iteration:  1396 \teval_rewards:  -50.0\n",
      "iteration:  1397 \teval_rewards:  -50.0\n",
      "iteration:  1398 \teval_rewards:  -50.0\n",
      "iteration:  1399 \teval_rewards:  -50.0\n",
      "Iter:1400| Ep_Reward:-50.000| Running_reward:-48.052| Actor_Loss:-0.252| Critic_Loss:0.157| Iter_duration:5.549| lr:[1.9999999999999995e-05]\n",
      "iteration:  1400 \teval_rewards:  -50.0\n",
      "iteration:  1401 \teval_rewards:  -50.0\n",
      "iteration:  1402 \teval_rewards:  -50.0\n",
      "iteration:  1403 \teval_rewards:  -50.0\n",
      "iteration:  1404 \teval_rewards:  -50.0\n",
      "iteration:  1405 \teval_rewards:  -50.0\n",
      "iteration:  1406 \teval_rewards:  -50.0\n",
      "iteration:  1407 \teval_rewards:  -50.0\n",
      "iteration:  1408 \teval_rewards:  -50.0\n",
      "iteration:  1409 \teval_rewards:  -50.0\n",
      "iteration:  1410 \teval_rewards:  -50.0\n",
      "iteration:  1411 \teval_rewards:  -50.0\n",
      "iteration:  1412 \teval_rewards:  -50.0\n",
      "iteration:  1413 \teval_rewards:  -50.0\n",
      "iteration:  1414 \teval_rewards:  -50.0\n",
      "iteration:  1415 \teval_rewards:  -50.0\n",
      "iteration:  1416 \teval_rewards:  -50.0\n",
      "iteration:  1417 \teval_rewards:  -50.0\n",
      "iteration:  1418 \teval_rewards:  -50.0\n",
      "iteration:  1419 \teval_rewards:  -50.0\n",
      "iteration:  1420 \teval_rewards:  -50.0\n",
      "iteration:  1421 \teval_rewards:  -50.0\n",
      "iteration:  1422 \teval_rewards:  -50.0\n",
      "iteration:  1423 \teval_rewards:  -50.0\n",
      "iteration:  1424 \teval_rewards:  -50.0\n",
      "iteration:  1425 \teval_rewards:  -50.0\n",
      "iteration:  1426 \teval_rewards:  -50.0\n",
      "iteration:  1427 \teval_rewards:  0.0\n",
      "iteration:  1428 \teval_rewards:  -50.0\n",
      "iteration:  1429 \teval_rewards:  -50.0\n",
      "iteration:  1430 \teval_rewards:  -50.0\n",
      "iteration:  1431 \teval_rewards:  -50.0\n",
      "iteration:  1432 \teval_rewards:  -50.0\n",
      "iteration:  1433 \teval_rewards:  -50.0\n",
      "iteration:  1434 \teval_rewards:  -50.0\n",
      "iteration:  1435 \teval_rewards:  -50.0\n",
      "iteration:  1436 \teval_rewards:  -50.0\n",
      "iteration:  1437 \teval_rewards:  -50.0\n",
      "iteration:  1438 \teval_rewards:  -50.0\n",
      "iteration:  1439 \teval_rewards:  -50.0\n",
      "iteration:  1440 \teval_rewards:  -50.0\n",
      "iteration:  1441 \teval_rewards:  -50.0\n",
      "iteration:  1442 \teval_rewards:  -50.0\n",
      "iteration:  1443 \teval_rewards:  -50.0\n",
      "iteration:  1444 \teval_rewards:  -50.0\n",
      "iteration:  1445 \teval_rewards:  -50.0\n",
      "iteration:  1446 \teval_rewards:  -50.0\n",
      "iteration:  1447 \teval_rewards:  -50.0\n",
      "iteration:  1448 \teval_rewards:  -50.0\n",
      "iteration:  1449 \teval_rewards:  -50.0\n",
      "iteration:  1450 \teval_rewards:  -50.0\n",
      "iteration:  1451 \teval_rewards:  -50.0\n",
      "iteration:  1452 \teval_rewards:  -50.0\n",
      "iteration:  1453 \teval_rewards:  -50.0\n",
      "iteration:  1454 \teval_rewards:  -50.0\n",
      "iteration:  1455 \teval_rewards:  -50.0\n",
      "iteration:  1456 \teval_rewards:  -50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1457 \teval_rewards:  -50.0\n",
      "iteration:  1458 \teval_rewards:  -50.0\n",
      "iteration:  1459 \teval_rewards:  -50.0\n",
      "iteration:  1460 \teval_rewards:  -50.0\n",
      "iteration:  1461 \teval_rewards:  -50.0\n",
      "iteration:  1462 \teval_rewards:  -50.0\n",
      "iteration:  1463 \teval_rewards:  -50.0\n",
      "iteration:  1464 \teval_rewards:  -50.0\n",
      "iteration:  1465 \teval_rewards:  -50.0\n",
      "iteration:  1466 \teval_rewards:  -50.0\n",
      "iteration:  1467 \teval_rewards:  -50.0\n",
      "iteration:  1468 \teval_rewards:  -50.0\n",
      "iteration:  1469 \teval_rewards:  -50.0\n",
      "iteration:  1470 \teval_rewards:  -50.0\n",
      "iteration:  1471 \teval_rewards:  -50.0\n",
      "iteration:  1472 \teval_rewards:  -50.0\n",
      "iteration:  1473 \teval_rewards:  -50.0\n",
      "iteration:  1474 \teval_rewards:  0.0\n",
      "iteration:  1475 \teval_rewards:  -50.0\n",
      "iteration:  1476 \teval_rewards:  -50.0\n",
      "iteration:  1477 \teval_rewards:  -50.0\n",
      "iteration:  1478 \teval_rewards:  -50.0\n",
      "iteration:  1479 \teval_rewards:  -50.0\n",
      "iteration:  1480 \teval_rewards:  -50.0\n",
      "iteration:  1481 \teval_rewards:  -50.0\n",
      "iteration:  1482 \teval_rewards:  -50.0\n",
      "iteration:  1483 \teval_rewards:  -50.0\n",
      "iteration:  1484 \teval_rewards:  -50.0\n",
      "iteration:  1485 \teval_rewards:  -50.0\n",
      "iteration:  1486 \teval_rewards:  -50.0\n",
      "iteration:  1487 \teval_rewards:  -50.0\n",
      "iteration:  1488 \teval_rewards:  -50.0\n",
      "iteration:  1489 \teval_rewards:  -50.0\n",
      "iteration:  1490 \teval_rewards:  -50.0\n",
      "iteration:  1491 \teval_rewards:  -50.0\n",
      "iteration:  1492 \teval_rewards:  -50.0\n",
      "iteration:  1493 \teval_rewards:  -50.0\n",
      "iteration:  1494 \teval_rewards:  -50.0\n",
      "iteration:  1495 \teval_rewards:  -50.0\n",
      "iteration:  1496 \teval_rewards:  -50.0\n",
      "iteration:  1497 \teval_rewards:  -50.0\n",
      "iteration:  1498 \teval_rewards:  -50.0\n",
      "iteration:  1499 \teval_rewards:  -50.0\n",
      "Iter:1500| Ep_Reward:-50.000| Running_reward:-48.662| Actor_Loss:0.042| Critic_Loss:0.096| Iter_duration:5.436| lr:[0.0]\n",
      "iteration:  1500 \teval_rewards:  -50.0\n",
      "episode reward:-50.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of states:{n_states}\\n\"\n",
    "      f\"action bounds:{action_bounds}\\n\"\n",
    "      f\"number of actions:{n_actions}\")\n",
    "\n",
    "if not os.path.exists(ENV_NAME):\n",
    "    os.mkdir(ENV_NAME)\n",
    "    os.mkdir(ENV_NAME + \"/logs\")\n",
    "\n",
    "env = gym.make(ENV_NAME + \"-v1\")\n",
    "\n",
    "agent = Agent(n_states=n_states + n_goals,\n",
    "              n_iter=n_iterations,\n",
    "              env_name=ENV_NAME,\n",
    "              action_bounds=action_bounds,\n",
    "              n_actions=n_actions,\n",
    "              lr=lr)\n",
    "if TRAIN_FLAG:\n",
    "    trainer = Train(env=env,\n",
    "                    test_env=test_env,\n",
    "                    env_name=ENV_NAME,\n",
    "                    agent=agent,\n",
    "                    horizon=T,\n",
    "                    n_iterations=n_iterations,\n",
    "                    epochs=epochs,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    epsilon=clip_range)\n",
    "    trainer.step()\n",
    "\n",
    "player = Play(env, agent, ENV_NAME)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As from the result, the output looks not so good. This may from the normalization technic or loss function and the way of the PPO may not good enough for the FetchPickAndPlace.\n",
    "\n",
    "Let's see another algorithm **DDPG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "DDPG is purposed by Deepmind in the name \"Continuous Control With Deep Reinforcement Learning\".\n",
    "\n",
    "### Network Schematics\n",
    "DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.\n",
    "\n",
    "- $\\theta^Q$: Q network\n",
    "- $\\theta^{\\mu}$: Deterministic policy function\n",
    "- $\\theta^{Q'}$: target Q network\n",
    "- $\\theta^{\\mu'}$: target policy network\n",
    "\n",
    "The Q network and policy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space\n",
    "\n",
    "The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improve stability in learning. Here’s why: In methods that do not use target networks, the update equations of the network are interdependent on the values calculated by the network itself, which makes it prone to divergence.\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[R(s,a) + \\gamma \\max W(s',a') - Q(s,a)]$$\n",
    "\n",
    "Note that $Q(s',a')$ depends Q function itself (at the moment it is being optimized)\n",
    "\n",
    "So, we can use the standard Actor Critic architecture for the deterministic policy network and the Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def init_weights_biases(size):\n",
    "    v = 1.0 / np.sqrt(size[0])\n",
    "    return torch.FloatTensor(size).uniform_(-v, v)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_goals, n_hidden1=256, n_hidden2=256, n_hidden3=256, initial_w=3e-3):\n",
    "        self.n_states = n_states[0]\n",
    "        self.n_actions = n_actions\n",
    "        self.n_goals = n_goals\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.initial_w = initial_w\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states + self.n_goals, out_features=self.n_hidden1)\n",
    "        self.fc2 = nn.Linear(in_features=self.n_hidden1, out_features=self.n_hidden2)\n",
    "        self.fc3 = nn.Linear(in_features=self.n_hidden2, out_features=self.n_hidden3)\n",
    "        self.output = nn.Linear(in_features=self.n_hidden3, out_features=self.n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        output = torch.tanh(self.output(x))  # TODO add scale of the action\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_states, n_goals, n_hidden1=256, n_hidden2=256, n_hidden3=256, initial_w=3e-3, action_size=1):\n",
    "        self.n_states = n_states[0]\n",
    "        self.n_goals = n_goals\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.initial_w = initial_w\n",
    "        self.action_size = action_size\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.n_states + self.n_goals + self.action_size, out_features=self.n_hidden1)\n",
    "        self.fc2 = nn.Linear(in_features=self.n_hidden1, out_features=self.n_hidden2)\n",
    "        self.fc3 = nn.Linear(in_features=self.n_hidden2, out_features=self.n_hidden3)\n",
    "        self.output = nn.Linear(in_features=self.n_hidden3, out_features=1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = F.relu(self.fc1(torch.cat([x, a], dim=-1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        output = self.output(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO vs DDPG\n",
    "\n",
    "\n",
    "PPO actor-critic objective functions are based on a set of trajectories obtained by running the current policy over T timesteps. After the policy is updated, trajectories generated from old/stale policies are no longer applicable. i.e. it needs to be trained \"on-policy\".\n",
    "\n",
    "(Why? Because PPO uses a stochastic policy (i.e. a conditional probability distribution of actions given states) and the policy's objective function is based on sampling from trajectories from a probability distribution that depends the current policy's probability distribution (i.e. you need to use the current policy to generate the trajectories). NOTE that this is true for any policy gradients approach using a stochastic policy, not just PPO.)\n",
    "\n",
    "DDPG/TD3 only needs a single timestep for each actor / critic update (via Bellman equation) and it is straightforward to apply the current deterministic policy to old data tuples $(s_t, a_t, r_t, s_{t+1})$. i.e. it is trained \"off-policy\".\n",
    "\n",
    "(WHY? Because DDPG/TD3 use a deterministic policy and Silver, David, et al. \"Deterministic policy gradient algorithms.\" 2014. proved that the policy's objective function is an expectation value of state trajectories from the Markov Decision Process state transition function...but does not depend on the probability distribution induced by the policy, which after all is deterministic not stochastic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting PPO or DDPG\n",
    "\n",
    "If the environment is expensive to sample from, use DDPG or SAC, since they're more sample efficient. If it's cheap to sample from, using PPO or a REINFORCE-based algorithm, since they're straightforward to implement, robust to hyperparameters, and easy to get working. You'll spend less wall-clock time training a PPO-like algorithm in a cheap environment.\n",
    "\n",
    "If you need to decide between DDPG and SAC, choose TD3. The performance of SAC and DDPG is nearly identical when you compare on the basis of whether or not a twin delayed update is used. SAC can be troublesome to get working, and the temperature parameter controls the stochasticity of your final policy -- effectively, it means your reward scheme can give you a policy that is too random to be useful, and picking a temperature parameter isn't necessarily straightforward. TD3 is almost the same as SAC, but noise injection is often easier to visualize and tune than setting the right temperature parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning DDPG\n",
    "\n",
    "Here is the pseudo code\n",
    "\n",
    "<img src=\"img/ddpg_pseudo.png\" title=\"The A2C architecture\" style=\"width: 600px;\" />\n",
    "\n",
    "The important things of DDPG are:\n",
    "1. Experience replay\n",
    "2. Actor & Critic network updates\n",
    "3. Target network updates\n",
    "4. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters. During each trajectory roll-out, we save all the experience tuples (state, action, reward, next_state) and store them in a finite-sized cache — a “replay buffer.” Then, we sample random mini-batches of experience from the replay buffer when we update the value and policy networks.\n",
    "\n",
    "In optimization tasks, we want the data to be independently distributed. This fails to be the case when we optimize a sequential decision process in an on-policy way, because the data then would not be independent of each other. When we store them in a replay buffer and take random batches for training, we overcome this issue.\n",
    "\n",
    "### Hindsight Experience Replay (HER)\n",
    "\n",
    "Reference: https://arxiv.org/abs/1707.01495\n",
    "\n",
    "One ability humans have is to learn from our mistakes and adjust next time to avoid making the same mistake. We can apply the same concept to our reinforcement learning algorithm.\n",
    "\n",
    "Now this time, instead of concluding that the course of action you took was useless because you didn’t score a goal, what if you say that maybe it didn’t teach you how to score a goal, but it certainly taught you how NOT to shoot the puck. Or more precisely, what if you say it taught you how to shoot the puck slightly to the right side of the net? Now you can learn not only how to shoot towards the right, but you learn something new that might help you achieve the final goal every run. This is the idea behind Hindsight Experience Replay (HER).\n",
    "\n",
    "The HER procedure looks like this:\n",
    "1. Run your policy and store everything you observe (state and goal, action, reward, next state and goal) tuple into an experience buffer.\n",
    "2. Sample K goals from the states visited in the episode obtained at step 1, and for each goal store (state and sampled goal, action, reward with respect to the sampled goal, next state and sampled goal) tuple into the buffer.\n",
    "3. Repeat step 1–2 N times.\n",
    "4. Sample M number of experience tuples (batch) from the buffer and train the network with the said batch experience. (Do this B times)\n",
    "\n",
    "How does this help? By adding goal into the input space we are stating that there are multiple goals for our agent to observe. The new Q-function indicates how good taking each action is, given the current state, to achieving the current goal. And because we are sampling the new goals from the episode, the goals are now nodes that have been visited by our agent. So even if our agent fails to achieve the main goal in this episode, it has reached some states. By using those states as the new goal, now the agent can be trained with positive (or non negative) rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy as dc\n",
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, capacity, k_future, env):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.memory_counter = 0\n",
    "        self.memory_length = 0\n",
    "        self.env = env\n",
    "\n",
    "        self.future_p = 1 - (1. / (1 + k_future))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        ep_indices = np.random.randint(0, len(self.memory), batch_size)\n",
    "        time_indices = np.random.randint(0, len(self.memory[0][\"next_state\"]), batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        desired_goals = []\n",
    "        next_states = []\n",
    "        next_achieved_goals = []\n",
    "\n",
    "        for episode, timestep in zip(ep_indices, time_indices):\n",
    "            states.append(dc(self.memory[episode][\"state\"][timestep]))\n",
    "            actions.append(dc(self.memory[episode][\"action\"][timestep]))\n",
    "            desired_goals.append(dc(self.memory[episode][\"desired_goal\"][timestep]))\n",
    "            next_achieved_goals.append(dc(self.memory[episode][\"next_achieved_goal\"][timestep]))\n",
    "            next_states.append(dc(self.memory[episode][\"next_state\"][timestep]))\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        actions = np.vstack(actions)\n",
    "        desired_goals = np.vstack(desired_goals)\n",
    "        next_achieved_goals = np.vstack(next_achieved_goals)\n",
    "        next_states = np.vstack(next_states)\n",
    "\n",
    "        her_indices = np.where(np.random.uniform(size=batch_size) < self.future_p)\n",
    "        future_offset = np.random.uniform(size=batch_size) * (len(self.memory[0][\"next_state\"]) - time_indices)\n",
    "        future_offset = future_offset.astype(int)\n",
    "        future_t = (time_indices + 1 + future_offset)[her_indices]\n",
    "\n",
    "        future_ag = []\n",
    "        for episode, f_offset in zip(ep_indices[her_indices], future_t):\n",
    "            future_ag.append(dc(self.memory[episode][\"achieved_goal\"][f_offset]))\n",
    "        future_ag = np.vstack(future_ag)\n",
    "\n",
    "        desired_goals[her_indices] = future_ag\n",
    "        rewards = np.expand_dims(self.env.compute_reward(next_achieved_goals, desired_goals, None), 1)\n",
    "\n",
    "        return self.clip_obs(states), actions, rewards, self.clip_obs(next_states), self.clip_obs(desired_goals)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            self.memory.pop(0)\n",
    "        assert len(self.memory) <= self.capacity\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def clip_obs(x):\n",
    "        return np.clip(x, -200, 200)\n",
    "\n",
    "    def sample_for_normalization(self, batch):\n",
    "        size = len(batch[0][\"next_state\"])\n",
    "        ep_indices = np.random.randint(0, len(batch), size)\n",
    "        time_indices = np.random.randint(0, len(batch[0][\"next_state\"]), size)\n",
    "        states = []\n",
    "        desired_goals = []\n",
    "\n",
    "        for episode, timestep in zip(ep_indices, time_indices):\n",
    "            states.append(dc(batch[episode][\"state\"][timestep]))\n",
    "            desired_goals.append(dc(batch[episode][\"desired_goal\"][timestep]))\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        desired_goals = np.vstack(desired_goals)\n",
    "\n",
    "        her_indices = np.where(np.random.uniform(size=size) < self.future_p)\n",
    "        future_offset = np.random.uniform(size=size) * (len(batch[0][\"next_state\"]) - time_indices)\n",
    "        future_offset = future_offset.astype(int)\n",
    "        future_t = (time_indices + 1 + future_offset)[her_indices]\n",
    "\n",
    "        future_ag = []\n",
    "        for episode, f_offset in zip(ep_indices[her_indices], future_t):\n",
    "            future_ag.append(dc(batch[episode][\"achieved_goal\"][f_offset]))\n",
    "        future_ag = np.vstack(future_ag)\n",
    "\n",
    "        desired_goals[her_indices] = future_ag\n",
    "\n",
    "        return self.clip_obs(states), self.clip_obs(desired_goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor (Policy) & Critic (Value) Network Updates\n",
    "The value network is updated similarly as is done in Q-learning. The updated Q value is obtained by the Bellman equation:\n",
    "\n",
    "$$y_i=r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'})$$\n",
    "\n",
    "However, in DDPG, the next-state Q values are calculated with the target value network and target policy network. Then, we minimize the mean-squared loss between the updated Q value and the original Q value:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum(y_i-Q(s_i,a_i|\\theta^Q))^2$$\n",
    "\n",
    "Note that the original Q value is calculated with the value network, not the target value network.\n",
    "\n",
    "For the policy function, our objective is to maximize the expected return:\n",
    "\n",
    "$$J(\\theta)=\\mathbb{E}[Q(s,a)|_{s=s_t, a_t=\\mu(s_t)}]$$\n",
    "\n",
    "To calculate the policy loss, we take the derivative of the objective function with respect to the policy parameter. Keep in mind that the actor (policy) function is differentiable, so we have to apply the chain rule.\n",
    "\n",
    "$$\\nabla_{\\theta^\\mu}J(\\theta) \\approx \\nabla_a Q(s,a) \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)$$\n",
    "\n",
    "But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:\n",
    "\n",
    "$$\\nabla_{\\theta^\\mu}J(\\theta) \\approx \\frac{1}{N}\\sum[\\nabla_a Q(s,a|\\theta^Q)|_{s=s_i,a=\\mu(s_i)} \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)|_{s=s_i}]$$\n",
    "\n",
    "### Target Network Updates\n",
    "We make a copy of the target network parameters and have them slowly track those of the learned networks via “soft updates,” as illustrated below:\n",
    "\n",
    "$$\\theta^{Q'} \\leftarrow \\tau \\theta^Q + (1-\\tau)\\theta^{Q'}$$\n",
    "\n",
    "$$\\theta^{\\mu'} \\leftarrow \\tau \\theta^\\mu + (1-\\tau)\\theta^{\\mu'}$$\n",
    "\n",
    "Where $\\tau < 1$\n",
    "\n",
    "### Exploration\n",
    "In Reinforcement learning for discrete action spaces, exploration is done via probabilistically selecting a random action (such as epsilon-greedy or Boltzmann exploration). For continuous action spaces, exploration is done via adding noise to the action itself (there is also the parameter space noise but we will skip that for now). In the DDPG paper, the authors use Ornstein-Uhlenbeck Process to add noise to the action output:\n",
    "\n",
    "$$\\mu'(s_t)=\\mu(s_t|\\theta_t^\\mu)+\\mathcal{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <code>mpi4py</code> can be installed from\n",
    "\n",
    "<code>pip install mpi4py</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make normalizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, size, eps=1e-2, default_clip_range=np.inf):\n",
    "        self.size = size\n",
    "        self.eps = eps\n",
    "        self.default_clip_range = default_clip_range\n",
    "        # some local information\n",
    "        self.local_sum = np.zeros(self.size, np.float32)\n",
    "        self.local_sumsq = np.zeros(self.size, np.float32)\n",
    "        self.local_count = np.zeros(1, np.float32)\n",
    "        # get the total sum sumsq and sum count\n",
    "        self.total_sum = np.zeros(self.size, np.float32)\n",
    "        self.total_sumsq = np.zeros(self.size, np.float32)\n",
    "        self.total_count = np.ones(1, np.float32)\n",
    "        # get the mean and std\n",
    "        self.mean = np.zeros(self.size, np.float32)\n",
    "        self.std = np.ones(self.size, np.float32)\n",
    "        # thread locker\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # update the parameters of the normalizer\n",
    "    def update(self, v):\n",
    "        v = v.reshape(-1, self.size)\n",
    "        # do the computing\n",
    "        with self.lock:\n",
    "            self.local_sum += v.sum(axis=0)\n",
    "            self.local_sumsq += (np.square(v)).sum(axis=0)\n",
    "            self.local_count[0] += v.shape[0]\n",
    "\n",
    "    # sync the parameters across the cpus\n",
    "    def sync(self, local_sum, local_sumsq, local_count):\n",
    "        local_sum[...] = self._mpi_average(local_sum)\n",
    "        local_sumsq[...] = self._mpi_average(local_sumsq)\n",
    "        local_count[...] = self._mpi_average(local_count)\n",
    "        return local_sum, local_sumsq, local_count\n",
    "\n",
    "    def recompute_stats(self):\n",
    "        with self.lock:\n",
    "            local_count = self.local_count.copy()\n",
    "            local_sum = self.local_sum.copy()\n",
    "            local_sumsq = self.local_sumsq.copy()\n",
    "            # reset\n",
    "            self.local_count[...] = 0\n",
    "            self.local_sum[...] = 0\n",
    "            self.local_sumsq[...] = 0\n",
    "        # sync the stats\n",
    "        sync_sum, sync_sumsq, sync_count = self.sync(local_sum, local_sumsq, local_count)\n",
    "        # update the total stuff\n",
    "        self.total_sum += sync_sum\n",
    "        self.total_sumsq += sync_sumsq\n",
    "        self.total_count += sync_count\n",
    "        # calculate the new mean and std\n",
    "        self.mean = self.total_sum / self.total_count\n",
    "        self.std = np.sqrt(np.maximum(np.square(self.eps), (self.total_sumsq / self.total_count) - np.square(\n",
    "            self.total_sum / self.total_count)))\n",
    "\n",
    "    # average across the cpu's data\n",
    "    def _mpi_average(self, x):\n",
    "        buf = np.zeros_like(x)\n",
    "        MPI.COMM_WORLD.Allreduce(x, buf, op=MPI.SUM)\n",
    "        buf /= MPI.COMM_WORLD.Get_size()\n",
    "        return buf\n",
    "\n",
    "    # normalize the observation\n",
    "    def normalize(self, v, clip_range=None):\n",
    "        if clip_range is None:\n",
    "            clip_range = self.default_clip_range\n",
    "        return np.clip((v - self.mean) / self.std, -clip_range, clip_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class\n",
    "\n",
    "The core of DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import from_numpy, device\n",
    "import numpy as np\n",
    "# from models import Actor, Critic\n",
    "# from memory import Memory\n",
    "from torch.optim import Adam\n",
    "from mpi4py import MPI\n",
    "# from normalizer import Normalizer\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_states, n_actions, n_goals, action_bounds, capacity, env,\n",
    "                 k_future,\n",
    "                 batch_size,\n",
    "                 action_size=1,\n",
    "                 tau=0.05,\n",
    "                 actor_lr=1e-3,\n",
    "                 critic_lr=1e-3,\n",
    "                 gamma=0.98):\n",
    "        self.device = device(\"cpu\")\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.n_goals = n_goals\n",
    "        self.k_future = k_future\n",
    "        self.action_bounds = action_bounds\n",
    "        self.action_size = action_size\n",
    "        self.env = env\n",
    "\n",
    "        self.actor = Actor(self.n_states, n_actions=self.n_actions, n_goals=self.n_goals).to(self.device)\n",
    "        self.critic = Critic(self.n_states, action_size=self.action_size, n_goals=self.n_goals).to(self.device)\n",
    "        self.sync_networks(self.actor)\n",
    "        self.sync_networks(self.critic)\n",
    "        self.actor_target = Actor(self.n_states, n_actions=self.n_actions, n_goals=self.n_goals).to(self.device)\n",
    "        self.critic_target = Critic(self.n_states, action_size=self.action_size, n_goals=self.n_goals).to(self.device)\n",
    "        self.init_target_networks()\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.memory = Memory(self.capacity, self.k_future, self.env)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_optim = Adam(self.actor.parameters(), self.actor_lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), self.critic_lr)\n",
    "\n",
    "        self.state_normalizer = Normalizer(self.n_states[0], default_clip_range=5)\n",
    "        self.goal_normalizer = Normalizer(self.n_goals, default_clip_range=5)\n",
    "\n",
    "    def choose_action(self, state, goal, train_mode=True):\n",
    "        state = self.state_normalizer.normalize(state)\n",
    "        goal = self.goal_normalizer.normalize(goal)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        goal = np.expand_dims(goal, axis=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = np.concatenate([state, goal], axis=1)\n",
    "            x = from_numpy(x).float().to(self.device)\n",
    "            action = self.actor(x)[0].cpu().data.numpy()\n",
    "\n",
    "        if train_mode:\n",
    "            action += 0.2 * np.random.randn(self.n_actions)\n",
    "            action = np.clip(action, self.action_bounds[0], self.action_bounds[1])\n",
    "\n",
    "            random_actions = np.random.uniform(low=self.action_bounds[0], high=self.action_bounds[1],\n",
    "                                               size=self.n_actions)\n",
    "            action += np.random.binomial(1, 0.3, 1)[0] * (random_actions - action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store(self, mini_batch):\n",
    "        for batch in mini_batch:\n",
    "            self.memory.add(batch)\n",
    "        self._update_normalizer(mini_batch)\n",
    "\n",
    "    def init_target_networks(self):\n",
    "        self.hard_update_networks(self.actor, self.actor_target)\n",
    "        self.hard_update_networks(self.critic, self.critic_target)\n",
    "\n",
    "    @staticmethod\n",
    "    def hard_update_networks(local_model, target_model):\n",
    "        target_model.load_state_dict(local_model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update_networks(local_model, target_model, tau=0.05):\n",
    "        for t_params, e_params in zip(target_model.parameters(), local_model.parameters()):\n",
    "            t_params.data.copy_(tau * e_params.data + (1 - tau) * t_params.data)\n",
    "\n",
    "    def train(self):\n",
    "        states, actions, rewards, next_states, goals = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = self.state_normalizer.normalize(states)\n",
    "        next_states = self.state_normalizer.normalize(next_states)\n",
    "        goals = self.goal_normalizer.normalize(goals)\n",
    "        inputs = np.concatenate([states, goals], axis=1)\n",
    "        next_inputs = np.concatenate([next_states, goals], axis=1)\n",
    "\n",
    "        inputs = torch.Tensor(inputs).to(self.device)\n",
    "        rewards = torch.Tensor(rewards).to(self.device)\n",
    "        next_inputs = torch.Tensor(next_inputs).to(self.device)\n",
    "        actions = torch.Tensor(actions).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_q = self.critic_target(next_inputs, self.actor_target(next_inputs))\n",
    "            target_returns = rewards + self.gamma * target_q.detach()\n",
    "            target_returns = torch.clamp(target_returns, -1 / (1 - self.gamma), 0)\n",
    "\n",
    "        q_eval = self.critic(inputs, actions)\n",
    "        critic_loss = (target_returns - q_eval).pow(2).mean()\n",
    "\n",
    "        a = self.actor(inputs)\n",
    "        actor_loss = -self.critic(inputs, a).mean()\n",
    "        actor_loss += a.pow(2).mean()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.sync_grads(self.actor)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.sync_grads(self.critic)\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    def save_weights(self):\n",
    "        torch.save({\"actor_state_dict\": self.actor.state_dict(),\n",
    "                    \"state_normalizer_mean\": self.state_normalizer.mean,\n",
    "                    \"state_normalizer_std\": self.state_normalizer.std,\n",
    "                    \"goal_normalizer_mean\": self.goal_normalizer.mean,\n",
    "                    \"goal_normalizer_std\": self.goal_normalizer.std}, \"FetchPickAndPlace.pth\")\n",
    "\n",
    "    def load_weights(self):\n",
    "\n",
    "        checkpoint = torch.load(\"FetchPickAndPlace.pth\")\n",
    "        actor_state_dict = checkpoint[\"actor_state_dict\"]\n",
    "        self.actor.load_state_dict(actor_state_dict)\n",
    "        state_normalizer_mean = checkpoint[\"state_normalizer_mean\"]\n",
    "        self.state_normalizer.mean = state_normalizer_mean\n",
    "        state_normalizer_std = checkpoint[\"state_normalizer_std\"]\n",
    "        self.state_normalizer.std = state_normalizer_std\n",
    "        goal_normalizer_mean = checkpoint[\"goal_normalizer_mean\"]\n",
    "        self.goal_normalizer.mean = goal_normalizer_mean\n",
    "        goal_normalizer_std = checkpoint[\"goal_normalizer_std\"]\n",
    "        self.goal_normalizer.std = goal_normalizer_std\n",
    "\n",
    "    def set_to_eval_mode(self):\n",
    "        self.actor.eval()\n",
    "        # self.critic.eval()\n",
    "\n",
    "    def update_networks(self):\n",
    "        self.soft_update_networks(self.actor, self.actor_target, self.tau)\n",
    "        self.soft_update_networks(self.critic, self.critic_target, self.tau)\n",
    "\n",
    "    def _update_normalizer(self, mini_batch):\n",
    "        states, goals = self.memory.sample_for_normalization(mini_batch)\n",
    "\n",
    "        self.state_normalizer.update(states)\n",
    "        self.goal_normalizer.update(goals)\n",
    "        self.state_normalizer.recompute_stats()\n",
    "        self.goal_normalizer.recompute_stats()\n",
    "\n",
    "    @staticmethod\n",
    "    def sync_networks(network):\n",
    "        comm = MPI.COMM_WORLD\n",
    "        flat_params = _get_flat_params_or_grads(network, mode='params')\n",
    "        comm.Bcast(flat_params, root=0)\n",
    "        _set_flat_params_or_grads(network, flat_params, mode='params')\n",
    "\n",
    "    @staticmethod\n",
    "    def sync_grads(network):\n",
    "        flat_grads = _get_flat_params_or_grads(network, mode='grads')\n",
    "        comm = MPI.COMM_WORLD\n",
    "        global_grads = np.zeros_like(flat_grads)\n",
    "        comm.Allreduce(flat_grads, global_grads, op=MPI.SUM)\n",
    "        _set_flat_params_or_grads(network, global_grads, mode='grads')\n",
    "\n",
    "\n",
    "def _get_flat_params_or_grads(network, mode='params'):\n",
    "    attr = 'data' if mode == 'params' else 'grad'\n",
    "    return np.concatenate([getattr(param, attr).cpu().numpy().flatten() for param in network.parameters()])\n",
    "\n",
    "\n",
    "def _set_flat_params_or_grads(network, flat_params, mode='params'):\n",
    "    attr = 'data' if mode == 'params' else 'grad'\n",
    "    pointer = 0\n",
    "    for param in network.parameters():\n",
    "        getattr(param, attr).copy_(\n",
    "            torch.tensor(flat_params[pointer:pointer + param.data.numel()]).view_as(param.data))\n",
    "        pointer += param.data.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play class (run and record the vdo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import device\n",
    "import numpy as np\n",
    "import cv2\n",
    "from gym import wrappers\n",
    "from mujoco_py import GlfwContext\n",
    "\n",
    "GlfwContext(offscreen=True)\n",
    "\n",
    "from mujoco_py.generated import const\n",
    "\n",
    "\n",
    "class Play:\n",
    "    def __init__(self, env, agent, max_episode=4):\n",
    "        self.env = env\n",
    "        self.env = wrappers.Monitor(env, \"./videos\", video_callable=lambda episode_id: True, force=True)\n",
    "        self.max_episode = max_episode\n",
    "        self.agent = agent\n",
    "        self.agent.load_weights()\n",
    "        self.agent.set_to_eval_mode()\n",
    "        self.device = device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        for _ in range(self.max_episode):\n",
    "            env_dict = self.env.reset()\n",
    "            state = env_dict[\"observation\"]\n",
    "            achieved_goal = env_dict[\"achieved_goal\"]\n",
    "            desired_goal = env_dict[\"desired_goal\"]\n",
    "            while np.linalg.norm(achieved_goal - desired_goal) <= 0.05:\n",
    "                env_dict = self.env.reset()\n",
    "                state = env_dict[\"observation\"]\n",
    "                achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                desired_goal = env_dict[\"desired_goal\"]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, desired_goal, train_mode=False)\n",
    "                next_env_dict, r, done, _ = self.env.step(action)\n",
    "                next_state = next_env_dict[\"observation\"]\n",
    "                next_desired_goal = next_env_dict[\"desired_goal\"]\n",
    "                episode_reward += r\n",
    "                state = next_state.copy()\n",
    "                desired_goal = next_desired_goal.copy()\n",
    "                I = self.env.render(mode=\"rgb_array\")  # mode = \"rgb_array\n",
    "                self.env.viewer.cam.type = const.CAMERA_FREE\n",
    "                self.env.viewer.cam.fixedcamid = 0\n",
    "                # I = cv2.cvtColor(I, cv2.COLOR_RGB2BGR)\n",
    "                # cv2.imshow(\"I\", I)\n",
    "                # cv2.waitKey(2)\n",
    "            print(f\"episode_reward:{episode_reward:3.3f}\")\n",
    "\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0| Running_reward:-50.000| EP_reward:-50.000| Memory_length:100| Duration:35.970| Actor_Loss:0.537| Critic_Loss:0.001| Success rate:0.000| 27.9/31.3 GB RAM\n",
      "Epoch:1| Running_reward:-50.000| EP_reward:-50.000| Memory_length:200| Duration:37.208| Actor_Loss:0.919| Critic_Loss:0.003| Success rate:0.000| 27.9/31.3 GB RAM\n",
      "Epoch:2| Running_reward:-50.000| EP_reward:-50.000| Memory_length:300| Duration:37.243| Actor_Loss:1.526| Critic_Loss:0.009| Success rate:0.000| 27.9/31.3 GB RAM\n",
      "Epoch:3| Running_reward:-50.000| EP_reward:-50.000| Memory_length:400| Duration:37.876| Actor_Loss:1.794| Critic_Loss:0.009| Success rate:0.000| 28.1/31.3 GB RAM\n",
      "Epoch:4| Running_reward:-50.000| EP_reward:-50.000| Memory_length:500| Duration:38.474| Actor_Loss:1.971| Critic_Loss:0.005| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:5| Running_reward:-50.000| EP_reward:-50.000| Memory_length:600| Duration:38.091| Actor_Loss:2.509| Critic_Loss:0.016| Success rate:0.000| 28.1/31.3 GB RAM\n",
      "Epoch:6| Running_reward:-50.000| EP_reward:-50.000| Memory_length:700| Duration:38.272| Actor_Loss:2.366| Critic_Loss:0.034| Success rate:0.000| 27.9/31.3 GB RAM\n",
      "Epoch:7| Running_reward:-50.000| EP_reward:-50.000| Memory_length:800| Duration:38.361| Actor_Loss:3.143| Critic_Loss:0.029| Success rate:0.000| 28.1/31.3 GB RAM\n",
      "Epoch:8| Running_reward:-50.000| EP_reward:-50.000| Memory_length:900| Duration:38.807| Actor_Loss:2.639| Critic_Loss:0.100| Success rate:0.000| 27.9/31.3 GB RAM\n",
      "Epoch:9| Running_reward:-49.981| EP_reward:-50.000| Memory_length:1000| Duration:38.193| Actor_Loss:3.732| Critic_Loss:0.038| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:10| Running_reward:-50.000| EP_reward:-50.000| Memory_length:1100| Duration:38.181| Actor_Loss:4.024| Critic_Loss:0.021| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:11| Running_reward:-50.000| EP_reward:-50.000| Memory_length:1200| Duration:38.831| Actor_Loss:4.380| Critic_Loss:0.061| Success rate:0.000| 28.2/31.3 GB RAM\n",
      "Epoch:12| Running_reward:-50.000| EP_reward:-50.000| Memory_length:1300| Duration:38.757| Actor_Loss:3.487| Critic_Loss:0.049| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:13| Running_reward:-50.000| EP_reward:-50.000| Memory_length:1400| Duration:38.790| Actor_Loss:4.676| Critic_Loss:0.057| Success rate:0.000| 28.1/31.3 GB RAM\n",
      "Epoch:14| Running_reward:-49.982| EP_reward:-50.000| Memory_length:1500| Duration:38.473| Actor_Loss:4.440| Critic_Loss:0.114| Success rate:0.000| 28.2/31.3 GB RAM\n",
      "Epoch:15| Running_reward:-50.000| EP_reward:-50.000| Memory_length:1600| Duration:38.072| Actor_Loss:4.888| Critic_Loss:0.094| Success rate:0.000| 28.2/31.3 GB RAM\n",
      "Epoch:16| Running_reward:-49.450| EP_reward:-50.000| Memory_length:1700| Duration:39.133| Actor_Loss:4.309| Critic_Loss:0.076| Success rate:0.200| 27.9/31.3 GB RAM\n",
      "Epoch:17| Running_reward:-50.000| EP_reward:-50.000| Memory_length:1800| Duration:39.443| Actor_Loss:4.905| Critic_Loss:0.113| Success rate:0.000| 27.8/31.3 GB RAM\n",
      "Epoch:18| Running_reward:-50.000| EP_reward:-50.000| Memory_length:1900| Duration:38.725| Actor_Loss:5.160| Critic_Loss:0.150| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:19| Running_reward:-50.000| EP_reward:-50.000| Memory_length:2000| Duration:38.312| Actor_Loss:5.059| Critic_Loss:0.091| Success rate:0.000| 28.1/31.3 GB RAM\n",
      "Epoch:20| Running_reward:-50.000| EP_reward:-50.000| Memory_length:2100| Duration:39.011| Actor_Loss:5.194| Critic_Loss:0.068| Success rate:0.000| 27.8/31.3 GB RAM\n",
      "Epoch:21| Running_reward:-50.000| EP_reward:-50.000| Memory_length:2200| Duration:39.396| Actor_Loss:4.360| Critic_Loss:0.145| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:22| Running_reward:-50.000| EP_reward:-50.000| Memory_length:2300| Duration:39.896| Actor_Loss:4.429| Critic_Loss:0.193| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:23| Running_reward:-49.220| EP_reward:-10.000| Memory_length:2400| Duration:40.505| Actor_Loss:4.381| Critic_Loss:0.218| Success rate:0.200| 28.1/31.3 GB RAM\n",
      "Epoch:24| Running_reward:-49.328| EP_reward:-50.000| Memory_length:2500| Duration:41.023| Actor_Loss:5.486| Critic_Loss:0.172| Success rate:0.200| 28.1/31.3 GB RAM\n",
      "Epoch:25| Running_reward:-49.196| EP_reward:-50.000| Memory_length:2600| Duration:41.261| Actor_Loss:5.430| Critic_Loss:0.113| Success rate:0.200| 28.0/31.3 GB RAM\n",
      "Epoch:26| Running_reward:-50.000| EP_reward:-50.000| Memory_length:2700| Duration:42.360| Actor_Loss:4.937| Critic_Loss:0.159| Success rate:0.000| 28.1/31.3 GB RAM\n",
      "Epoch:27| Running_reward:-11.214| EP_reward:-50.000| Memory_length:2800| Duration:42.300| Actor_Loss:5.987| Critic_Loss:0.116| Success rate:0.200| 27.9/31.3 GB RAM\n",
      "Epoch:28| Running_reward:-50.000| EP_reward:-50.000| Memory_length:2900| Duration:42.768| Actor_Loss:6.418| Critic_Loss:0.264| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:29| Running_reward:-50.000| EP_reward:-50.000| Memory_length:3000| Duration:43.425| Actor_Loss:4.440| Critic_Loss:0.233| Success rate:0.000| 27.9/31.3 GB RAM\n",
      "Epoch:30| Running_reward:-50.000| EP_reward:-50.000| Memory_length:3100| Duration:43.920| Actor_Loss:6.215| Critic_Loss:0.151| Success rate:0.000| 28.1/31.3 GB RAM\n",
      "Epoch:31| Running_reward:-49.627| EP_reward:-50.000| Memory_length:3200| Duration:45.443| Actor_Loss:4.997| Critic_Loss:0.119| Success rate:0.100| 28.0/31.3 GB RAM\n",
      "Epoch:32| Running_reward:-50.000| EP_reward:-50.000| Memory_length:3300| Duration:45.505| Actor_Loss:4.624| Critic_Loss:0.101| Success rate:0.000| 27.7/31.3 GB RAM\n",
      "Epoch:33| Running_reward:-49.597| EP_reward:-50.000| Memory_length:3400| Duration:47.073| Actor_Loss:5.260| Critic_Loss:0.136| Success rate:0.100| 27.8/31.3 GB RAM\n",
      "Epoch:34| Running_reward:-48.871| EP_reward:-50.000| Memory_length:3500| Duration:47.686| Actor_Loss:5.599| Critic_Loss:0.179| Success rate:0.300| 27.9/31.3 GB RAM\n",
      "Epoch:35| Running_reward:-50.000| EP_reward:-50.000| Memory_length:3600| Duration:48.875| Actor_Loss:4.652| Critic_Loss:0.136| Success rate:0.000| 28.0/31.3 GB RAM\n",
      "Epoch:36| Running_reward:-50.000| EP_reward:-50.000| Memory_length:3700| Duration:48.551| Actor_Loss:5.061| Critic_Loss:0.114| Success rate:0.000| 27.9/31.3 GB RAM\n",
      "Epoch:37| Running_reward:-49.592| EP_reward:-50.000| Memory_length:3800| Duration:49.315| Actor_Loss:4.626| Critic_Loss:0.097| Success rate:0.100| 27.9/31.3 GB RAM\n",
      "Epoch:38| Running_reward:-48.924| EP_reward:-50.000| Memory_length:3900| Duration:48.999| Actor_Loss:4.340| Critic_Loss:0.361| Success rate:0.300| 28.0/31.3 GB RAM\n",
      "Epoch:39| Running_reward:-49.963| EP_reward:-50.000| Memory_length:4000| Duration:49.182| Actor_Loss:5.239| Critic_Loss:0.178| Success rate:0.000| 27.8/31.3 GB RAM\n",
      "Epoch:40| Running_reward:-8.617| EP_reward:-15.000| Memory_length:4100| Duration:49.933| Actor_Loss:4.212| Critic_Loss:0.246| Success rate:0.400| 27.8/31.3 GB RAM\n",
      "Epoch:41| Running_reward:-49.486| EP_reward:-50.000| Memory_length:4200| Duration:49.544| Actor_Loss:4.757| Critic_Loss:0.472| Success rate:0.200| 28.0/31.3 GB RAM\n",
      "Epoch:42| Running_reward:-49.392| EP_reward:-50.000| Memory_length:4300| Duration:50.438| Actor_Loss:5.657| Critic_Loss:0.358| Success rate:0.200| 28.0/31.3 GB RAM\n",
      "Epoch:43| Running_reward:-48.772| EP_reward:-14.000| Memory_length:4400| Duration:50.018| Actor_Loss:5.753| Critic_Loss:0.460| Success rate:0.300| 28.0/31.3 GB RAM\n",
      "Epoch:44| Running_reward:-49.623| EP_reward:-50.000| Memory_length:4500| Duration:50.387| Actor_Loss:5.898| Critic_Loss:0.556| Success rate:0.100| 27.8/31.3 GB RAM\n",
      "Epoch:45| Running_reward:-15.903| EP_reward:-12.000| Memory_length:4600| Duration:49.879| Actor_Loss:6.551| Critic_Loss:0.317| Success rate:0.500| 27.8/31.3 GB RAM\n",
      "Epoch:46| Running_reward:-49.271| EP_reward:-50.000| Memory_length:4700| Duration:49.704| Actor_Loss:5.824| Critic_Loss:0.176| Success rate:0.200| 28.0/31.3 GB RAM\n",
      "Epoch:47| Running_reward:-47.033| EP_reward:-50.000| Memory_length:4800| Duration:49.690| Actor_Loss:4.425| Critic_Loss:0.221| Success rate:0.400| 28.0/31.3 GB RAM\n",
      "Epoch:48| Running_reward:-49.355| EP_reward:-15.000| Memory_length:4900| Duration:49.984| Actor_Loss:5.583| Critic_Loss:1.236| Success rate:0.200| 27.8/31.3 GB RAM\n",
      "Epoch:49| Running_reward:-49.100| EP_reward:-14.000| Memory_length:5000| Duration:50.626| Actor_Loss:5.177| Critic_Loss:0.885| Success rate:0.300| 27.9/31.3 GB RAM\n",
      "Epoch:50| Running_reward:-48.588| EP_reward:-27.000| Memory_length:5100| Duration:51.944| Actor_Loss:5.032| Critic_Loss:0.142| Success rate:0.400| 28.0/31.3 GB RAM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:51| Running_reward:-49.171| EP_reward:-15.000| Memory_length:5200| Duration:51.584| Actor_Loss:5.285| Critic_Loss:0.179| Success rate:0.300| 27.9/31.3 GB RAM\n",
      "Epoch:52| Running_reward:-48.536| EP_reward:-50.000| Memory_length:5300| Duration:49.819| Actor_Loss:6.127| Critic_Loss:0.552| Success rate:0.400| 28.0/31.3 GB RAM\n",
      "Epoch:53| Running_reward:-48.944| EP_reward:-50.000| Memory_length:5400| Duration:50.655| Actor_Loss:5.060| Critic_Loss:0.188| Success rate:0.300| 28.1/31.3 GB RAM\n",
      "Epoch:54| Running_reward:-48.911| EP_reward:-50.000| Memory_length:5500| Duration:49.998| Actor_Loss:5.604| Critic_Loss:0.503| Success rate:0.300| 28.1/31.3 GB RAM\n",
      "Epoch:55| Running_reward:-48.863| EP_reward:-50.000| Memory_length:5600| Duration:50.052| Actor_Loss:5.662| Critic_Loss:0.223| Success rate:0.300| 27.9/31.3 GB RAM\n",
      "Epoch:56| Running_reward:-49.088| EP_reward:-50.000| Memory_length:5700| Duration:49.874| Actor_Loss:6.072| Critic_Loss:0.301| Success rate:0.300| 27.9/31.3 GB RAM\n",
      "Epoch:57| Running_reward:-48.774| EP_reward:-43.000| Memory_length:5800| Duration:49.933| Actor_Loss:4.219| Critic_Loss:0.145| Success rate:0.300| 28.0/31.3 GB RAM\n",
      "Epoch:58| Running_reward:-27.889| EP_reward:-50.000| Memory_length:5900| Duration:50.271| Actor_Loss:6.259| Critic_Loss:0.162| Success rate:0.300| 27.8/31.3 GB RAM\n",
      "Epoch:59| Running_reward:-49.226| EP_reward:-50.000| Memory_length:6000| Duration:49.811| Actor_Loss:5.230| Critic_Loss:0.316| Success rate:0.200| 27.9/31.3 GB RAM\n",
      "Epoch:60| Running_reward:-48.677| EP_reward:-12.000| Memory_length:6100| Duration:50.292| Actor_Loss:4.285| Critic_Loss:0.984| Success rate:0.400| 28.0/31.3 GB RAM\n",
      "Epoch:61| Running_reward:-12.595| EP_reward:-50.000| Memory_length:6200| Duration:50.934| Actor_Loss:6.164| Critic_Loss:0.145| Success rate:0.300| 27.8/31.3 GB RAM\n",
      "Epoch:62| Running_reward:-46.158| EP_reward:-17.000| Memory_length:6300| Duration:50.044| Actor_Loss:5.778| Critic_Loss:0.438| Success rate:0.300| 27.8/31.3 GB RAM\n",
      "Epoch:63| Running_reward:-15.115| EP_reward:-19.000| Memory_length:6400| Duration:50.463| Actor_Loss:5.662| Critic_Loss:0.116| Success rate:0.500| 27.9/31.3 GB RAM\n",
      "Epoch:64| Running_reward:-16.743| EP_reward:-13.000| Memory_length:6500| Duration:50.380| Actor_Loss:6.131| Critic_Loss:0.420| Success rate:0.200| 27.9/31.3 GB RAM\n",
      "Epoch:65| Running_reward:-49.285| EP_reward:-50.000| Memory_length:6600| Duration:50.309| Actor_Loss:5.920| Critic_Loss:0.111| Success rate:0.300| 28.1/31.3 GB RAM\n",
      "Epoch:66| Running_reward:-11.138| EP_reward:-21.000| Memory_length:6700| Duration:50.611| Actor_Loss:5.049| Critic_Loss:0.486| Success rate:0.500| 27.9/31.3 GB RAM\n",
      "Epoch:67| Running_reward:-49.341| EP_reward:-50.000| Memory_length:6800| Duration:50.839| Actor_Loss:5.920| Critic_Loss:0.252| Success rate:0.200| 27.9/31.3 GB RAM\n",
      "Epoch:68| Running_reward:-49.012| EP_reward:-49.000| Memory_length:6900| Duration:49.887| Actor_Loss:5.528| Critic_Loss:0.889| Success rate:0.300| 28.1/31.3 GB RAM\n",
      "Epoch:69| Running_reward:-11.727| EP_reward:-50.000| Memory_length:7000| Duration:50.187| Actor_Loss:5.629| Critic_Loss:0.390| Success rate:0.300| 27.9/31.3 GB RAM\n",
      "Epoch:70| Running_reward:-48.579| EP_reward:-50.000| Memory_length:7100| Duration:50.298| Actor_Loss:5.727| Critic_Loss:0.113| Success rate:0.400| 28.1/31.3 GB RAM\n",
      "Epoch:71| Running_reward:-10.419| EP_reward:-50.000| Memory_length:7200| Duration:50.648| Actor_Loss:5.046| Critic_Loss:0.118| Success rate:0.700| 28.1/31.3 GB RAM\n",
      "Epoch:72| Running_reward:-49.459| EP_reward:-50.000| Memory_length:7300| Duration:50.052| Actor_Loss:5.070| Critic_Loss:0.250| Success rate:0.200| 27.9/31.3 GB RAM\n",
      "Epoch:73| Running_reward:-13.196| EP_reward:-50.000| Memory_length:7400| Duration:50.832| Actor_Loss:5.468| Critic_Loss:0.196| Success rate:0.500| 28.1/31.3 GB RAM\n",
      "Epoch:74| Running_reward:-14.890| EP_reward:-50.000| Memory_length:7500| Duration:50.029| Actor_Loss:5.335| Critic_Loss:0.866| Success rate:0.200| 28.1/31.3 GB RAM\n",
      "Epoch:75| Running_reward:-15.347| EP_reward:-50.000| Memory_length:7600| Duration:50.434| Actor_Loss:5.422| Critic_Loss:0.324| Success rate:0.600| 28.1/31.3 GB RAM\n",
      "Epoch:76| Running_reward:-16.837| EP_reward:-50.000| Memory_length:7700| Duration:50.518| Actor_Loss:5.181| Critic_Loss:0.258| Success rate:0.400| 28.2/31.3 GB RAM\n",
      "Epoch:77| Running_reward:-13.127| EP_reward:-50.000| Memory_length:7800| Duration:50.787| Actor_Loss:5.017| Critic_Loss:0.249| Success rate:0.600| 28.0/31.3 GB RAM\n",
      "Epoch:78| Running_reward:-49.629| EP_reward:-50.000| Memory_length:7900| Duration:50.280| Actor_Loss:3.871| Critic_Loss:0.159| Success rate:0.100| 28.2/31.3 GB RAM\n",
      "Epoch:79| Running_reward:-48.293| EP_reward:-50.000| Memory_length:8000| Duration:49.810| Actor_Loss:5.242| Critic_Loss:0.262| Success rate:0.600| 28.0/31.3 GB RAM\n",
      "Epoch:80| Running_reward:-48.966| EP_reward:-50.000| Memory_length:8100| Duration:50.190| Actor_Loss:5.036| Critic_Loss:0.189| Success rate:0.300| 28.0/31.3 GB RAM\n",
      "Epoch:81| Running_reward:-49.253| EP_reward:-7.000| Memory_length:8200| Duration:50.779| Actor_Loss:5.295| Critic_Loss:0.214| Success rate:0.200| 28.0/31.3 GB RAM\n",
      "Epoch:82| Running_reward:-48.381| EP_reward:-50.000| Memory_length:8300| Duration:50.573| Actor_Loss:5.422| Critic_Loss:0.279| Success rate:0.400| 28.1/31.3 GB RAM\n",
      "Epoch:83| Running_reward:-48.693| EP_reward:-50.000| Memory_length:8400| Duration:50.437| Actor_Loss:4.812| Critic_Loss:0.170| Success rate:0.400| 28.2/31.3 GB RAM\n",
      "Epoch:84| Running_reward:-47.689| EP_reward:-12.000| Memory_length:8500| Duration:51.049| Actor_Loss:6.341| Critic_Loss:0.650| Success rate:0.600| 28.0/31.3 GB RAM\n",
      "Epoch:85| Running_reward:-48.527| EP_reward:-9.000| Memory_length:8600| Duration:50.744| Actor_Loss:5.826| Critic_Loss:0.186| Success rate:0.400| 28.1/31.3 GB RAM\n",
      "Epoch:86| Running_reward:-47.895| EP_reward:-50.000| Memory_length:8700| Duration:50.924| Actor_Loss:6.460| Critic_Loss:0.478| Success rate:0.600| 28.2/31.3 GB RAM\n",
      "Epoch:87| Running_reward:-48.839| EP_reward:-50.000| Memory_length:8800| Duration:51.122| Actor_Loss:4.488| Critic_Loss:0.170| Success rate:0.400| 28.0/31.3 GB RAM\n",
      "Epoch:88| Running_reward:-49.166| EP_reward:-8.000| Memory_length:8900| Duration:50.510| Actor_Loss:6.206| Critic_Loss:0.306| Success rate:0.300| 28.0/31.3 GB RAM\n",
      "Epoch:89| Running_reward:-26.182| EP_reward:-6.000| Memory_length:9000| Duration:50.752| Actor_Loss:3.651| Critic_Loss:0.225| Success rate:0.500| 28.1/31.3 GB RAM\n",
      "Epoch:90| Running_reward:-47.909| EP_reward:-8.000| Memory_length:9100| Duration:50.894| Actor_Loss:4.599| Critic_Loss:0.239| Success rate:0.600| 28.2/31.3 GB RAM\n",
      "Epoch:91| Running_reward:-47.676| EP_reward:-9.000| Memory_length:9200| Duration:50.910| Actor_Loss:4.650| Critic_Loss:0.197| Success rate:0.600| 28.1/31.3 GB RAM\n",
      "Epoch:92| Running_reward:-17.651| EP_reward:-50.000| Memory_length:9300| Duration:50.697| Actor_Loss:5.329| Critic_Loss:0.123| Success rate:0.200| 28.0/31.3 GB RAM\n",
      "Epoch:93| Running_reward:-15.771| EP_reward:-7.000| Memory_length:9400| Duration:51.306| Actor_Loss:4.996| Critic_Loss:0.578| Success rate:0.700| 28.0/31.3 GB RAM\n",
      "Epoch:94| Running_reward:-17.712| EP_reward:-4.000| Memory_length:9500| Duration:50.826| Actor_Loss:6.027| Critic_Loss:0.259| Success rate:0.400| 28.1/31.3 GB RAM\n",
      "Epoch:95| Running_reward:-12.023| EP_reward:-50.000| Memory_length:9600| Duration:50.467| Actor_Loss:5.571| Critic_Loss:0.708| Success rate:0.500| 28.3/31.3 GB RAM\n",
      "Epoch:96| Running_reward:-48.845| EP_reward:-50.000| Memory_length:9700| Duration:50.399| Actor_Loss:5.465| Critic_Loss:0.196| Success rate:0.300| 28.3/31.3 GB RAM\n",
      "Epoch:97| Running_reward:-48.514| EP_reward:-5.000| Memory_length:9800| Duration:49.615| Actor_Loss:4.326| Critic_Loss:0.358| Success rate:0.400| 28.1/31.3 GB RAM\n",
      "Epoch:98| Running_reward:-11.815| EP_reward:-50.000| Memory_length:9900| Duration:50.285| Actor_Loss:5.339| Critic_Loss:0.316| Success rate:0.300| 28.1/31.3 GB RAM\n",
      "Epoch:99| Running_reward:-18.516| EP_reward:-50.000| Memory_length:10000| Duration:50.499| Actor_Loss:6.029| Critic_Loss:0.107| Success rate:0.500| 28.2/31.3 GB RAM\n",
      "Epoch:100| Running_reward:-8.923| EP_reward:-7.000| Memory_length:10100| Duration:50.745| Actor_Loss:6.126| Critic_Loss:0.089| Success rate:0.800| 28.1/31.3 GB RAM\n",
      "Epoch:101| Running_reward:-47.687| EP_reward:-50.000| Memory_length:10200| Duration:50.583| Actor_Loss:5.770| Critic_Loss:0.311| Success rate:0.600| 28.3/31.3 GB RAM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:102| Running_reward:-49.167| EP_reward:-50.000| Memory_length:10300| Duration:50.736| Actor_Loss:6.324| Critic_Loss:0.344| Success rate:0.300| 28.3/31.3 GB RAM\n",
      "Epoch:103| Running_reward:-48.754| EP_reward:-50.000| Memory_length:10400| Duration:50.620| Actor_Loss:6.285| Critic_Loss:0.637| Success rate:0.300| 28.3/31.3 GB RAM\n",
      "Epoch:104| Running_reward:-26.560| EP_reward:-50.000| Memory_length:10500| Duration:51.386| Actor_Loss:5.084| Critic_Loss:0.220| Success rate:0.600| 28.3/31.3 GB RAM\n",
      "Epoch:105| Running_reward:-48.428| EP_reward:-22.000| Memory_length:10600| Duration:50.825| Actor_Loss:5.188| Critic_Loss:0.808| Success rate:0.500| 28.0/31.3 GB RAM\n",
      "Epoch:106| Running_reward:-47.563| EP_reward:-50.000| Memory_length:10700| Duration:50.803| Actor_Loss:6.261| Critic_Loss:0.292| Success rate:0.600| 28.3/31.3 GB RAM\n",
      "Epoch:107| Running_reward:-48.803| EP_reward:-50.000| Memory_length:10800| Duration:51.432| Actor_Loss:5.547| Critic_Loss:0.306| Success rate:0.300| 28.1/31.3 GB RAM\n",
      "Epoch:108| Running_reward:-11.149| EP_reward:-6.000| Memory_length:10900| Duration:51.080| Actor_Loss:4.574| Critic_Loss:0.140| Success rate:0.900| 28.3/31.3 GB RAM\n",
      "Epoch:109| Running_reward:-25.759| EP_reward:-15.000| Memory_length:11000| Duration:50.793| Actor_Loss:4.323| Critic_Loss:0.586| Success rate:0.500| 28.3/31.3 GB RAM\n",
      "Epoch:110| Running_reward:-13.829| EP_reward:-7.000| Memory_length:11100| Duration:50.803| Actor_Loss:4.177| Critic_Loss:0.335| Success rate:0.500| 28.3/31.3 GB RAM\n",
      "Epoch:111| Running_reward:-6.207| EP_reward:-50.000| Memory_length:11200| Duration:50.538| Actor_Loss:5.062| Critic_Loss:0.305| Success rate:0.600| 28.1/31.3 GB RAM\n",
      "Epoch:112| Running_reward:-48.256| EP_reward:-50.000| Memory_length:11300| Duration:50.246| Actor_Loss:4.678| Critic_Loss:0.129| Success rate:0.500| 28.3/31.3 GB RAM\n",
      "Epoch:113| Running_reward:-49.274| EP_reward:-50.000| Memory_length:11400| Duration:51.120| Actor_Loss:4.487| Critic_Loss:0.126| Success rate:0.200| 28.3/31.3 GB RAM\n",
      "Epoch:114| Running_reward:-47.575| EP_reward:-8.000| Memory_length:11500| Duration:50.518| Actor_Loss:5.819| Critic_Loss:0.167| Success rate:0.600| 28.3/31.3 GB RAM\n",
      "Epoch:115| Running_reward:-21.795| EP_reward:-5.000| Memory_length:11600| Duration:50.520| Actor_Loss:7.472| Critic_Loss:0.482| Success rate:0.800| 28.3/31.3 GB RAM\n",
      "Epoch:116| Running_reward:-14.267| EP_reward:-4.000| Memory_length:11700| Duration:50.368| Actor_Loss:4.965| Critic_Loss:0.189| Success rate:0.600| 28.3/31.3 GB RAM\n",
      "Epoch:117| Running_reward:-48.078| EP_reward:-13.000| Memory_length:11800| Duration:50.538| Actor_Loss:4.365| Critic_Loss:0.915| Success rate:0.500| 28.1/31.3 GB RAM\n",
      "Epoch:118| Running_reward:-48.449| EP_reward:-50.000| Memory_length:11900| Duration:50.606| Actor_Loss:5.256| Critic_Loss:0.295| Success rate:0.400| 28.3/31.3 GB RAM\n",
      "Epoch:119| Running_reward:-8.032| EP_reward:-50.000| Memory_length:12000| Duration:50.747| Actor_Loss:5.014| Critic_Loss:0.405| Success rate:0.600| 28.3/31.3 GB RAM\n",
      "Epoch:120| Running_reward:-48.202| EP_reward:-50.000| Memory_length:12100| Duration:50.605| Actor_Loss:5.481| Critic_Loss:0.135| Success rate:0.500| 28.3/31.3 GB RAM\n",
      "Epoch:121| Running_reward:-48.462| EP_reward:-50.000| Memory_length:12200| Duration:50.782| Actor_Loss:5.699| Critic_Loss:0.188| Success rate:0.400| 28.2/31.3 GB RAM\n",
      "Epoch:122| Running_reward:-15.940| EP_reward:-4.000| Memory_length:12300| Duration:51.012| Actor_Loss:4.307| Critic_Loss:0.275| Success rate:0.600| 28.1/31.3 GB RAM\n",
      "Epoch:123| Running_reward:-48.637| EP_reward:-8.000| Memory_length:12400| Duration:50.856| Actor_Loss:5.029| Critic_Loss:0.194| Success rate:0.400| 28.3/31.3 GB RAM\n",
      "Epoch:124| Running_reward:-47.828| EP_reward:-50.000| Memory_length:12500| Duration:50.847| Actor_Loss:4.233| Critic_Loss:0.128| Success rate:0.600| 28.3/31.3 GB RAM\n",
      "Epoch:125| Running_reward:-44.607| EP_reward:-50.000| Memory_length:12600| Duration:51.007| Actor_Loss:4.773| Critic_Loss:0.168| Success rate:0.700| 28.1/31.3 GB RAM\n",
      "Epoch:126| Running_reward:-48.571| EP_reward:-50.000| Memory_length:12700| Duration:51.947| Actor_Loss:5.896| Critic_Loss:0.796| Success rate:0.400| 28.1/31.3 GB RAM\n",
      "Epoch:127| Running_reward:-48.363| EP_reward:-50.000| Memory_length:12800| Duration:50.854| Actor_Loss:5.055| Critic_Loss:0.956| Success rate:0.500| 28.2/31.3 GB RAM\n",
      "Epoch:128| Running_reward:-47.660| EP_reward:-12.000| Memory_length:12900| Duration:51.514| Actor_Loss:4.014| Critic_Loss:0.227| Success rate:0.600| 28.4/31.3 GB RAM\n",
      "Epoch:129| Running_reward:-48.037| EP_reward:-5.000| Memory_length:13000| Duration:53.937| Actor_Loss:4.237| Critic_Loss:0.234| Success rate:0.500| 28.5/31.3 GB RAM\n",
      "Epoch:130| Running_reward:-9.031| EP_reward:-50.000| Memory_length:13100| Duration:52.166| Actor_Loss:3.637| Critic_Loss:0.169| Success rate:0.300| 28.4/31.3 GB RAM\n",
      "Epoch:131| Running_reward:-48.669| EP_reward:-50.000| Memory_length:13200| Duration:52.085| Actor_Loss:4.605| Critic_Loss:0.361| Success rate:0.500| 28.7/31.3 GB RAM\n",
      "Epoch:132| Running_reward:-18.225| EP_reward:-6.000| Memory_length:13300| Duration:51.734| Actor_Loss:4.776| Critic_Loss:0.624| Success rate:0.500| 28.6/31.3 GB RAM\n",
      "Epoch:133| Running_reward:-15.953| EP_reward:-50.000| Memory_length:13400| Duration:51.575| Actor_Loss:4.521| Critic_Loss:0.240| Success rate:0.400| 28.5/31.3 GB RAM\n",
      "Epoch:134| Running_reward:-48.485| EP_reward:-12.000| Memory_length:13500| Duration:51.721| Actor_Loss:4.173| Critic_Loss:0.647| Success rate:0.400| 28.7/31.3 GB RAM\n",
      "Epoch:135| Running_reward:-48.274| EP_reward:-11.000| Memory_length:13600| Duration:51.603| Actor_Loss:5.466| Critic_Loss:0.231| Success rate:0.400| 28.7/31.3 GB RAM\n",
      "Epoch:136| Running_reward:-14.321| EP_reward:-50.000| Memory_length:13700| Duration:51.612| Actor_Loss:4.994| Critic_Loss:0.197| Success rate:0.600| 28.7/31.3 GB RAM\n",
      "Epoch:137| Running_reward:-47.417| EP_reward:-9.000| Memory_length:13800| Duration:51.374| Actor_Loss:4.625| Critic_Loss:0.123| Success rate:0.600| 28.5/31.3 GB RAM\n",
      "Epoch:138| Running_reward:-35.913| EP_reward:-9.000| Memory_length:13900| Duration:50.675| Actor_Loss:5.167| Critic_Loss:0.149| Success rate:0.500| 28.5/31.3 GB RAM\n",
      "Epoch:139| Running_reward:-22.360| EP_reward:-50.000| Memory_length:14000| Duration:51.830| Actor_Loss:4.667| Critic_Loss:0.185| Success rate:0.700| 28.6/31.3 GB RAM\n",
      "Epoch:140| Running_reward:-17.423| EP_reward:-50.000| Memory_length:14000| Duration:51.413| Actor_Loss:5.159| Critic_Loss:0.436| Success rate:0.500| 28.7/31.3 GB RAM\n",
      "Epoch:141| Running_reward:-48.129| EP_reward:-17.000| Memory_length:14000| Duration:51.266| Actor_Loss:5.147| Critic_Loss:0.192| Success rate:0.500| 28.5/31.3 GB RAM\n",
      "Epoch:142| Running_reward:-13.975| EP_reward:-5.000| Memory_length:14000| Duration:51.282| Actor_Loss:4.697| Critic_Loss:0.682| Success rate:0.700| 28.7/31.3 GB RAM\n",
      "Epoch:143| Running_reward:-11.146| EP_reward:-50.000| Memory_length:14000| Duration:51.833| Actor_Loss:4.748| Critic_Loss:0.182| Success rate:0.700| 28.7/31.3 GB RAM\n",
      "Epoch:144| Running_reward:-48.224| EP_reward:-47.000| Memory_length:14000| Duration:51.322| Actor_Loss:4.819| Critic_Loss:0.281| Success rate:0.500| 28.6/31.3 GB RAM\n",
      "Epoch:145| Running_reward:-47.514| EP_reward:-17.000| Memory_length:14000| Duration:51.613| Actor_Loss:3.945| Critic_Loss:0.252| Success rate:0.700| 28.5/31.3 GB RAM\n",
      "Epoch:146| Running_reward:-9.419| EP_reward:-50.000| Memory_length:14000| Duration:52.489| Actor_Loss:3.834| Critic_Loss:0.109| Success rate:0.700| 28.6/31.3 GB RAM\n",
      "Epoch:147| Running_reward:-47.276| EP_reward:-10.000| Memory_length:14000| Duration:51.649| Actor_Loss:4.761| Critic_Loss:3.113| Success rate:0.700| 28.7/31.3 GB RAM\n",
      "Epoch:148| Running_reward:-47.725| EP_reward:-50.000| Memory_length:14000| Duration:51.573| Actor_Loss:4.010| Critic_Loss:0.175| Success rate:0.600| 28.6/31.3 GB RAM\n",
      "Epoch:149| Running_reward:-11.492| EP_reward:-45.000| Memory_length:14000| Duration:51.827| Actor_Loss:3.449| Critic_Loss:0.257| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:150| Running_reward:-23.516| EP_reward:-50.000| Memory_length:14000| Duration:51.519| Actor_Loss:4.795| Critic_Loss:0.228| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:151| Running_reward:-11.241| EP_reward:-19.000| Memory_length:14000| Duration:51.364| Actor_Loss:4.414| Critic_Loss:0.242| Success rate:0.700| 28.6/31.3 GB RAM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:152| Running_reward:-10.958| EP_reward:-10.000| Memory_length:14000| Duration:50.973| Actor_Loss:4.848| Critic_Loss:0.213| Success rate:0.600| 28.8/31.3 GB RAM\n",
      "Epoch:153| Running_reward:-14.000| EP_reward:-45.000| Memory_length:14000| Duration:52.056| Actor_Loss:4.238| Critic_Loss:0.348| Success rate:0.500| 28.6/31.3 GB RAM\n",
      "Epoch:154| Running_reward:-12.386| EP_reward:-50.000| Memory_length:14000| Duration:51.891| Actor_Loss:3.111| Critic_Loss:0.272| Success rate:0.400| 28.7/31.3 GB RAM\n",
      "Epoch:155| Running_reward:-9.368| EP_reward:-43.000| Memory_length:14000| Duration:52.068| Actor_Loss:4.543| Critic_Loss:0.648| Success rate:0.600| 28.7/31.3 GB RAM\n",
      "Epoch:156| Running_reward:-47.761| EP_reward:-50.000| Memory_length:14000| Duration:51.389| Actor_Loss:4.461| Critic_Loss:0.433| Success rate:0.600| 28.6/31.3 GB RAM\n",
      "Epoch:157| Running_reward:-35.428| EP_reward:-8.000| Memory_length:14000| Duration:51.385| Actor_Loss:4.107| Critic_Loss:0.869| Success rate:0.700| 28.5/31.3 GB RAM\n",
      "Epoch:158| Running_reward:-14.267| EP_reward:-50.000| Memory_length:14000| Duration:51.705| Actor_Loss:3.942| Critic_Loss:0.232| Success rate:0.600| 28.6/31.3 GB RAM\n",
      "Epoch:159| Running_reward:-11.882| EP_reward:-11.000| Memory_length:14000| Duration:51.802| Actor_Loss:4.828| Critic_Loss:0.408| Success rate:1.000| 28.5/31.3 GB RAM\n",
      "Epoch:160| Running_reward:-47.954| EP_reward:-40.000| Memory_length:14000| Duration:51.940| Actor_Loss:3.857| Critic_Loss:0.132| Success rate:0.500| 28.5/31.3 GB RAM\n",
      "Epoch:161| Running_reward:-47.858| EP_reward:-50.000| Memory_length:14000| Duration:52.292| Actor_Loss:5.660| Critic_Loss:0.712| Success rate:0.300| 28.7/31.3 GB RAM\n",
      "Epoch:162| Running_reward:-7.866| EP_reward:-11.000| Memory_length:14000| Duration:51.771| Actor_Loss:5.046| Critic_Loss:0.238| Success rate:0.400| 28.5/31.3 GB RAM\n",
      "Epoch:163| Running_reward:-28.984| EP_reward:-50.000| Memory_length:14000| Duration:51.757| Actor_Loss:5.373| Critic_Loss:0.492| Success rate:0.600| 28.7/31.3 GB RAM\n",
      "Epoch:164| Running_reward:-12.047| EP_reward:-7.000| Memory_length:14000| Duration:51.989| Actor_Loss:4.897| Critic_Loss:0.336| Success rate:0.700| 28.7/31.3 GB RAM\n",
      "Epoch:165| Running_reward:-35.402| EP_reward:-19.000| Memory_length:14000| Duration:51.621| Actor_Loss:4.622| Critic_Loss:0.277| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:166| Running_reward:-10.050| EP_reward:-8.000| Memory_length:14000| Duration:51.933| Actor_Loss:3.272| Critic_Loss:0.177| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:167| Running_reward:-10.828| EP_reward:-11.000| Memory_length:14000| Duration:52.468| Actor_Loss:4.186| Critic_Loss:0.203| Success rate:0.800| 28.7/31.3 GB RAM\n",
      "Epoch:168| Running_reward:-9.919| EP_reward:-50.000| Memory_length:14000| Duration:52.214| Actor_Loss:4.084| Critic_Loss:0.200| Success rate:0.600| 28.8/31.3 GB RAM\n",
      "Epoch:169| Running_reward:-21.140| EP_reward:-50.000| Memory_length:14000| Duration:52.193| Actor_Loss:3.721| Critic_Loss:0.300| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:170| Running_reward:-47.556| EP_reward:-12.000| Memory_length:14000| Duration:51.657| Actor_Loss:3.608| Critic_Loss:0.393| Success rate:0.600| 28.6/31.3 GB RAM\n",
      "Epoch:171| Running_reward:-7.177| EP_reward:-12.000| Memory_length:14000| Duration:52.412| Actor_Loss:3.759| Critic_Loss:0.247| Success rate:0.300| 28.8/31.3 GB RAM\n",
      "Epoch:172| Running_reward:-11.021| EP_reward:-50.000| Memory_length:14000| Duration:51.919| Actor_Loss:3.709| Critic_Loss:0.821| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:173| Running_reward:-48.170| EP_reward:-50.000| Memory_length:14000| Duration:51.838| Actor_Loss:4.018| Critic_Loss:0.624| Success rate:0.500| 28.6/31.3 GB RAM\n",
      "Epoch:174| Running_reward:-47.224| EP_reward:-13.000| Memory_length:14000| Duration:51.870| Actor_Loss:3.884| Critic_Loss:0.337| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:175| Running_reward:-12.189| EP_reward:-50.000| Memory_length:14000| Duration:51.304| Actor_Loss:4.001| Critic_Loss:0.189| Success rate:0.500| 28.6/31.3 GB RAM\n",
      "Epoch:176| Running_reward:-13.221| EP_reward:-6.000| Memory_length:14000| Duration:51.818| Actor_Loss:3.838| Critic_Loss:0.359| Success rate:0.900| 28.8/31.3 GB RAM\n",
      "Epoch:177| Running_reward:-12.853| EP_reward:-50.000| Memory_length:14000| Duration:51.991| Actor_Loss:4.241| Critic_Loss:0.298| Success rate:0.500| 28.7/31.3 GB RAM\n",
      "Epoch:178| Running_reward:-6.936| EP_reward:-8.000| Memory_length:14000| Duration:52.146| Actor_Loss:3.673| Critic_Loss:0.383| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:179| Running_reward:-13.072| EP_reward:-9.000| Memory_length:14000| Duration:52.103| Actor_Loss:3.844| Critic_Loss:0.284| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:180| Running_reward:-10.019| EP_reward:-50.000| Memory_length:14000| Duration:52.541| Actor_Loss:4.479| Critic_Loss:0.757| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:181| Running_reward:-11.301| EP_reward:-7.000| Memory_length:14000| Duration:52.309| Actor_Loss:3.864| Critic_Loss:0.199| Success rate:0.900| 28.6/31.3 GB RAM\n",
      "Epoch:182| Running_reward:-11.194| EP_reward:-50.000| Memory_length:14000| Duration:51.579| Actor_Loss:3.325| Critic_Loss:0.367| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:183| Running_reward:-7.496| EP_reward:-6.000| Memory_length:14000| Duration:51.945| Actor_Loss:4.487| Critic_Loss:0.263| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:184| Running_reward:-42.763| EP_reward:-10.000| Memory_length:14000| Duration:51.491| Actor_Loss:3.220| Critic_Loss:0.326| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:185| Running_reward:-8.188| EP_reward:-7.000| Memory_length:14000| Duration:51.238| Actor_Loss:3.015| Critic_Loss:0.408| Success rate:0.800| 28.7/31.3 GB RAM\n",
      "Epoch:186| Running_reward:-48.053| EP_reward:-11.000| Memory_length:14000| Duration:52.004| Actor_Loss:3.568| Critic_Loss:0.210| Success rate:0.500| 28.7/31.3 GB RAM\n",
      "Epoch:187| Running_reward:-9.314| EP_reward:-12.000| Memory_length:14000| Duration:52.134| Actor_Loss:4.196| Critic_Loss:0.286| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:188| Running_reward:-8.272| EP_reward:-6.000| Memory_length:14000| Duration:52.080| Actor_Loss:3.467| Critic_Loss:0.564| Success rate:1.000| 28.7/31.3 GB RAM\n",
      "Epoch:189| Running_reward:-46.876| EP_reward:-7.000| Memory_length:14000| Duration:51.847| Actor_Loss:2.638| Critic_Loss:0.227| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:190| Running_reward:-8.369| EP_reward:-5.000| Memory_length:14000| Duration:51.441| Actor_Loss:2.680| Critic_Loss:0.246| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:191| Running_reward:-14.356| EP_reward:-8.000| Memory_length:14000| Duration:51.454| Actor_Loss:2.971| Critic_Loss:0.647| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:192| Running_reward:-31.052| EP_reward:-14.000| Memory_length:14000| Duration:51.825| Actor_Loss:2.965| Critic_Loss:0.341| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:193| Running_reward:-13.251| EP_reward:-50.000| Memory_length:14000| Duration:51.753| Actor_Loss:3.243| Critic_Loss:0.206| Success rate:0.600| 28.8/31.3 GB RAM\n",
      "Epoch:194| Running_reward:-8.005| EP_reward:-50.000| Memory_length:14000| Duration:52.042| Actor_Loss:2.605| Critic_Loss:0.269| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:195| Running_reward:-23.118| EP_reward:-50.000| Memory_length:14000| Duration:51.331| Actor_Loss:3.028| Critic_Loss:0.175| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:196| Running_reward:-8.465| EP_reward:-13.000| Memory_length:14000| Duration:51.679| Actor_Loss:2.803| Critic_Loss:0.215| Success rate:0.900| 28.8/31.3 GB RAM\n",
      "Epoch:197| Running_reward:-6.730| EP_reward:-15.000| Memory_length:14000| Duration:51.943| Actor_Loss:2.956| Critic_Loss:0.166| Success rate:0.700| 28.6/31.3 GB RAM\n",
      "Epoch:198| Running_reward:-41.613| EP_reward:-41.000| Memory_length:14000| Duration:52.299| Actor_Loss:2.774| Critic_Loss:0.182| Success rate:0.700| 28.6/31.3 GB RAM\n",
      "Epoch:199| Running_reward:-46.544| EP_reward:-7.000| Memory_length:14000| Duration:51.873| Actor_Loss:3.329| Critic_Loss:1.812| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:200| Running_reward:-12.067| EP_reward:-13.000| Memory_length:14000| Duration:51.946| Actor_Loss:2.543| Critic_Loss:0.386| Success rate:0.900| 28.6/31.3 GB RAM\n",
      "Epoch:201| Running_reward:-13.049| EP_reward:-6.000| Memory_length:14000| Duration:51.514| Actor_Loss:2.943| Critic_Loss:0.237| Success rate:0.900| 28.6/31.3 GB RAM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:202| Running_reward:-7.236| EP_reward:-47.000| Memory_length:14000| Duration:51.685| Actor_Loss:2.701| Critic_Loss:0.236| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:203| Running_reward:-11.245| EP_reward:-9.000| Memory_length:14000| Duration:51.604| Actor_Loss:3.055| Critic_Loss:0.419| Success rate:0.900| 28.8/31.3 GB RAM\n",
      "Epoch:204| Running_reward:-47.041| EP_reward:-12.000| Memory_length:14000| Duration:51.295| Actor_Loss:3.256| Critic_Loss:0.211| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:205| Running_reward:-12.216| EP_reward:-20.000| Memory_length:14000| Duration:51.443| Actor_Loss:3.161| Critic_Loss:0.158| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:206| Running_reward:-12.115| EP_reward:-14.000| Memory_length:14000| Duration:51.906| Actor_Loss:3.374| Critic_Loss:0.280| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:207| Running_reward:-9.896| EP_reward:-9.000| Memory_length:14000| Duration:51.881| Actor_Loss:2.691| Critic_Loss:0.240| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:208| Running_reward:-7.200| EP_reward:-19.000| Memory_length:14000| Duration:51.892| Actor_Loss:2.613| Critic_Loss:0.248| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:209| Running_reward:-12.796| EP_reward:-9.000| Memory_length:14000| Duration:51.774| Actor_Loss:2.294| Critic_Loss:0.338| Success rate:1.000| 28.6/31.3 GB RAM\n",
      "Epoch:210| Running_reward:-13.951| EP_reward:-7.000| Memory_length:14000| Duration:51.861| Actor_Loss:3.177| Critic_Loss:0.280| Success rate:0.700| 28.8/31.3 GB RAM\n",
      "Epoch:211| Running_reward:-10.740| EP_reward:-10.000| Memory_length:14000| Duration:51.543| Actor_Loss:2.403| Critic_Loss:0.342| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:212| Running_reward:-32.404| EP_reward:-47.000| Memory_length:14000| Duration:51.749| Actor_Loss:2.710| Critic_Loss:0.214| Success rate:0.500| 28.7/31.3 GB RAM\n",
      "Epoch:213| Running_reward:-46.993| EP_reward:-8.000| Memory_length:14000| Duration:51.517| Actor_Loss:2.575| Critic_Loss:0.336| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:214| Running_reward:-10.829| EP_reward:-8.000| Memory_length:14000| Duration:51.695| Actor_Loss:2.771| Critic_Loss:0.687| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:215| Running_reward:-13.157| EP_reward:-11.000| Memory_length:14000| Duration:50.991| Actor_Loss:2.913| Critic_Loss:0.241| Success rate:0.900| 28.6/31.3 GB RAM\n",
      "Epoch:216| Running_reward:-14.765| EP_reward:-8.000| Memory_length:14000| Duration:51.236| Actor_Loss:2.780| Critic_Loss:0.148| Success rate:0.700| 28.7/31.3 GB RAM\n",
      "Epoch:217| Running_reward:-4.408| EP_reward:-12.000| Memory_length:14000| Duration:51.739| Actor_Loss:3.502| Critic_Loss:0.246| Success rate:1.000| 28.8/31.3 GB RAM\n",
      "Epoch:218| Running_reward:-7.940| EP_reward:-10.000| Memory_length:14000| Duration:51.333| Actor_Loss:3.389| Critic_Loss:0.365| Success rate:0.800| 28.7/31.3 GB RAM\n",
      "Epoch:219| Running_reward:-11.476| EP_reward:-6.000| Memory_length:14000| Duration:52.021| Actor_Loss:2.919| Critic_Loss:0.245| Success rate:0.600| 28.8/31.3 GB RAM\n",
      "Epoch:220| Running_reward:-16.083| EP_reward:-8.000| Memory_length:14000| Duration:51.717| Actor_Loss:2.972| Critic_Loss:0.218| Success rate:0.900| 28.6/31.3 GB RAM\n",
      "Epoch:221| Running_reward:-6.880| EP_reward:-7.000| Memory_length:14000| Duration:51.239| Actor_Loss:3.167| Critic_Loss:0.186| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:222| Running_reward:-11.264| EP_reward:-8.000| Memory_length:14000| Duration:51.498| Actor_Loss:3.018| Critic_Loss:0.460| Success rate:0.900| 28.8/31.3 GB RAM\n",
      "Epoch:223| Running_reward:-7.150| EP_reward:-14.000| Memory_length:14000| Duration:51.814| Actor_Loss:2.408| Critic_Loss:0.238| Success rate:0.800| 28.9/31.3 GB RAM\n",
      "Epoch:224| Running_reward:-7.477| EP_reward:-12.000| Memory_length:14000| Duration:51.859| Actor_Loss:2.479| Critic_Loss:0.583| Success rate:0.900| 28.9/31.3 GB RAM\n",
      "Epoch:225| Running_reward:-47.062| EP_reward:-5.000| Memory_length:14000| Duration:51.388| Actor_Loss:3.457| Critic_Loss:0.362| Success rate:0.700| 28.6/31.3 GB RAM\n",
      "Epoch:226| Running_reward:-47.649| EP_reward:-50.000| Memory_length:14000| Duration:51.926| Actor_Loss:3.043| Critic_Loss:0.339| Success rate:0.600| 28.7/31.3 GB RAM\n",
      "Epoch:227| Running_reward:-5.276| EP_reward:-12.000| Memory_length:14000| Duration:51.610| Actor_Loss:3.240| Critic_Loss:0.361| Success rate:0.800| 28.7/31.3 GB RAM\n",
      "Epoch:228| Running_reward:-10.323| EP_reward:-9.000| Memory_length:14000| Duration:51.477| Actor_Loss:2.418| Critic_Loss:0.483| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:229| Running_reward:-46.427| EP_reward:-10.000| Memory_length:14000| Duration:52.398| Actor_Loss:2.695| Critic_Loss:0.149| Success rate:0.800| 28.8/31.3 GB RAM\n",
      "Epoch:230| Running_reward:-17.696| EP_reward:-7.000| Memory_length:14000| Duration:51.843| Actor_Loss:2.107| Critic_Loss:0.182| Success rate:0.600| 28.7/31.3 GB RAM\n",
      "Epoch:231| Running_reward:-6.425| EP_reward:-11.000| Memory_length:14000| Duration:51.539| Actor_Loss:2.450| Critic_Loss:0.825| Success rate:1.000| 28.7/31.3 GB RAM\n",
      "Epoch:232| Running_reward:-14.804| EP_reward:-11.000| Memory_length:14000| Duration:51.477| Actor_Loss:2.692| Critic_Loss:0.405| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:233| Running_reward:-11.140| EP_reward:-47.000| Memory_length:14000| Duration:52.027| Actor_Loss:2.671| Critic_Loss:0.178| Success rate:0.700| 28.7/31.3 GB RAM\n",
      "Epoch:234| Running_reward:-46.386| EP_reward:-15.000| Memory_length:14000| Duration:52.066| Actor_Loss:2.528| Critic_Loss:0.345| Success rate:0.700| 28.9/31.3 GB RAM\n",
      "Epoch:235| Running_reward:-11.034| EP_reward:-44.000| Memory_length:14000| Duration:52.381| Actor_Loss:3.010| Critic_Loss:0.359| Success rate:0.900| 28.7/31.3 GB RAM\n",
      "Epoch:236| Running_reward:-11.150| EP_reward:-7.000| Memory_length:14000| Duration:51.771| Actor_Loss:2.818| Critic_Loss:0.564| Success rate:0.700| 28.9/31.3 GB RAM\n",
      "Epoch:237| Running_reward:-14.477| EP_reward:-10.000| Memory_length:14000| Duration:51.784| Actor_Loss:2.933| Critic_Loss:0.236| Success rate:0.800| 28.9/31.3 GB RAM\n",
      "Epoch:238| Running_reward:-5.365| EP_reward:-13.000| Memory_length:14000| Duration:51.672| Actor_Loss:2.476| Critic_Loss:0.403| Success rate:1.000| 28.7/31.3 GB RAM\n",
      "Epoch:239| Running_reward:-11.344| EP_reward:-12.000| Memory_length:14000| Duration:51.754| Actor_Loss:2.493| Critic_Loss:0.359| Success rate:0.900| 28.9/31.3 GB RAM\n",
      "Epoch:240| Running_reward:-11.206| EP_reward:-9.000| Memory_length:14000| Duration:52.113| Actor_Loss:2.453| Critic_Loss:0.315| Success rate:0.900| 28.8/31.3 GB RAM\n",
      "Epoch:241| Running_reward:-44.001| EP_reward:-42.000| Memory_length:14000| Duration:51.757| Actor_Loss:2.483| Critic_Loss:0.240| Success rate:0.600| 28.9/31.3 GB RAM\n",
      "Epoch:242| Running_reward:-9.978| EP_reward:-13.000| Memory_length:14000| Duration:51.927| Actor_Loss:2.662| Critic_Loss:0.217| Success rate:1.000| 28.7/31.3 GB RAM\n",
      "Epoch:243| Running_reward:-45.132| EP_reward:-14.000| Memory_length:14000| Duration:51.549| Actor_Loss:2.481| Critic_Loss:0.285| Success rate:0.800| 28.9/31.3 GB RAM\n",
      "Epoch:244| Running_reward:-43.186| EP_reward:-13.000| Memory_length:14000| Duration:51.835| Actor_Loss:2.228| Critic_Loss:0.166| Success rate:0.800| 28.7/31.3 GB RAM\n",
      "Epoch:245| Running_reward:-43.652| EP_reward:-8.000| Memory_length:14000| Duration:51.696| Actor_Loss:2.537| Critic_Loss:0.242| Success rate:0.700| 28.9/31.3 GB RAM\n",
      "Epoch:246| Running_reward:-9.130| EP_reward:-11.000| Memory_length:14000| Duration:54.056| Actor_Loss:2.583| Critic_Loss:0.142| Success rate:1.000| 28.7/31.3 GB RAM\n",
      "Epoch:247| Running_reward:-11.882| EP_reward:-10.000| Memory_length:14000| Duration:52.021| Actor_Loss:2.527| Critic_Loss:0.232| Success rate:1.000| 28.8/31.3 GB RAM\n",
      "Epoch:248| Running_reward:-6.130| EP_reward:-50.000| Memory_length:14000| Duration:52.646| Actor_Loss:2.803| Critic_Loss:0.171| Success rate:0.800| 28.6/31.3 GB RAM\n",
      "Epoch:249| Running_reward:-10.334| EP_reward:-10.000| Memory_length:14000| Duration:51.889| Actor_Loss:2.394| Critic_Loss:0.361| Success rate:0.900| 28.6/31.3 GB RAM\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "# from agent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "# from play import Play\n",
    "import mujoco_py\n",
    "import random\n",
    "from mpi4py import MPI\n",
    "import psutil\n",
    "import time\n",
    "from copy import deepcopy as dc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ENV_NAME = \"FetchPickAndPlace-v1\"\n",
    "INTRO = False\n",
    "Train = True\n",
    "Play_FLAG = False\n",
    "MAX_EPOCHS = 250\n",
    "MAX_CYCLES = 50\n",
    "num_updates = 40\n",
    "MAX_EPISODES = 2\n",
    "memory_size = 7e+5 // 50\n",
    "batch_size = 256\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-3\n",
    "gamma = 0.98\n",
    "tau = 0.05\n",
    "k_future = 4\n",
    "\n",
    "test_env = gym.make(ENV_NAME)\n",
    "state_shape = test_env.observation_space.spaces[\"observation\"].shape\n",
    "n_actions = test_env.action_space.shape[0]\n",
    "n_goals = test_env.observation_space.spaces[\"desired_goal\"].shape[0]\n",
    "action_bounds = [test_env.action_space.low[0], test_env.action_space.high[0]]\n",
    "to_gb = lambda in_bytes: in_bytes / 1024 / 1024 / 1024\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['IN_MPI'] = '1'\n",
    "\n",
    "\n",
    "def eval_agent(env_, agent_):\n",
    "    total_success_rate = []\n",
    "    running_r = []\n",
    "    for ep in range(10):\n",
    "        per_success_rate = []\n",
    "        env_dictionary = env_.reset()\n",
    "        s = env_dictionary[\"observation\"]\n",
    "        ag = env_dictionary[\"achieved_goal\"]\n",
    "        g = env_dictionary[\"desired_goal\"]\n",
    "        while np.linalg.norm(ag - g) <= 0.05:\n",
    "            env_dictionary = env_.reset()\n",
    "            s = env_dictionary[\"observation\"]\n",
    "            ag = env_dictionary[\"achieved_goal\"]\n",
    "            g = env_dictionary[\"desired_goal\"]\n",
    "        ep_r = 0\n",
    "        for t in range(50):\n",
    "            with torch.no_grad():\n",
    "                a = agent_.choose_action(s, g, train_mode=False)\n",
    "            observation_new, r, _, info_ = env_.step(a)\n",
    "            s = observation_new['observation']\n",
    "            g = observation_new['desired_goal']\n",
    "            per_success_rate.append(info_['is_success'])\n",
    "            ep_r += r\n",
    "        total_success_rate.append(per_success_rate)\n",
    "        if ep == 0:\n",
    "            running_r.append(ep_r)\n",
    "        else:\n",
    "            running_r.append(running_r[-1] * 0.99 + 0.01 * ep_r)\n",
    "    total_success_rate = np.array(total_success_rate)\n",
    "    local_success_rate = np.mean(total_success_rate[:, -1])\n",
    "    global_success_rate = MPI.COMM_WORLD.allreduce(local_success_rate, op=MPI.SUM)\n",
    "    return global_success_rate / MPI.COMM_WORLD.Get_size(), running_r, ep_r\n",
    "\n",
    "\n",
    "if INTRO:\n",
    "    print(f\"state_shape:{state_shape[0]}\\n\"\n",
    "          f\"number of actions:{n_actions}\\n\"\n",
    "          f\"action boundaries:{action_bounds}\\n\"\n",
    "          f\"max timesteps:{test_env._max_episode_steps}\")\n",
    "    for _ in range(3):\n",
    "        done = False\n",
    "        test_env.reset()\n",
    "        while not done:\n",
    "            action = test_env.action_space.sample()\n",
    "            test_state, test_reward, test_done, test_info = test_env.step(action)\n",
    "            # substitute_goal = test_state[\"achieved_goal\"].copy()\n",
    "            # substitute_reward = test_env.compute_reward(\n",
    "            #     test_state[\"achieved_goal\"], substitute_goal, test_info)\n",
    "            # print(\"r is {}, substitute_reward is {}\".format(r, substitute_reward))\n",
    "            test_env.render()\n",
    "    exit(0)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env.seed(MPI.COMM_WORLD.Get_rank())\n",
    "random.seed(MPI.COMM_WORLD.Get_rank())\n",
    "np.random.seed(MPI.COMM_WORLD.Get_rank())\n",
    "torch.manual_seed(MPI.COMM_WORLD.Get_rank())\n",
    "agent = Agent(n_states=state_shape,\n",
    "              n_actions=n_actions,\n",
    "              n_goals=n_goals,\n",
    "              action_bounds=action_bounds,\n",
    "              capacity=memory_size,\n",
    "              action_size=n_actions,\n",
    "              batch_size=batch_size,\n",
    "              actor_lr=actor_lr,\n",
    "              critic_lr=critic_lr,\n",
    "              gamma=gamma,\n",
    "              tau=tau,\n",
    "              k_future=k_future,\n",
    "              env=dc(env))\n",
    "if Train:\n",
    "\n",
    "    t_success_rate = []\n",
    "    total_ac_loss = []\n",
    "    total_cr_loss = []\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        epoch_actor_loss = 0\n",
    "        epoch_critic_loss = 0\n",
    "        for cycle in range(0, MAX_CYCLES):\n",
    "            mb = []\n",
    "            cycle_actor_loss = 0\n",
    "            cycle_critic_loss = 0\n",
    "            for episode in range(MAX_EPISODES):\n",
    "                episode_dict = {\n",
    "                    \"state\": [],\n",
    "                    \"action\": [],\n",
    "                    \"info\": [],\n",
    "                    \"achieved_goal\": [],\n",
    "                    \"desired_goal\": [],\n",
    "                    \"next_state\": [],\n",
    "                    \"next_achieved_goal\": []}\n",
    "                env_dict = env.reset()\n",
    "                state = env_dict[\"observation\"]\n",
    "                achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                desired_goal = env_dict[\"desired_goal\"]\n",
    "                while np.linalg.norm(achieved_goal - desired_goal) <= 0.05:\n",
    "                    env_dict = env.reset()\n",
    "                    state = env_dict[\"observation\"]\n",
    "                    achieved_goal = env_dict[\"achieved_goal\"]\n",
    "                    desired_goal = env_dict[\"desired_goal\"]\n",
    "                for t in range(500):\n",
    "                    action = agent.choose_action(state, desired_goal)\n",
    "                    next_env_dict, reward, done, info = env.step(action)\n",
    "\n",
    "                    next_state = next_env_dict[\"observation\"]\n",
    "                    next_achieved_goal = next_env_dict[\"achieved_goal\"]\n",
    "                    next_desired_goal = next_env_dict[\"desired_goal\"]\n",
    "\n",
    "                    episode_dict[\"state\"].append(state.copy())\n",
    "                    episode_dict[\"action\"].append(action.copy())\n",
    "                    episode_dict[\"achieved_goal\"].append(achieved_goal.copy())\n",
    "                    episode_dict[\"desired_goal\"].append(desired_goal.copy())\n",
    "\n",
    "                    state = next_state.copy()\n",
    "                    achieved_goal = next_achieved_goal.copy()\n",
    "                    desired_goal = next_desired_goal.copy()\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                episode_dict[\"state\"].append(state.copy())\n",
    "                episode_dict[\"achieved_goal\"].append(achieved_goal.copy())\n",
    "                episode_dict[\"desired_goal\"].append(desired_goal.copy())\n",
    "                episode_dict[\"next_state\"] = episode_dict[\"state\"][1:]\n",
    "                episode_dict[\"next_achieved_goal\"] = episode_dict[\"achieved_goal\"][1:]\n",
    "                mb.append(dc(episode_dict))\n",
    "\n",
    "            agent.store(mb)\n",
    "            for n_update in range(num_updates):\n",
    "                actor_loss, critic_loss = agent.train()\n",
    "                cycle_actor_loss += actor_loss\n",
    "                cycle_critic_loss += critic_loss\n",
    "\n",
    "            epoch_actor_loss += cycle_actor_loss / num_updates\n",
    "            epoch_critic_loss += cycle_critic_loss /num_updates\n",
    "            agent.update_networks()\n",
    "\n",
    "        ram = psutil.virtual_memory()\n",
    "        success_rate, running_reward, episode_reward = eval_agent(env, agent)\n",
    "        total_ac_loss.append(epoch_actor_loss)\n",
    "        total_cr_loss.append(epoch_critic_loss)\n",
    "        if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "            t_success_rate.append(success_rate)\n",
    "            print(f\"Epoch:{epoch}| \"\n",
    "                  f\"Running_reward:{running_reward[-1]:.3f}| \"\n",
    "                  f\"EP_reward:{episode_reward:.3f}| \"\n",
    "                  f\"Memory_length:{len(agent.memory)}| \"\n",
    "                  f\"Duration:{time.time() - start_time:.3f}| \"\n",
    "                  f\"Actor_Loss:{actor_loss:.3f}| \"\n",
    "                  f\"Critic_Loss:{critic_loss:.3f}| \"\n",
    "                  f\"Success rate:{success_rate:.3f}| \"\n",
    "                  f\"{to_gb(ram.used):.1f}/{to_gb(ram.total):.1f} GB RAM\")\n",
    "            agent.save_weights()\n",
    "\n",
    "    if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "\n",
    "        with SummaryWriter(\"logs\") as writer:\n",
    "            for i, success_rate in enumerate(t_success_rate):\n",
    "                writer.add_scalar(\"Success_rate\", success_rate, i)\n",
    "\n",
    "        plt.style.use('ggplot')\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(0, MAX_EPOCHS), t_success_rate)\n",
    "        plt.title(\"Success rate\")\n",
    "        plt.savefig(\"success_rate.png\")\n",
    "        plt.show()\n",
    "\n",
    "elif Play_FLAG:\n",
    "    player = Play(env, agent, max_episode=100)\n",
    "    player.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_reward:-8.000\n",
      "episode_reward:-50.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-47.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-6.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-50.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-12.000\n",
      "episode_reward:-16.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-50.000\n",
      "episode_reward:-50.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-49.000\n",
      "episode_reward:-6.000\n",
      "episode_reward:-13.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-46.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-13.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-6.000\n",
      "episode_reward:-48.000\n",
      "episode_reward:-14.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-50.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-6.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-13.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-50.000\n",
      "episode_reward:-48.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-46.000\n",
      "episode_reward:-12.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-13.000\n",
      "episode_reward:-6.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-45.000\n",
      "episode_reward:-45.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-5.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-13.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-50.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-48.000\n",
      "episode_reward:-44.000\n",
      "episode_reward:-45.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-4.000\n",
      "episode_reward:-8.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-14.000\n",
      "episode_reward:-12.000\n",
      "episode_reward:-7.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-50.000\n",
      "episode_reward:-11.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-5.000\n",
      "episode_reward:-12.000\n",
      "episode_reward:-9.000\n",
      "episode_reward:-10.000\n",
      "episode_reward:-6.000\n",
      "episode_reward:-10.000\n"
     ]
    }
   ],
   "source": [
    "player = Play(env, agent, max_episode=100)\n",
    "player.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise\n",
    "\n",
    "Use DDPG in two different OpenAI gym environments.\n",
    "- Do not use FetchPickandPlace\n",
    "- Write in the report what you did.\n",
    "- Take the vdo and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
