{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: MuZero\n",
    "\n",
    "In this lab, we explain about the reinforcement learning method which claim that it is the prototype of AGI (Artificial General Intelligence): MuZero.\n",
    "\n",
    "Reference:\n",
    "- https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules\n",
    "- https://github.com/werner-duvaud/muzero-general\n",
    "- https://medium.com/applied-data-science/how-to-build-your-own-muzero-in-python-f77d5718061a\n",
    "- https://arxiv.org/src/1911.08265v1/anc/pseudocode.py\n",
    "- https://github.com/suragnair/alpha-zero-general\n",
    "- https://arxiv.org/pdf/1911.08265.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial General Intelligence (AGI)\n",
    "\n",
    "Currently, the AIs that can do tasks for solving only one problem such as classification, object detection, and text translation are called \"Narrow AI\". It is not intended to have general cognitive abilities because it cannot do other tasks without \"modifying\" their models. Thus, there is a concept which AI can do many problems as human can do -- That is call \"General AI\".\n",
    "\n",
    "**Artificial general intelligence (AGI)** is the hypothetical ability of an intelligent agent to understand or learn any intellectual task that a human being can.\n",
    "It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies.\n",
    "AGI can also be referred to as strong AI, full AI, or general intelligent action (although some academic sources reserve the term \"strong AI\" for computer programs that experience sentience or consciousness.)\n",
    "\n",
    "A 2020 survey identified 72 active AGI R&D projects spread across 37 countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MuZero\n",
    "\n",
    "In 2016, Deepmind introduced AlphaGo, the first artificial intelligence (AI) program to defeat humans at the ancient game of Go that can be a high rank professional Go player named Lee Sedol. Two years later, they released AlphaZero that can learn from scratch to master of three games: Go, chess and shogi. In 2020, they described MuZero, a significant step forward in the pursuit of general-purpose algorithms. MuZero has an advantage that it can learn any games without needing to be told the rules.\n",
    "\n",
    "MuZero, first introduced in a preliminary paper in 2019, solves this problem by learning a model that focuses only on the most important aspects of the environment for planning. By combining this model with AlphaZero’s powerful lookahead tree search, MuZero set a new state of the art result on the Atari benchmark, while simultaneously matching the performance of AlphaZero in the classic planning challenges of Go, chess and shogi. In doing so, MuZero demonstrates a significant leap forward in the capabilities of reinforcement learning algorithms.\n",
    "\n",
    "<img src=\"img/alphago_summary.jpeg\" title=\"alphago_summary\" style=\"width: 800px;\" />\n",
    "Ref: https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MuZero vs. AlphaZero\n",
    "\n",
    "AlphaZero is set the challenge as learn how to play this game on your own with the given rules of how piece moves and which move are legal.\n",
    "\n",
    "MuZero is set the challenge as learn how to play this game on your own without rules. We just tell it what moves are legal in the current position and when one side has won (or it’s a draw).\n",
    "\n",
    "Alongside developing winning strategies, MuZero must develop its own dynamic model of the environment so that it can understand the implications of its choices and plan ahead. Imagine trying to become better than the world champion at a game where you are never told the rules. MuZero achieves precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalising to unknown models\n",
    "\n",
    "The major challenge in AI are two main approaches: lookahead search or model-based planning.\n",
    "\n",
    "Systems that use lookahead search have achieved remarkable success in classic games such as checkers, chess and poker, but we need to give knowledge of their environment’s dynamics, such as the rules of the game or an accurate simulator. This makes it difficult to apply them to the real world problems. The example of lookahead search are AlphaGo, AlphaZero.\n",
    "\n",
    "Model-based systems aim to address this issue by learning an accurate model of an environment’s dynamics, and then using it to plan. However, the complexity of modelling every aspect of an environment has meant these algorithms are unable to compete in visually rich domains, such as Atari.  Until now, the best results on Atari are from model-free systems, such as DQN. As the name suggests, model-free algorithms do not use a learned model and instead estimate what is the best action to take next.\n",
    "\n",
    "MuZero uses a different approach to overcome the limitations of previous approaches. Instead of trying to model the entire environment, MuZero just models aspects that are important to the agent’s decision-making process.\n",
    "\n",
    "MuZero models three elements of the environment that are critical to planning:\n",
    "\n",
    "- Value: how good is the current position?\n",
    "- Policy: which action is the best to take?\n",
    "- Reward: how good was the last action?\n",
    "\n",
    "These are all learned using a deep neural network and are all that is needed for MuZero to understand what happens when it takes a certain action and to plan accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "Alongside the [MuZero paper](https://arxiv.org/pdf/1911.08265.pdf), DeepMind have released [Python pseudocode]((https://arxiv.org/src/1911.08265v1/anc/pseudocode.py)) detailing the interactions between each part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lint as: python3\n",
    "\"\"\"Pseudocode description of the MuZero algorithm.\"\"\"\n",
    "# pylint: disable=unused-argument\n",
    "# pylint: disable=missing-docstring\n",
    "# pylint: disable=g-explicit-length-test\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import google_type_annotations\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import typing\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "##########################\n",
    "####### Helpers ##########\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float('inf')\n",
    "\n",
    "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "  \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
    "\n",
    "  def __init__(self, known_bounds: Optional[KnownBounds]):\n",
    "    self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "    self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "  def update(self, value: float):\n",
    "    self.maximum = max(self.maximum, value)\n",
    "    self.minimum = min(self.minimum, value)\n",
    "\n",
    "  def normalize(self, value: float) -> float:\n",
    "    if self.maximum > self.minimum:\n",
    "      # We normalize only when we have set the maximum and minimum values.\n",
    "      return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "    return value\n",
    "\n",
    "\n",
    "class MuZeroConfig(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               action_space_size: int,\n",
    "               max_moves: int,\n",
    "               discount: float,\n",
    "               dirichlet_alpha: float,\n",
    "               num_simulations: int,\n",
    "               batch_size: int,\n",
    "               td_steps: int,\n",
    "               num_actors: int,\n",
    "               lr_init: float,\n",
    "               lr_decay_steps: float,\n",
    "               visit_softmax_temperature_fn,\n",
    "               known_bounds: Optional[KnownBounds] = None):\n",
    "    ### Self-Play\n",
    "    self.action_space_size = action_space_size\n",
    "    self.num_actors = num_actors\n",
    "\n",
    "    self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
    "    self.max_moves = max_moves\n",
    "    self.num_simulations = num_simulations\n",
    "    self.discount = discount\n",
    "\n",
    "    # Root prior exploration noise.\n",
    "    self.root_dirichlet_alpha = dirichlet_alpha\n",
    "    self.root_exploration_fraction = 0.25\n",
    "\n",
    "    # UCB formula\n",
    "    self.pb_c_base = 19652\n",
    "    self.pb_c_init = 1.25\n",
    "\n",
    "    # If we already have some information about which values occur in the\n",
    "    # environment, we can use them to initialize the rescaling.\n",
    "    # This is not strictly necessary, but establishes identical behaviour to\n",
    "    # AlphaZero in board games.\n",
    "    self.known_bounds = known_bounds\n",
    "\n",
    "    ### Training\n",
    "    self.training_steps = int(1000e3)\n",
    "    self.checkpoint_interval = int(1e3)\n",
    "    self.window_size = int(1e6)\n",
    "    self.batch_size = batch_size\n",
    "    self.num_unroll_steps = 5\n",
    "    self.td_steps = td_steps\n",
    "\n",
    "    self.weight_decay = 1e-4\n",
    "    self.momentum = 0.9\n",
    "\n",
    "    # Exponential learning rate schedule\n",
    "    self.lr_init = lr_init\n",
    "    self.lr_decay_rate = 0.1\n",
    "    self.lr_decay_steps = lr_decay_steps\n",
    "\n",
    "  def new_game(self):\n",
    "    return Game(self.action_space_size, self.discount)\n",
    "\n",
    "\n",
    "def make_board_game_config(action_space_size: int, max_moves: int,\n",
    "                           dirichlet_alpha: float,\n",
    "                           lr_init: float) -> MuZeroConfig:\n",
    "\n",
    "  def visit_softmax_temperature(num_moves, training_steps):\n",
    "    if num_moves < 30:\n",
    "      return 1.0\n",
    "    else:\n",
    "      return 0.0  # Play according to the max.\n",
    "\n",
    "  return MuZeroConfig(\n",
    "      action_space_size=action_space_size,\n",
    "      max_moves=max_moves,\n",
    "      discount=1.0,\n",
    "      dirichlet_alpha=dirichlet_alpha,\n",
    "      num_simulations=800,\n",
    "      batch_size=2048,\n",
    "      td_steps=max_moves,  # Always use Monte Carlo return.\n",
    "      num_actors=3000,\n",
    "      lr_init=lr_init,\n",
    "      lr_decay_steps=400e3,\n",
    "      visit_softmax_temperature_fn=visit_softmax_temperature,\n",
    "      known_bounds=KnownBounds(-1, 1))\n",
    "\n",
    "\n",
    "def make_go_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=362, max_moves=722, dirichlet_alpha=0.03, lr_init=0.01)\n",
    "\n",
    "\n",
    "def make_chess_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=4672, max_moves=512, dirichlet_alpha=0.3, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_shogi_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=11259, max_moves=512, dirichlet_alpha=0.15, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_atari_config() -> MuZeroConfig:\n",
    "\n",
    "  def visit_softmax_temperature(num_moves, training_steps):\n",
    "    if training_steps < 500e3:\n",
    "      return 1.0\n",
    "    elif training_steps < 750e3:\n",
    "      return 0.5\n",
    "    else:\n",
    "      return 0.25\n",
    "\n",
    "  return MuZeroConfig(\n",
    "      action_space_size=18,\n",
    "      max_moves=27000,  # Half an hour at action repeat 4.\n",
    "      discount=0.997,\n",
    "      dirichlet_alpha=0.25,\n",
    "      num_simulations=50,\n",
    "      batch_size=1024,\n",
    "      td_steps=10,\n",
    "      num_actors=350,\n",
    "      lr_init=0.05,\n",
    "      lr_decay_steps=350e3,\n",
    "      visit_softmax_temperature_fn=visit_softmax_temperature)\n",
    "\n",
    "\n",
    "class Action(object):\n",
    "\n",
    "  def __init__(self, index: int):\n",
    "    self.index = index\n",
    "\n",
    "  def __hash__(self):\n",
    "    return self.index\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    return self.index == other.index\n",
    "\n",
    "  def __gt__(self, other):\n",
    "    return self.index > other.index\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "  pass\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "  def __init__(self, prior: float):\n",
    "    self.visit_count = 0\n",
    "    self.to_play = -1\n",
    "    self.prior = prior\n",
    "    self.value_sum = 0\n",
    "    self.children = {}\n",
    "    self.hidden_state = None\n",
    "    self.reward = 0\n",
    "\n",
    "  def expanded(self) -> bool:\n",
    "    return len(self.children) > 0\n",
    "\n",
    "  def value(self) -> float:\n",
    "    if self.visit_count == 0:\n",
    "      return 0\n",
    "    return self.value_sum / self.visit_count\n",
    "\n",
    "\n",
    "class ActionHistory(object):\n",
    "  \"\"\"Simple history container used inside the search.\n",
    "\n",
    "  Only used to keep track of the actions executed.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, history: List[Action], action_space_size: int):\n",
    "    self.history = list(history)\n",
    "    self.action_space_size = action_space_size\n",
    "\n",
    "  def clone(self):\n",
    "    return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "  def add_action(self, action: Action):\n",
    "    self.history.append(action)\n",
    "\n",
    "  def last_action(self) -> Action:\n",
    "    return self.history[-1]\n",
    "\n",
    "  def action_space(self) -> List[Action]:\n",
    "    return [Action(i) for i in range(self.action_space_size)]\n",
    "\n",
    "  def to_play(self) -> Player:\n",
    "    return Player()\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "  \"\"\"The environment MuZero is interacting with.\"\"\"\n",
    "\n",
    "  def step(self, action):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Game(object):\n",
    "  \"\"\"A single episode of interaction with the environment.\"\"\"\n",
    "\n",
    "  def __init__(self, action_space_size: int, discount: float):\n",
    "    self.environment = Environment()  # Game specific environment.\n",
    "    self.history = []\n",
    "    self.rewards = []\n",
    "    self.child_visits = []\n",
    "    self.root_values = []\n",
    "    self.action_space_size = action_space_size\n",
    "    self.discount = discount\n",
    "\n",
    "  def terminal(self) -> bool:\n",
    "    # Game specific termination rules.\n",
    "    pass\n",
    "\n",
    "  def legal_actions(self) -> List[Action]:\n",
    "    # Game specific calculation of legal actions.\n",
    "    return []\n",
    "\n",
    "  def apply(self, action: Action):\n",
    "    reward = self.environment.step(action)\n",
    "    self.rewards.append(reward)\n",
    "    self.history.append(action)\n",
    "\n",
    "  def store_search_statistics(self, root: Node):\n",
    "    sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "    action_space = (Action(index) for index in range(self.action_space_size))\n",
    "    self.child_visits.append([\n",
    "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
    "        for a in action_space\n",
    "    ])\n",
    "    self.root_values.append(root.value())\n",
    "\n",
    "  def make_image(self, state_index: int):\n",
    "    # Game specific feature planes.\n",
    "    return []\n",
    "\n",
    "  def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,\n",
    "                  to_play: Player):\n",
    "    # The value target is the discounted root value of the search tree N steps\n",
    "    # into the future, plus the discounted sum of all rewards until then.\n",
    "    targets = []\n",
    "    for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
    "      bootstrap_index = current_index + td_steps\n",
    "      if bootstrap_index < len(self.root_values):\n",
    "        value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
    "      else:\n",
    "        value = 0\n",
    "\n",
    "      for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
    "        value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
    "\n",
    "      if current_index < len(self.root_values):\n",
    "        targets.append((value, self.rewards[current_index],\n",
    "                        self.child_visits[current_index]))\n",
    "      else:\n",
    "        # States past the end of games are treated as absorbing states.\n",
    "        targets.append((0, 0, []))\n",
    "    return targets\n",
    "\n",
    "  def to_play(self) -> Player:\n",
    "    return Player()\n",
    "\n",
    "  def action_history(self) -> ActionHistory:\n",
    "    return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, config: MuZeroConfig):\n",
    "    self.window_size = config.window_size\n",
    "    self.batch_size = config.batch_size\n",
    "    self.buffer = []\n",
    "\n",
    "  def save_game(self, game):\n",
    "    if len(self.buffer) > self.window_size:\n",
    "      self.buffer.pop(0)\n",
    "    self.buffer.append(game)\n",
    "\n",
    "  def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
    "    games = [self.sample_game() for _ in range(self.batch_size)]\n",
    "    game_pos = [(g, self.sample_position(g)) for g in games]\n",
    "    return [(g.make_image(i), g.history[i:i + num_unroll_steps],\n",
    "             g.make_target(i, num_unroll_steps, td_steps, g.to_play()))\n",
    "            for (g, i) in game_pos]\n",
    "\n",
    "  def sample_game(self) -> Game:\n",
    "    # Sample game from buffer either uniformly or according to some priority.\n",
    "    return self.buffer[0]\n",
    "\n",
    "  def sample_position(self, game) -> int:\n",
    "    # Sample position from game either uniformly or according to some priority.\n",
    "    return -1\n",
    "\n",
    "\n",
    "class NetworkOutput(typing.NamedTuple):\n",
    "  value: float\n",
    "  reward: float\n",
    "  policy_logits: Dict[Action, float]\n",
    "  hidden_state: List[float]\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "  def initial_inference(self, image) -> NetworkOutput:\n",
    "    # representation + prediction function\n",
    "    return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "  def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "    # dynamics + prediction function\n",
    "    return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "  def get_weights(self):\n",
    "    # Returns the weights of this network.\n",
    "    return []\n",
    "\n",
    "  def training_steps(self) -> int:\n",
    "    # How many steps / batches the network has been trained for.\n",
    "    return 0\n",
    "\n",
    "\n",
    "class SharedStorage(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._networks = {}\n",
    "\n",
    "  def latest_network(self) -> Network:\n",
    "    if self._networks:\n",
    "      return self._networks[max(self._networks.keys())]\n",
    "    else:\n",
    "      # policy -> uniform, value -> 0, reward -> 0\n",
    "      return make_uniform_network()\n",
    "\n",
    "  def save_network(self, step: int, network: Network):\n",
    "    self._networks[step] = network\n",
    "\n",
    "\n",
    "##### End Helpers ########\n",
    "##########################\n",
    "\n",
    "\n",
    "# MuZero training is split into two independent parts: Network training and\n",
    "# self-play data generation.\n",
    "# These two parts only communicate by transferring the latest network checkpoint\n",
    "# from the training to the self-play, and the finished games from the self-play\n",
    "# to the training.\n",
    "def muzero(config: MuZeroConfig):\n",
    "  storage = SharedStorage()\n",
    "  replay_buffer = ReplayBuffer(config)\n",
    "\n",
    "  for _ in range(config.num_actors):\n",
    "    launch_job(run_selfplay, config, storage, replay_buffer)\n",
    "\n",
    "  train_network(config, storage, replay_buffer)\n",
    "\n",
    "  return storage.latest_network()\n",
    "\n",
    "\n",
    "##################################\n",
    "####### Part 1: Self-Play ########\n",
    "\n",
    "\n",
    "# Each self-play job is independent of all others; it takes the latest network\n",
    "# snapshot, produces a game and makes it available to the training job by\n",
    "# writing it to a shared replay buffer.\n",
    "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
    "                 replay_buffer: ReplayBuffer):\n",
    "  while True:\n",
    "    network = storage.latest_network()\n",
    "    game = play_game(config, network)\n",
    "    replay_buffer.save_game(game)\n",
    "\n",
    "\n",
    "# Each game is produced by starting at the initial board position, then\n",
    "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
    "# of the game is reached.\n",
    "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
    "  game = config.new_game()\n",
    "\n",
    "  while not game.terminal() and len(game.history) < config.max_moves:\n",
    "    # At the root of the search tree we use the representation function to\n",
    "    # obtain a hidden state given the current observation.\n",
    "    root = Node(0)\n",
    "    current_observation = game.make_image(-1)\n",
    "    expand_node(root, game.to_play(), game.legal_actions(),\n",
    "                network.initial_inference(current_observation))\n",
    "    add_exploration_noise(config, root)\n",
    "\n",
    "    # We then run a Monte Carlo Tree Search using only action sequences and the\n",
    "    # model learned by the network.\n",
    "    run_mcts(config, root, game.action_history(), network)\n",
    "    action = select_action(config, len(game.history), root, network)\n",
    "    game.apply(action)\n",
    "    game.store_search_statistics(root)\n",
    "  return game\n",
    "\n",
    "\n",
    "# Core Monte Carlo Tree Search algorithm.\n",
    "# To decide on an action, we run N simulations, always starting at the root of\n",
    "# the search tree and traversing the tree according to the UCB formula until we\n",
    "# reach a leaf node.\n",
    "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
    "             network: Network):\n",
    "  min_max_stats = MinMaxStats(config.known_bounds)\n",
    "\n",
    "  for _ in range(config.num_simulations):\n",
    "    history = action_history.clone()\n",
    "    node = root\n",
    "    search_path = [node]\n",
    "\n",
    "    while node.expanded():\n",
    "      action, node = select_child(config, node, min_max_stats)\n",
    "      history.add_action(action)\n",
    "      search_path.append(node)\n",
    "\n",
    "    # Inside the search tree we use the dynamics function to obtain the next\n",
    "    # hidden state given an action and the previous hidden state.\n",
    "    parent = search_path[-2]\n",
    "    network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                 history.last_action())\n",
    "    expand_node(node, history.to_play(), history.action_space(), network_output)\n",
    "\n",
    "    backpropagate(search_path, network_output.value, history.to_play(),\n",
    "                  config.discount, min_max_stats)\n",
    "\n",
    "\n",
    "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
    "                  network: Network):\n",
    "  visit_counts = [\n",
    "      (child.visit_count, action) for action, child in node.children.items()\n",
    "  ]\n",
    "  t = config.visit_softmax_temperature_fn(\n",
    "      num_moves=num_moves, training_steps=network.training_steps())\n",
    "  _, action = softmax_sample(visit_counts, t)\n",
    "  return action\n",
    "\n",
    "\n",
    "# Select the child with the highest UCB score.\n",
    "def select_child(config: MuZeroConfig, node: Node,\n",
    "                 min_max_stats: MinMaxStats):\n",
    "  _, action, child = max(\n",
    "      (ucb_score(config, node, child, min_max_stats), action,\n",
    "       child) for action, child in node.children.items())\n",
    "  return action, child\n",
    "\n",
    "\n",
    "# The score for a node is based on its value, plus an exploration bonus based on\n",
    "# the prior.\n",
    "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\n",
    "              min_max_stats: MinMaxStats) -> float:\n",
    "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
    "                  config.pb_c_base) + config.pb_c_init\n",
    "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "  prior_score = pb_c * child.prior\n",
    "  value_score = min_max_stats.normalize(child.value())\n",
    "  return prior_score + value_score\n",
    "\n",
    "\n",
    "# We expand a node using the value, reward and policy prediction obtained from\n",
    "# the neural network.\n",
    "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
    "                network_output: NetworkOutput):\n",
    "  node.to_play = to_play\n",
    "  node.hidden_state = network_output.hidden_state\n",
    "  node.reward = network_output.reward\n",
    "  policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "  policy_sum = sum(policy.values())\n",
    "  for action, p in policy.items():\n",
    "    node.children[action] = Node(p / policy_sum)\n",
    "\n",
    "\n",
    "# At the end of a simulation, we propagate the evaluation all the way up the\n",
    "# tree to the root.\n",
    "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
    "                  discount: float, min_max_stats: MinMaxStats):\n",
    "  for node in search_path:\n",
    "    node.value_sum += value if node.to_play == to_play else -value\n",
    "    node.visit_count += 1\n",
    "    min_max_stats.update(node.value())\n",
    "\n",
    "    value = node.reward + discount * value\n",
    "\n",
    "\n",
    "# At the start of each search, we add dirichlet noise to the prior of the root\n",
    "# to encourage the search to explore new actions.\n",
    "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "  actions = list(node.children.keys())\n",
    "  noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "  frac = config.root_exploration_fraction\n",
    "  for a, n in zip(actions, noise):\n",
    "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "\n",
    "######### End Self-Play ##########\n",
    "##################################\n",
    "\n",
    "##################################\n",
    "####### Part 2: Training #########\n",
    "\n",
    "\n",
    "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
    "                  replay_buffer: ReplayBuffer):\n",
    "  network = Network()\n",
    "  learning_rate = config.lr_init * config.lr_decay_rate**(\n",
    "      tf.train.get_global_step() / config.lr_decay_steps)\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)\n",
    "\n",
    "  for i in range(config.training_steps):\n",
    "    if i % config.checkpoint_interval == 0:\n",
    "      storage.save_network(i, network)\n",
    "    batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
    "    update_weights(optimizer, network, batch, config.weight_decay)\n",
    "  storage.save_network(config.training_steps, network)\n",
    "\n",
    "\n",
    "def update_weights(optimizer: tf.train.Optimizer, network: Network, batch,\n",
    "                   weight_decay: float):\n",
    "  loss = 0\n",
    "  for image, actions, targets in batch:\n",
    "    # Initial step, from the real observation.\n",
    "    value, reward, policy_logits, hidden_state = network.initial_inference(\n",
    "        image)\n",
    "    predictions = [(1.0, value, reward, policy_logits)]\n",
    "\n",
    "    # Recurrent steps, from action and previous hidden state.\n",
    "    for action in actions:\n",
    "      value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
    "          hidden_state, action)\n",
    "      predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
    "\n",
    "      hidden_state = tf.scale_gradient(hidden_state, 0.5)\n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "      gradient_scale, value, reward, policy_logits = prediction\n",
    "      target_value, target_reward, target_policy = target\n",
    "\n",
    "      l = (\n",
    "          scalar_loss(value, target_value) +\n",
    "          scalar_loss(reward, target_reward) +\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "              logits=policy_logits, labels=target_policy))\n",
    "\n",
    "      loss += tf.scale_gradient(l, gradient_scale)\n",
    "\n",
    "  for weights in network.get_weights():\n",
    "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "  optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "def scalar_loss(prediction, target) -> float:\n",
    "  # MSE in board games, cross entropy between categorical values in Atari.\n",
    "  return -1\n",
    "\n",
    "######### End Training ###########\n",
    "##################################\n",
    "\n",
    "################################################################################\n",
    "############################# End of pseudocode ################################\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Stubs to make the typechecker happy.\n",
    "def softmax_sample(distribution, temperature: float):\n",
    "  return 0, 0\n",
    "\n",
    "\n",
    "def launch_job(f, *args):\n",
    "  f(*args)\n",
    "\n",
    "\n",
    "def make_uniform_network():\n",
    "  return Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Muzero function\n",
    "\n",
    "Muzero function is an overview of the entire process.\n",
    "\n",
    "<img src=\"img/muzero_sum.png\" title=\"muzero sum\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muzero(config: MuZeroConfig):\n",
    "  storage = SharedStorage()\n",
    "  replay_buffer = ReplayBuffer(config)\n",
    "\n",
    "  for _ in range(config.num_actors):\n",
    "    launch_job(run_selfplay, config, storage, replay_buffer)\n",
    "\n",
    "  train_network(config, storage, replay_buffer)\n",
    "\n",
    "  return storage.latest_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point function muzero is passed a <code>MuZeroConfig</code> object, which stores important information about the parameters of the setting, such as the <code>action_space_size</code> (number of possible actions) and <code>num_actors</code> (the number of parallel game simulations to spin up).\n",
    "\n",
    "There are two independent parts to the MuZero algorithm — self-play (creating game data) and training (producing improved versions of the neural network). The SharedStorage and ReplayBuffer objects can be accessed by both halves of the algorithm and store neural network versions and game data respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Storage and the Replay Buffer\n",
    "\n",
    "The SharedStorage object contains methods for saving a version of the neural network and retrieving the latest neural network from the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedStorage(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._networks = {}\n",
    "\n",
    "  def latest_network(self) -> Network:\n",
    "    if self._networks:\n",
    "      return self._networks[max(self._networks.keys())]\n",
    "    else:\n",
    "      # policy -> uniform, value -> 0, reward -> 0\n",
    "      return make_uniform_network()\n",
    "\n",
    "  def save_network(self, step: int, network: Network):\n",
    "    self._networks[step] = network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n",
    "\n",
    "The ReplayBuffer stores data from previous games. The ReplayBuffer class contains a sample_batch method to sample a batch of observations from the buffer.\n",
    "\n",
    "The default batch_size of MuZero for chess is 2048. This number of games are selected from the buffer and one position is chosen from each.\n",
    "\n",
    "A single batch is a list of tuples, where each tuple consists of three elements:\n",
    "- g.make_image(i) — the observation at the chosen position\n",
    "- g.history[i:i + num_unroll_steps] — a list of the next num_unroll_steps actions taken after the chosen position (if they exist)\n",
    "- g.make_target(i, num_unroll_steps, td_steps, g.to_play() — a list of the targets that will be used to train the neural networks. Specifically, this is a list of tuples:target_value, target_reward and target_policy.\n",
    "\n",
    "For each observation in the batch, we will be ‘unrolling’ the position num_unroll_steps into the future using the actions provided. For the initial position, we will use the initial_inference function to predict the value, reward and policy and compare these to the target value, target reward and target policy. For subsequent actions, we will use the recurrent_inference function to predict the value, reward and policy and compare to the target value, target reward and target policy. This way, all three networks are utilised in the predictive process and therefore the weights in all three networks will be updated.\n",
    "\n",
    "During training, the model is unrolled alongside the collected experience, at each step predicting the previously saved information: the value function v predicts the sum of observed rewards (u), the policy estimate (p) predicts the previous search outcome (π), the reward estimate r predicts the last observed reward (u). This process is the overall of recurrent_inference.\n",
    "\n",
    "<img src=\"img/unroll.gif\" title=\"\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, config: MuZeroConfig):\n",
    "    self.window_size = config.window_size\n",
    "    self.batch_size = config.batch_size\n",
    "    self.buffer = []\n",
    "\n",
    "  def save_game(self, game):\n",
    "    if len(self.buffer) > self.window_size:\n",
    "      self.buffer.pop(0)\n",
    "    self.buffer.append(game)\n",
    "\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>window_size</code> parameter limits the maximum number of games stored in the buffer. In MuZero, this is set to the latest 1,000,000 games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-play (run_selfplay)\n",
    "\n",
    "MuZero launches <code>num_actors</code> parallel game environments, that run independently.\n",
    "\n",
    "In MuZero, it plays thousands of games against itself. In the process, the games have been saved to a buffer and then training itself on data from those games. In this step, it is no different from AlphaZero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
    "                 replay_buffer: ReplayBuffer):\n",
    "  while True:\n",
    "    network = storage.latest_network()\n",
    "    game = play_game(config, network)\n",
    "    replay_buffer.save_game(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 Neural Networks of MuZero\n",
    "\n",
    "Both AlphaZero and MuZero use a technique known as **Monte Carlo Tree Search (MCTS)** to select the next best move.\n",
    "\n",
    "The idea is that in order to select the next best move, it makes sense to ‘play out’ likely future scenarios from the current position, evaluate their value using a neural network and choose the action that maximises the future expected value. This seems to be what we humans are doing in our head when playing chess, and the AI is also designed to make use of this technique.\n",
    "\n",
    "#### Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "The photo below illustrate how Monte Carlo Tree Search is used to plan with the MuZero neural networks. Starting at the current position in the game (top), MuZero uses the representation function (h) to map from the observation to an embedding used by the neural network (s0). Using the dynamics function (g) and the prediction function (f), MuZero can then consider possible future sequences of actions (a), and choose the best action.\n",
    "\n",
    "<img src=\"img/mcts.gif\" title=\"MCTS\" style=\"width: 800px;\" />\n",
    "\n",
    "However, MuZero has a problem. As it **doesn’t know the rules of the game**, it has no idea how a given action will affect the game state, so it cannot imagine future scenarios in the MCTS. It doesn’t even know how to work out what moves are legal from a given position, or whether one side has won.\n",
    "\n",
    "The MuZero paper told us that this doesn’t matter. MuZero learns how to play the game by creating a dynamic model of the environment within its own imagination and optimising within this model.\n",
    "\n",
    "The diagram below shows a comparison between the MCTS processes in AlphaZero and MuZero:\n",
    "\n",
    "<img src=\"img/alphagovsmuzero.png\" title=\"AlphaZero VS MuZero\" style=\"width: 800px;\" />\n",
    "\n",
    "For the network models, AlphaZero has only one **prediction** network, but Muzero has three networks: prediction, dynamics, representation.\n",
    "\n",
    "#### AlphaZero Networks\n",
    "\n",
    "The AlphaZero prediction neural network $f$ is to predict the policy $p$ and value $v$ of a given game state. The policy is a probability distribution over all moves and the value is just a single number that estimates the future rewards. This prediction is made every time the MCTS hits an unexplored leaf node, so that it can immediately assign an estimated value to the new position and also assign a probability to each subsequent action. The values are backfilled up the tree, back to the root node, so that after many simulations, the root node has a good idea of the future value of the current state, having explored lots of different possible futures.\n",
    "\n",
    "#### MuZero Networks\n",
    "\n",
    "As mention above, MuZero has 3 networks\n",
    "\n",
    "1. Represenation: To operate from *state input* to be *hidden state*. The hidden state is looked like the features state or function approximate state for the real input to prediction network.\n",
    "2. Prediction: Same as AlphaZero\n",
    "3. Dynamics: Taking the current hidden state $s$ and chosen action $a$ and outputs a reward $r$ and new state.\n",
    "\n",
    "\n",
    "<img src=\"img/muzero-network.png\" title=\"MuZero Network\" style=\"width: 800px;\" />\n",
    "\n",
    "\n",
    "There are therefore two inference functions MuZero needs, in order to move through the MCTS tree making predictions:\n",
    "- initial_inference for the current state. h followed by f (representation followed by prediction) .\n",
    "- recurrent_inference for moving between states inside the MCTS tree.g followed by f (representation followed by dynamics).\n",
    "\n",
    "MuZero uses the experience it collects when interacting with the environment to train its neural network. This experience includes both observations and rewards from the environment, as well as the results of searches performed when deciding on the best action. \n",
    "\n",
    "<img src=\"img/experience.gif\" title=\"\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkOutput(typing.NamedTuple):\n",
    "  value: float\n",
    "  reward: float\n",
    "  policy_logits: Dict[Action, float]\n",
    "  hidden_state: List[float]\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "  def initial_inference(self, image) -> NetworkOutput:\n",
    "    # representation + prediction function\n",
    "    return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "  def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "    # dynamics + prediction function\n",
    "    return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "  def get_weights(self):\n",
    "    # Returns the weights of this network.\n",
    "    return []\n",
    "\n",
    "  def training_steps(self) -> int:\n",
    "    # How many steps / batches the network has been trained for.\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing a game\n",
    "\n",
    "When creating a game, it is a loop. The game will be ends when a terminal condition has been met or the number of moves is reached.\n",
    "\n",
    "When a new game is started, the MCTS must be started at the root node too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
    "  game = config.new_game()\n",
    "\n",
    "  while not game.terminal() and len(game.history) < config.max_moves:\n",
    "    # At the root of the search tree we use the representation function to\n",
    "    # obtain a hidden state given the current observation.\n",
    "    root = Node(0)\n",
    "    current_observation = game.make_image(-1)\n",
    "    expand_node(root, game.to_play(), game.legal_actions(),\n",
    "                network.initial_inference(current_observation))\n",
    "    add_exploration_noise(config, root)\n",
    "\n",
    "    # We then run a Monte Carlo Tree Search using only action sequences and the\n",
    "    # model learned by the network.\n",
    "    run_mcts(config, root, game.action_history(), network)\n",
    "    action = select_action(config, len(game.history), root, network)\n",
    "    game.apply(action)\n",
    "    game.store_search_statistics(root)\n",
    "  return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Node stores key statistics relating to the number of times it has been visited <code>visit_count</code>, whose turn it is <code>to_play</code>, the predicted prior probability of choosing the action that leads to this node prior, the backfilled value sum of the node <code>node_sum</code>, its child nodes children, the hidden state it corresponds to <code>hidden_state</code> and the predicted reward received by moving to this node reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "\n",
    "  def __init__(self, prior: float):\n",
    "    self.visit_count = 0\n",
    "    self.to_play = -1\n",
    "    self.prior = prior\n",
    "    self.value_sum = 0\n",
    "    self.children = {}\n",
    "    self.hidden_state = None\n",
    "    self.reward = 0\n",
    "\n",
    "  def expanded(self) -> bool:\n",
    "    return len(self.children) > 0\n",
    "\n",
    "  def value(self) -> float:\n",
    "    if self.visit_count == 0:\n",
    "      return 0\n",
    "    return self.value_sum / self.visit_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then request the game to return the current observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_observation = game.make_image(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expand the root node using the known legal actions provided by the game and the inference about the current observation provided by the <code>initial_inference</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
    "                network_output: NetworkOutput):\n",
    "  node.to_play = to_play\n",
    "  node.hidden_state = network_output.hidden_state\n",
    "  node.reward = network_output.reward\n",
    "  policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "  policy_sum = sum(policy.values())\n",
    "  for action, p in policy.items():\n",
    "    node.children[action] = Node(p / policy_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_node(root, game.to_play(), game.legal_actions(),\n",
    "network.initial_inference(current_observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " add exploration noise to the root node actions — this is important to ensure that the MCTS explores a range of possible actions rather than only exploring the action which it currently believes to be optimal. For chess, root_dirichlet_alpha= 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "  actions = list(node.children.keys())\n",
    "  noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "  frac = config.root_exploration_fraction\n",
    "  for a, n in zip(actions, noise):\n",
    "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_exploration_noise(config, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS run function\n",
    "\n",
    "As MuZero has no knowledge of the environment rules, it also has no knowledge of the bounds on the rewards that it may receive throughout the learning process. The MinMaxStats object is created to store information on the current minimum and maximum rewards encountered so that MuZero can normalise its value output accordingly. Alternatively, this can also be initialised with known bounds for a game such as chess (-1, 1).\n",
    "\n",
    "The main MCTS loop iterates over num_simulations, where one simulation is a pass through the MCTS tree until a leaf node (i.e. unexplored node) is reached and subsequent backpropagation.\n",
    "1. The <code>history</code> is initialized with the list of actions taken from the start of the game. The current node is an initialize node, so it is the root node, and currently the search path has only one node.\n",
    "2. MuZero first traverses down the MCTS tree, always selecting the action with the highest UCB (Upper Confidence Bound) score.\n",
    "3. The UCB score is a measure that balances the estimated value of the action Q(s,a)with a exploration bonus based on the prior probability of selecting the action P(s,a) and the number of times the action has already been selected N(s,a).\n",
    "\n",
    "$$a^k=\\text{arg} \\max_a[Q(s,a) + P(s,a) \\cdot \\frac{\\sqrt{\\sum_b N(s,b)}}{1+N(s,a)}(c_1 + \\log (\\frac{\\sum_b N(s,b)+c_2+1}{c_2}))]$$\n",
    "\n",
    "4. the recurrent_inference function is called on the parent of the leaf node, in order to obtain the predicted reward and new hidden state (from the dynamics network) and policy and value of the new hidden state (from the prediction network).\n",
    "5. the value predicted by the network is back-propagated up the tree, along the search path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_child(config: MuZeroConfig, node: Node,\n",
    "                 min_max_stats: MinMaxStats):\n",
    "  _, action, child = max(\n",
    "      (ucb_score(config, node, child, min_max_stats), action,\n",
    "       child) for action, child in node.children.items())\n",
    "  return action, child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
    "                  discount: float, min_max_stats: MinMaxStats):\n",
    "  for node in search_path:\n",
    "    node.value_sum += value if node.to_play == to_play else -value\n",
    "    node.visit_count += 1\n",
    "    min_max_stats.update(node.value())\n",
    "\n",
    "    value = node.reward + discount * value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
    "             network: Network):\n",
    "  min_max_stats = MinMaxStats(config.known_bounds)\n",
    "\n",
    "  for _ in range(config.num_simulations):\n",
    "    history = action_history.clone()\n",
    "    node = root\n",
    "    search_path = [node]\n",
    "\n",
    "    while node.expanded():\n",
    "      action, node = select_child(config, node, min_max_stats)\n",
    "      history.add_action(action)\n",
    "      search_path.append(node)\n",
    "\n",
    "    # Inside the search tree we use the dynamics function to obtain the next\n",
    "    # hidden state given an action and the previous hidden state.\n",
    "    parent = search_path[-2]\n",
    "    network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                 history.last_action())\n",
    "    expand_node(node, history.to_play(), history.action_space(), network_output)\n",
    "\n",
    "    backpropagate(search_path, network_output.value, history.to_play(),\n",
    "                  config.discount, min_max_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mcts(config, root, game.action_history(), network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After num_simulations passes through the tree, the process stops and an action is chosen based on the number of times each child node of the root has been visited\n",
    "\n",
    "For the first 30 moves, the temperate of the softmax is set to 1, meaning that the probability of selection for each action is proportional to the number of times it has been visited. From the 30th move onwards, the action with the maximum number of visits is selected.\n",
    "\n",
    "$$p_\\alpha = \\frac{N(\\alpha)^{1/T}}{\\sum_b N(b)^{1/T}}$$\n",
    "\n",
    "Though the number of visits may feel a strange metric on which to select the final action, it isn’t really, as the UCB selection criteria within the MCTS process is designed to eventually spend more time exploring actions that it feels are truly high value opportunities, once it has sufficiently explored the alternatives early on in the process.\n",
    "\n",
    "The chosen action is then applied to the true environment and relevant values are appended to the following lists in the gameobject.\n",
    "- game.rewards — a list of true rewards received at each turn of the game\n",
    "- game.history — a list of actions taken at each turn of the game\n",
    "- game.child_visits — a list of action probability distributions from the root node at each turn of the game\n",
    "- game.root_values — a list of values of the root node at each turn of the game\n",
    "\n",
    "All of the game data (rewards, history, child_visits, root_values) is saved to the replay buffer and the actor is then free to start a new game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
    "                  network: Network):\n",
    "  visit_counts = [\n",
    "      (child.visit_count, action) for action, child in node.children.items()\n",
    "  ]\n",
    "  t = config.visit_softmax_temperature_fn(\n",
    "      num_moves=num_moves, training_steps=network.training_steps())\n",
    "  _, action = softmax_sample(visit_counts, t)\n",
    "  return action\n",
    "\n",
    "def visit_softmax_temperature(num_moves, training_steps):\n",
    "  if num_moves < 30:\n",
    "    return 1.0\n",
    "  else:\n",
    "    return 0.0  # Play according to the max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training\n",
    "\n",
    "It first creates a new Network object (that stores randomly initialised instances of MuZero’s three neural networks) and sets the learning rate to decay based on the number of training steps that have been completed. We also create the gradient descent optimiser that will calculate the magnitude and direction of the weight updates at each training step.\n",
    "\n",
    "The last part of this function simply loops over training_steps (=1,000,000 in the paper, for chess). At each step, it samples a batch of positions from the replay buffer and uses them to update the networks, which is saved to storage every checkpoint_interval batches (=1000).\n",
    "\n",
    "There are therefore two finals parts we need to cover — how MuZero creates a batch of training data and how it uses this to update the weights of the three neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
    "                  replay_buffer: ReplayBuffer):\n",
    "  network = Network()\n",
    "  learning_rate = config.lr_init * config.lr_decay_rate**(\n",
    "      tf.train.get_global_step() / config.lr_decay_steps)\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)\n",
    "\n",
    "  for i in range(config.training_steps):\n",
    "    if i % config.checkpoint_interval == 0:\n",
    "      storage.save_network(i, network)\n",
    "    batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
    "    update_weights(optimizer, network, batch, config.weight_decay)\n",
    "  storage.save_network(config.training_steps, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MuZero loss function\n",
    "\n",
    "The loss function of MuZero is shown as:\n",
    "\n",
    "$$ \\mathcal{L}_t(\\theta) = \\sum_{k=0}^K \\mathcal{L}^r(u_{t+k}, r_t^k)+\\mathcal{L}^v (z_{t+k},v_t^k) + \\mathcal{L}^p (\\pi_{t+k},p_t^k + c||\\theta||^2 $$\n",
    "\n",
    "$K$ is the <code>num_unroll_steps</code> variable. There are three losses we are trying to minimise:\n",
    "1. The difference between the predicted reward $k$ steps ahead of turn $t$ ($r$) and the actual reward ($u$)\n",
    "2. The difference between the predicted value $k$ steps ahead of turn $t$ ($v$) and the TD target value ($z$)\n",
    "3. The difference between the predicted policy $k$ steps ahead of turn $t$ ($p$) and the MCTS policy($\\pi$)\n",
    "\n",
    "These losses are summed over the rollout to generate the loss for a given position in the batch. There is also a regularisation term to penalise large weights in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(optimizer: tf.train.Optimizer, network: Network, batch,\n",
    "                   weight_decay: float):\n",
    "  loss = 0\n",
    "  for image, actions, targets in batch:\n",
    "    # Initial step, from the real observation.\n",
    "    value, reward, policy_logits, hidden_state = network.initial_inference(\n",
    "        image)\n",
    "    predictions = [(1.0, value, reward, policy_logits)]\n",
    "\n",
    "    # Recurrent steps, from action and previous hidden state.\n",
    "    for action in actions:\n",
    "      value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
    "          hidden_state, action)\n",
    "      predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
    "\n",
    "      hidden_state = tf.scale_gradient(hidden_state, 0.5)\n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "      gradient_scale, value, reward, policy_logits = prediction\n",
    "      target_value, target_reward, target_policy = target\n",
    "\n",
    "      l = (\n",
    "          scalar_loss(value, target_value) +\n",
    "          scalar_loss(reward, target_reward) +\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "              logits=policy_logits, labels=target_policy))\n",
    "\n",
    "      loss += tf.scale_gradient(l, gradient_scale)\n",
    "\n",
    "  for weights in network.get_weights():\n",
    "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "  optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In lab practice (MuZero)\n",
    "\n",
    "Because MuZero uses a lot of resources in GPU to run. This lab does not require to do. If you want to try you can download from [github respository](https://github.com/werner-duvaud/muzero-general) and open it in visual studio code. As the same, you can run it in google colab, please follow from this [link](https://stackoverflow.com/questions/51194303/how-to-run-a-python-script-in-a-py-file-from-a-google-colab-notebook)\n",
    "\n",
    "The code from the link is the same concept as pseudo code, but it has been written for support full running. This code version is supported multi-GPU running, and we cannot modify to run only 1 GPU for the short time, so it is not recommended to run in the shared server.\n",
    "\n",
    "The programs that you can run in this program (without modification) are:\n",
    "- Cartpole (Tested with the fully connected network)\n",
    "- Lunar Lander (Tested in deterministic mode with the fully connected network)\n",
    "- Gridworld (Tested with the fully connected network)\n",
    "- Tic-tac-toe (Tested with the fully connected network and the residual network)\n",
    "- Connect4 (Slightly tested with the residual network)\n",
    "- Gomoku\n",
    "- Twenty-One / Blackjack (Tested with the residual network)\n",
    "- Atari Breakout\n",
    "\n",
    "### Running MuZero\n",
    "\n",
    "When you want to run the MuZero, run is as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python muzero.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are command line functions that you can input the number to select the games and load pretrained weight, training and playing the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instruction in the github respository to get the results in Tensorboard.\n",
    "\n",
    "**Note**: you may need to install some extra requirements as:\n",
    "- gym[classic_control]\n",
    "- nevergrad\n",
    "- numpy\n",
    "- ray\n",
    "- seaborn\n",
    "- tensorboard\n",
    "- torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In lab practice 2 (AlphaZero)\n",
    "\n",
    "The AlphaZero is more light weight and more friendly to try! You can run in any pc as you want without any problems.\n",
    "\n",
    "Please download the code from [github respository](https://github.com/suragnair/alpha-zero-general)\n",
    "\n",
    "Before run, install a library named <code>coloredlogs</code>. This library will allow you to show the text color in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install coloredlogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running AlphaZero\n",
    "\n",
    "Same as MuZero, to run the AlphaZero in **Othello** game can run by the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the selected GPU, for example from slot 0 to 1, open the code file name <code>NNet.py</code>. Search all the file and change from <code>xxx.cuda()</code> to <code>xxx.to(\"cuda:1\")</code>\n",
    "\n",
    "Here is the all code which is changed from automatic cuda to \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../../')\n",
    "from utils import *\n",
    "from NeuralNet import NeuralNet\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from .OthelloNNet import OthelloNNet as onnet\n",
    "\n",
    "args = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'num_channels': 512,\n",
    "})\n",
    "\n",
    "\n",
    "class NNetWrapper(NeuralNet):\n",
    "    def __init__(self, game):\n",
    "        self.nnet = onnet(game, args)\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "\n",
    "        if args.cuda:\n",
    "            # self.nnet.cuda()\n",
    "            self.nnet.to(\"cuda:1\")\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        examples: list of examples, each example is of form (board, pi, v)\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.nnet.parameters())\n",
    "\n",
    "        for epoch in range(args.epochs):\n",
    "            print('EPOCH ::: ' + str(epoch + 1))\n",
    "            self.nnet.train()\n",
    "            pi_losses = AverageMeter()\n",
    "            v_losses = AverageMeter()\n",
    "\n",
    "            batch_count = int(len(examples) / args.batch_size)\n",
    "\n",
    "            t = tqdm(range(batch_count), desc='Training Net')\n",
    "            for _ in t:\n",
    "                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
    "                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
    "                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "                target_pis = torch.FloatTensor(np.array(pis))\n",
    "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "                # predict\n",
    "                if args.cuda:\n",
    "                    # boards, target_pis, target_vs = boards.contiguous().cuda(), target_pis.contiguous().cuda(), target_vs.contiguous().cuda()\n",
    "                    boards, target_pis, target_vs = boards.contiguous().to(\"cuda:1\"), target_pis.contiguous().to(\"cuda:1\"), target_vs.contiguous().to(\"cuda:1\")\n",
    "\n",
    "                # compute output\n",
    "                out_pi, out_v = self.nnet(boards)\n",
    "                l_pi = self.loss_pi(target_pis, out_pi)\n",
    "                l_v = self.loss_v(target_vs, out_v)\n",
    "                total_loss = l_pi + l_v\n",
    "\n",
    "                # record loss\n",
    "                pi_losses.update(l_pi.item(), boards.size(0))\n",
    "                v_losses.update(l_v.item(), boards.size(0))\n",
    "                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n",
    "\n",
    "                # compute gradient and do SGD step\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, board):\n",
    "        \"\"\"\n",
    "        board: np array with board\n",
    "        \"\"\"\n",
    "        # timing\n",
    "        start = time.time()\n",
    "\n",
    "        # preparing input\n",
    "        board = torch.FloatTensor(board.astype(np.float64))\n",
    "        # if args.cuda: board = board.contiguous().cuda()\n",
    "        if args.cuda: board = board.contiguous().to(\"cuda:1\")\n",
    "        board = board.view(1, self.board_x, self.board_y)\n",
    "        self.nnet.eval()\n",
    "        with torch.no_grad():\n",
    "            pi, v = self.nnet(board)\n",
    "\n",
    "        # print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n",
    "        return torch.exp(pi).data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n",
    "\n",
    "    def loss_pi(self, targets, outputs):\n",
    "        return -torch.sum(targets * outputs) / targets.size()[0]\n",
    "\n",
    "    def loss_v(self, targets, outputs):\n",
    "        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
    "\n",
    "    def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(folder):\n",
    "            print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "            os.mkdir(folder)\n",
    "        else:\n",
    "            print(\"Checkpoint Directory exists! \")\n",
    "        torch.save({\n",
    "            'state_dict': self.nnet.state_dict(),\n",
    "        }, filepath)\n",
    "\n",
    "    def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            raise (\"No model in path {}\".format(filepath))\n",
    "        map_location = None if args.cuda else 'cpu'\n",
    "        checkpoint = torch.load(filepath, map_location=map_location)\n",
    "        self.nnet.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the game, go to <code>main.py</code> and change the library above. In current version, the pytorch game support has only one game, othello. If you have keras or tensorflow, you can try connect4 and gobang (Go game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import coloredlogs\n",
    "\n",
    "from Coach import Coach\n",
    "from othello.OthelloGame import OthelloGame as Game        # Change here\n",
    "from othello.pytorch.NNet import NNetWrapper as nn         # Change here\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
